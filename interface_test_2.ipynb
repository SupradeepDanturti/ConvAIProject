{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcXJGoaPuHw+hrlE5mIGbY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/dev2/interface_test_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "6dygIo7xPt1a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Pull folder from github \"\"\"\n",
        "# !git clone --filter=blob:none --no-checkout https://github.com/SupradeepDanturti/ConvAIProject\n",
        "# %cd ConvAIProject\n",
        "# !git sparse-checkout init --cone\n",
        "# !git sparse-checkout set results\n",
        "# !git checkout\n",
        "\n",
        "\"\"\" From Google Drive \"\"\"\n",
        "%%capture\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown 1zDgXx_npH-DigA3shNEzEOTWgsy3MRF2\n",
        "!unzip results.zip"
      ],
      "metadata": {
        "id": "fESKpUX4evnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Interface for all models\n",
        "### <font color='289C4E'>Table of contents<font><a class='anchor' id='top'></a>\n",
        "- [Results & Comparision](#scrollTo=FQhSGXXCdy2C)\n",
        "- [XVector](#scrollTo=XNSkswC8RuCZ)\n",
        "- [Ecapa-TDNN](#scrollTo=R30fxJWFRxIB)\n",
        "- [Selfsupervised](#scrollTo=iOxJgJE_RxOY)\n",
        "- [Wav2vec2](#scrollTo=ltvGJDdaSDHo)"
      ],
      "metadata": {
        "id": "YkvDTwZ7PkNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "FQhSGXXCdy2C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g08KAY-ClskM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#wav2vec2"
      ],
      "metadata": {
        "id": "ltvGJDdaSDHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file hyperparams_selfsupervised_xvector.yaml\n",
        "\n",
        "sample_rate: 16000\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: /content/ssl_checkpoint\n",
        "\n",
        "freeze_ssl: False\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "encoder_dim: 768\n",
        "emb_dim: 128\n",
        "out_n_neurons: 5\n",
        "\n",
        "label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "# Mean and std normalization of the input features\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <encoder_dim>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 3\n",
        "    tdnn_channels: [ 64, 64, 64 ]\n",
        "    tdnn_kernel_sizes: [ 5, 2, 3 ]\n",
        "    tdnn_dilations: [ 1, 2, 3 ]\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, !ref <emb_dim>]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    embedding_model:  !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <embedding_model>, !ref <classifier>]\n",
        "\n",
        "\n",
        "pretrained_path: /content/\n",
        "\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "  loadables:\n",
        "      embedding_model: !ref <embedding_model>\n",
        "      classifier: !ref <classifier>\n",
        "      ssl_model: !ref <ssl_model>\n",
        "      model: !ref <model>\n",
        "      label_encoder: !ref <label_encoder>\n",
        "  paths:\n",
        "      embedding_model: !ref <pretrained_path>/embedding_model.ckpt\n",
        "      classifier: !ref <pretrained_path>/classifier.ckpt\n",
        "      ssl_model: !ref <pretrained_path>/ssl_model.ckpt\n",
        "      model: !ref <pretrained_path>/model.ckpt\n",
        "      label_encoder: !ref <pretrained_path>/label_encoder.txt"
      ],
      "metadata": {
        "id": "NULGrCcUezds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc1d092-5f5f-45de-862e-80df56de5808"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hyperparams_selfsupervised_xvector.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EiTnAGxEnxVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base"
      ],
      "metadata": {
        "id": "b7t8XHAUez7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "4KV97OetbYED"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "\n",
        "    def resample_waveform(self, waveform, orig_sample_rate):\n",
        "        \"\"\"\n",
        "        Resample the waveform to a new sample rate.\n",
        "        \"\"\"\n",
        "        if orig_sample_rate != self.sample_rate:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=self.sample_rate)\n",
        "            waveform = resample_transform(waveform)\n",
        "        return waveform\n",
        "\n",
        "    def merge_overlapping_segments(self, segments):\n",
        "      if not segments:\n",
        "          return []\n",
        "      merged = [segments[0]]\n",
        "      for current in segments[1:]:\n",
        "          prev = merged[-1]\n",
        "          if current[0] <= prev[1]:\n",
        "              if current[2] == prev[2]:\n",
        "                  merged[-1] = (prev[0], max(prev[1], current[1]), prev[2])\n",
        "              else:\n",
        "                  merged.append(current)\n",
        "          else:\n",
        "              merged.append(current)\n",
        "      return merged\n",
        "\n",
        "    def refine_transitions(self, aggregated_predictions):\n",
        "        \"\"\"\n",
        "        Refines transition times by potentially adjusting them to be at the start\n",
        "        or end of segments, aiming to make the transitions smoother and more accurate.\n",
        "        \"\"\"\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label = aggregated_predictions[i-1]\n",
        "\n",
        "            if current_start - prev_end <= 1.0:\n",
        "                new_start = prev_end\n",
        "            else:\n",
        "                new_start = current_start\n",
        "\n",
        "            refined_predictions.append((new_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "    def refine_transitions_with_confidence(self, aggregated_predictions, segment_confidences):\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label, prev_confidence = refined_predictions[-1] + (segment_confidences[i-1],)\n",
        "\n",
        "            current_confidence = segment_confidences[i]\n",
        "\n",
        "            if current_label != prev_label:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    transition_point = current_start\n",
        "                else:\n",
        "                    transition_point = prev_end\n",
        "                refined_predictions[-1] = (prev_start, transition_point, prev_label)\n",
        "                refined_predictions.append((transition_point, current_end, current_label))\n",
        "            else:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    refined_predictions[-1] = (prev_start, current_end, current_label)\n",
        "                else:\n",
        "                    refined_predictions.append((current_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "\n",
        "\n",
        "    def aggregate_segments_with_overlap(self, segment_predictions):\n",
        "        aggregated_predictions = []\n",
        "        last_start, last_end, last_label = segment_predictions[0]\n",
        "\n",
        "        for start, end, label in segment_predictions[1:]:\n",
        "            if label == last_label and start <= last_end:\n",
        "                last_end = max(last_end, end)\n",
        "            else:\n",
        "                aggregated_predictions.append((last_start, last_end, last_label))\n",
        "                last_start, last_end, last_label = start, end, label\n",
        "\n",
        "        aggregated_predictions.append((last_start, last_end, last_label))\n",
        "\n",
        "        merged = self.merge_overlapping_segments(aggregated_predictions)\n",
        "        return merged\n",
        "\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs) #For\n",
        "        # feats = self.mods.ssl_model(wavs, wav_lens) #For selfsupervised model\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def create_segments(self, waveform, segment_length, overlap):\n",
        "        num_samples = waveform.shape[1]\n",
        "        segment_samples = int(segment_length * self.sample_rate)\n",
        "        overlap_samples = int(overlap * self.sample_rate)\n",
        "        step_samples = segment_samples - overlap_samples\n",
        "        segments = []\n",
        "        segment_times = []\n",
        "\n",
        "        for start in range(0, num_samples - segment_samples + 1, step_samples):\n",
        "            end = start + segment_samples\n",
        "            segments.append(waveform[:, start:end])\n",
        "            start_time = start / self.sample_rate\n",
        "            end_time = end / self.sample_rate\n",
        "            segment_times.append((start_time, end_time))\n",
        "\n",
        "        return segments, segment_times\n",
        "\n",
        "    def classify_file(self, path, segment_length=2.0, overlap=1.47, **kwargs):\n",
        "        \"\"\"Adjusted to handle overlapped segment predictions and refining transitions\"\"\"\n",
        "        waveform, osr = torchaudio.load(path)\n",
        "        waveform = self.resample_waveform(waveform, osr)\n",
        "\n",
        "\n",
        "        \"\"\" Attempt - Overlap Segments \"\"\"\n",
        "        segments, segment_times = self.create_segments(waveform, segment_length, overlap)\n",
        "        segment_predictions = []\n",
        "\n",
        "        for segment, (start_time, end_time) in zip(segments, segment_times):\n",
        "            rel_length = torch.tensor([1.0])\n",
        "            emb = self.encode_batch(segment, rel_length)\n",
        "            out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "            score, index = torch.max(out_prob, dim=-1)\n",
        "            text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "            segment_predictions.append((start_time, end_time, text_lab[0]))\n",
        "\n",
        "        aggregated_predictions = self.aggregate_segments_with_overlap(segment_predictions)\n",
        "        refined_predictions = self.refine_transitions(aggregated_predictions)\n",
        "        preds = self.refine_transitions_with_confidence(aggregated_predictions , refined_predictions)\n",
        "\n",
        "\n",
        "        with open(\"sample_segment_predictions.txt\", \"w\") as file:\n",
        "            for start_time, end_time, prediction in preds:\n",
        "                speaker_text = \"no speech\" if str(prediction) == \"0\" else (\"1 speaker\" if str(prediction) == \"1\" else f\"{prediction} speakers\")\n",
        "                print(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\")\n",
        "                file.write(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\\n\")\n",
        "\n",
        "        \"\"\" End of Attempt - Overlap Segments \"\"\"\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_file(wavs, wav_lens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_4_spk_1_mixture_004_segment.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmb-5p4ab-Yu",
        "outputId": "3859e44e-f53a-493d-960d-c2454cb8118e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-2.00 has 1 speaker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UePYM0IeeyIG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}