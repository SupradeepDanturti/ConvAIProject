{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlwPb4nvUdxcK4II6nHSHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/dev2/interface_test_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6bCpuKZqJrDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "fFO7-WNnbdjH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "## drive link -> https://drive.google.com/file/d/1xfG7KJlP0SgriIE1BLpTrBhytACKIkAY/view?usp=sharing\n",
        "!unzip /content/drive/MyDrive/ConvAI/interface_files.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6R9k1rJ7Yp1",
        "outputId": "2a30d66e-bb00-4843-8f66-b3dabf755d9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Archive:  /content/drive/MyDrive/ConvAI/interface_files.zip\n",
            "  inflating: classifier.ckpt         \n",
            "  inflating: embedding_model.ckpt    \n",
            "  inflating: hyperparams.yaml        \n",
            "  inflating: label_encoder.txt       \n",
            "  inflating: session_2_spk_2_mixture.wav  \n",
            "  inflating: session_4_spk_1_mixture_004_segment.wav  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4KV97OetbYED"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "\n",
        "    def resample_waveform(self, waveform, orig_sample_rate):\n",
        "        \"\"\"\n",
        "        Resample the waveform to a new sample rate.\n",
        "        \"\"\"\n",
        "        if orig_sample_rate != self.sample_rate:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=self.sample_rate)\n",
        "            waveform = resample_transform(waveform)\n",
        "        return waveform\n",
        "\n",
        "    def merge_overlapping_segments(self, segments):\n",
        "      if not segments:\n",
        "          return []\n",
        "\n",
        "      # Start with the first segment\n",
        "      merged = [segments[0]]\n",
        "      for current in segments[1:]:\n",
        "          prev = merged[-1]\n",
        "          if current[0] <= prev[1]:  # Check if the current segment overlaps with the previous\n",
        "              # Merge the two segments if they have the same label\n",
        "              if current[2] == prev[2]:\n",
        "                  merged[-1] = (prev[0], max(prev[1], current[1]), prev[2])\n",
        "              else:\n",
        "                  merged.append(current)\n",
        "          else:\n",
        "              merged.append(current)\n",
        "      return merged\n",
        "\n",
        "    def aggregate_segments_with_overlap(self, segment_predictions):\n",
        "        aggregated_predictions = []\n",
        "        last_start, last_end, last_label = segment_predictions[0]\n",
        "\n",
        "        for start, end, label in segment_predictions[1:]:\n",
        "            if label == last_label and start <= last_end:\n",
        "                # Extend the current segment if it overlaps and has the same label\n",
        "                last_end = max(last_end, end)\n",
        "            else:\n",
        "                # Save the current segment and start a new one\n",
        "                aggregated_predictions.append((last_start, last_end, last_label))\n",
        "                last_start, last_end, last_label = start, end, label\n",
        "\n",
        "        # Add the last segment\n",
        "        aggregated_predictions.append((last_start, last_end, last_label))\n",
        "\n",
        "        # Merge any further overlapped segments\n",
        "        merged = self.merge_overlapping_segments(aggregated_predictions)\n",
        "        return merged\n",
        "\n",
        "\n",
        "    def analyze_transition(self, segment, start, end):\n",
        "\n",
        "        # Define the size of sub-segments for detailed analysis, e.g., 0.25 seconds\n",
        "        sub_segment_length = 0.25\n",
        "        sample_rate = self.sample_rate\n",
        "\n",
        "        sub_segment_samples = int(sub_segment_length * sample_rate)\n",
        "\n",
        "        label_counts = {}\n",
        "        max_label = None\n",
        "        max_count = 0\n",
        "\n",
        "        for sub_start in range(0, segment.shape[-1] - sub_segment_samples, sub_segment_samples):\n",
        "            sub_end = sub_start + sub_segment_samples\n",
        "            sub_segment = segment[:, sub_start:sub_end]\n",
        "\n",
        "            emb = self.encode_batch(sub_segment)\n",
        "            out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "            score, index = torch.max(out_prob, dim=-1)\n",
        "            label = self.hparams.label_encoder.decode_torch(index)[0]\n",
        "\n",
        "            # Update label counts\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "            if label_counts[label] > max_count:\n",
        "                max_label = label\n",
        "                max_count = label_counts[label]\n",
        "        return start, end, max_label\n",
        "\n",
        "\n",
        "    def refine_transitions(self, segments, segment_predictions):\n",
        "        \"\"\"Refined to analyze transitions more precisely\"\"\"\n",
        "        refined_predictions = []\n",
        "        for i in range(len(segment_predictions) - 1):\n",
        "            start_time, end_time, prediction = segment_predictions[i]\n",
        "            next_start_time, next_end_time, next_prediction = segment_predictions[i + 1]\n",
        "\n",
        "            if prediction != next_prediction:\n",
        "                transition_start = max(start_time, next_start_time - 0.25)\n",
        "                transition_end = min(end_time, next_start_time + 0.25)\n",
        "                refined_prediction = self.analyze_transition(segments[i], transition_start, transition_end)\n",
        "                refined_predictions.append(refined_prediction)\n",
        "            else:\n",
        "                refined_predictions.append(segment_predictions[i])\n",
        "\n",
        "        if segment_predictions and (len(refined_predictions) < len(segment_predictions)):\n",
        "            refined_predictions.append(segment_predictions[-1])\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        # Manage single waveforms in input\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs)\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def create_segments(self, waveform, segment_length, overlap):\n",
        "        num_samples = waveform.shape[1]\n",
        "        segment_samples = int(segment_length * self.sample_rate)\n",
        "        overlap_samples = int(overlap * self.sample_rate)\n",
        "        step_samples = segment_samples - overlap_samples\n",
        "        segments = []\n",
        "        segment_times = []\n",
        "\n",
        "        for start in range(0, num_samples - segment_samples + 1, step_samples):\n",
        "            end = start + segment_samples\n",
        "            segments.append(waveform[:, start:end])\n",
        "            start_time = start / self.sample_rate\n",
        "            end_time = end / self.sample_rate\n",
        "            segment_times.append((start_time, end_time))\n",
        "\n",
        "        return segments, segment_times\n",
        "\n",
        "    def classify_file(self, path, segment_length=2.0, overlap=1.0, **kwargs):\n",
        "        \"\"\"Adjusted to handle overlapped segment predictions and refining transitions\"\"\"\n",
        "        waveform, osr = torchaudio.load(path)\n",
        "        waveform = self.resample_waveform(waveform, osr)\n",
        "\n",
        "        segments, segment_times = self.create_segments(waveform, segment_length, overlap)\n",
        "        segment_predictions = []\n",
        "\n",
        "        for segment, (start_time, end_time) in zip(segments, segment_times):\n",
        "            rel_length = torch.tensor([1.0])\n",
        "            emb = self.encode_batch(segment, rel_length)\n",
        "            out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "            score, index = torch.max(out_prob, dim=-1)\n",
        "            text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "            segment_predictions.append((start_time, end_time, text_lab[0]))\n",
        "\n",
        "        # refined_segment_predictions = self.refine_transitions(segments, segment_predictions)\n",
        "        aggregated_predictions = self.aggregate_segments_with_overlap(segment_predictions)\n",
        "\n",
        "        with open(\"sample_segment_predictions.txt\", \"w\") as file:\n",
        "            for start_time, end_time, prediction in aggregated_predictions:\n",
        "                speaker_text = \"no speech\" if str(prediction) == \"0\" else (\"1 speaker\" if str(prediction) == \"1\" else f\"{prediction} speakers\")\n",
        "                print(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\")\n",
        "                file.write(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\\n\")\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_file(wavs, wav_lens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_2_spk_2_mixture.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmb-5p4ab-Yu",
        "outputId": "4097889f-97fb-4636-ff9a-c83b55d3651a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-2.00 has 1 speaker\n",
            "1.00-14.00 has 2 speakers\n",
            "13.00-15.00 has 1 speaker\n",
            "14.00-17.00 has 2 speakers\n",
            "16.00-19.00 has 1 speaker\n",
            "18.00-60.00 has 2 speakers\n",
            "59.00-61.00 has 1 speaker\n",
            "60.00-65.00 has 2 speakers\n",
            "64.00-66.00 has 1 speaker\n",
            "65.00-97.00 has 2 speakers\n",
            "96.00-99.00 has 1 speaker\n",
            "98.00-102.00 has 2 speakers\n",
            "101.00-105.00 has 1 speaker\n",
            "104.00-106.00 has 2 speakers\n",
            "105.00-108.00 has 1 speaker\n",
            "107.00-120.00 has 2 speakers\n",
            "119.00-145.00 has 1 speaker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUjFS_gQO_E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/SaveECAPAsampleinterface"
      ],
      "metadata": {
        "id": "kv1jU8pG-PDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file SpeakerCounter.py\n",
        "\n",
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs)\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def classify_batch(self, wavs, wav_lens=None):\n",
        "        emb = self.encode_batch(wavs, wav_lens)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        # text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index\n",
        "        # return out_prob, score, index, text_lab\n",
        "\n",
        "    def classify_file(self, path, **kwargs):\n",
        "        waveform = self.load_audio(path, **kwargs)\n",
        "        # Fake a batch:\n",
        "        batch = waveform.unsqueeze(0)\n",
        "        rel_length = torch.tensor([1.0])\n",
        "        emb = self.encode_batch(batch, rel_length)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index, text_lab\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_batch(wavs, wav_lens)"
      ],
      "metadata": {
        "id": "8qwhaMK5i-DS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef12809b-7df1-4a3b-ecaa-339359593cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting SpeakerCounter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_2_spk_2_mixture.wav\"\n",
        "# wav_path = \"/content/session_4_spk_1_mixture_004_segment.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "# audio_classifier.hparams.label_encoder.ignore_len()\n",
        "# signal, fs = torchaudio.load(wav_path)\n",
        "# # pred = audio_classifier.classify_file(wav_path)\n",
        "# embeddings = audio_classifier.encode_batch(signal)\n",
        "# prediction = audio_classifier.classify_batch(signal)\n",
        "# print(prediction)\n",
        "\n",
        "\"\"\"or \"\"\"\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "id": "a9H0xhvyjBy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b14dc6a4-b180-4064-8c10-65346e8dcac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"session_2_spk_2_mixture.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7da782cced87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7da782c7f75f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7da7828ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7da7828cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7da6bdc3a58e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7da6bdc32147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5bebc993410e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5bebc9942c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5bebc993f087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5bebc992ae2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7da7bf592244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5bebc9929c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5bebc993ea64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5bebc992aa1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #24: <unknown function> + 0x16893e (0x5bebc994293e in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #26: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #27: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #28: <unknown function> + 0x13f9c6 (0x5bebc99199c6 in /usr/bin/python3)\nframe #29: PyEval_EvalCode + 0x86 (0x5bebc9a0f256 in /usr/bin/python3)\nframe #30: <unknown function> + 0x23ae2d (0x5bebc9a14e2d in /usr/bin/python3)\nframe #31: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #33: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #35: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #36: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #37: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #38: <unknown function> + 0x2557af (0x5bebc9a2f7af in /usr/bin/python3)\nframe #39: <unknown function> + 0x1662ca (0x5bebc99402ca in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #43: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #45: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #46: PyObject_Call + 0x122 (0x5bebc9943492 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #48: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #49: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #50: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #51: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #52: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #53: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #54: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #56: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #58: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #59: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #60: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #61: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #62: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a038b7f68ed2>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\"\"\"or \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0maudio_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/SpeakerCounter.py\u001b[0m in \u001b[0;36mclassify_file\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mout_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/interfaces.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(self, path, savedir)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_normalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"session_2_spk_2_mixture.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7da782cced87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7da782c7f75f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7da7828ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7da7828cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7da6bdc3a58e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7da6bdc32147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5bebc993410e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5bebc9942c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5bebc993f087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5bebc992ae2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7da7bf592244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5bebc9929c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5bebc993ea64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5bebc992aa1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #24: <unknown function> + 0x16893e (0x5bebc994293e in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #26: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #27: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #28: <unknown function> + 0x13f9c6 (0x5bebc99199c6 in /usr/bin/python3)\nframe #29: PyEval_EvalCode + 0x86 (0x5bebc9a0f256 in /usr/bin/python3)\nframe #30: <unknown function> + 0x23ae2d (0x5bebc9a14e2d in /usr/bin/python3)\nframe #31: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #33: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #35: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #36: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #37: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #38: <unknown function> + 0x2557af (0x5bebc9a2f7af in /usr/bin/python3)\nframe #39: <unknown function> + 0x1662ca (0x5bebc99402ca in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #43: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #45: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #46: PyObject_Call + 0x122 (0x5bebc9943492 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #48: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #49: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #50: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #51: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #52: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #53: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #54: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #56: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #58: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #59: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #60: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #61: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #62: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "def divide_audio_into_segments(audio_path, segment_length=2):\n",
        "    # Load the audio file\n",
        "    sample_rate = 16000\n",
        "    waveform, _ = torchaudio.load(audio_path)\n",
        "\n",
        "    # Calculate the number of samples for the given segment length\n",
        "    num_samples_per_segment = sample_rate * segment_length\n",
        "\n",
        "    # Calculate the total number of segments, using standard Python operations for ceiling\n",
        "    total_segments = int(-(-waveform.size(1) // num_samples_per_segment))  # Ceiling division\n",
        "\n",
        "    # Process and save each segment\n",
        "    for i in range(total_segments):\n",
        "        # Calculate the start and end sample for the current segment\n",
        "        start_sample = i * num_samples_per_segment\n",
        "        end_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "        # If the end sample exceeds the waveform length, adjust it to the waveform length\n",
        "        end_sample = min(end_sample, waveform.size(1))\n",
        "\n",
        "        # Extract the segment\n",
        "        segment = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        # Save the segment to a file\n",
        "        segment_file_name = f'/content/samples/segment_{i + 1}.wav'\n",
        "        torchaudio.save(segment_file_name, segment, sample_rate)\n",
        "\n",
        "divide_audio_into_segments(wav_path)"
      ],
      "metadata": {
        "id": "CPMYmQ-PcCol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UePYM0IeeyIG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}