{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqo+ZdljdKtGcufml4fNp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/dev2/interface_test_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "6dygIo7xPt1a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Pull folder from github \"\"\"\n",
        "# !git clone --filter=blob:none --no-checkout https://github.com/SupradeepDanturti/ConvAIProject\n",
        "# %cd ConvAIProject\n",
        "# !git sparse-checkout init --cone\n",
        "# !git sparse-checkout set results\n",
        "# !git checkout\n",
        "\n",
        "\"\"\" From Google Drive \"\"\"\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown 13RWE9-rlvpAJ3UAUM--dW34EfLRL0wM8\n",
        "!unzip train_with_wav2vec2.zip"
      ],
      "metadata": {
        "id": "fESKpUX4evnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724e7583-a1ff-4781-e48b-bf4e5591060b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13RWE9-rlvpAJ3UAUM--dW34EfLRL0wM8\n",
            "From (redirected): https://drive.google.com/uc?id=13RWE9-rlvpAJ3UAUM--dW34EfLRL0wM8&confirm=t&uuid=a878a5fa-2d31-44c3-b17a-f33558648673\n",
            "To: /content/train_with_wav2vec2.zip\n",
            "100% 2.45G/2.45G [00:13<00:00, 176MB/s]\n",
            "Archive:  train_with_wav2vec2.zip\n",
            "   creating: train_with_wav2vec2/\n",
            "   creating: train_with_wav2vec2/1993/\n",
            "  inflating: train_with_wav2vec2/1993/env.log  \n",
            "  inflating: train_with_wav2vec2/1993/hyperparams.yaml  \n",
            "  inflating: train_with_wav2vec2/1993/log.txt  \n",
            "   creating: train_with_wav2vec2/1993/save/\n",
            "   creating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/\n",
            " extracting: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/brain.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/CKPT.yaml  \n",
            " extracting: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/counter.ckpt  \n",
            " extracting: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/dataloader-TRAIN.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/lr_annealing_output.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/lr_annealing_ssl.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/model.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/optimizer.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/ssl_model.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+07-39-22+00/ssl_opt.ckpt  \n",
            "   creating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/\n",
            " extracting: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/brain.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/CKPT.yaml  \n",
            " extracting: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/counter.ckpt  \n",
            " extracting: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/dataloader-TRAIN.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/lr_annealing_output.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/lr_annealing_ssl.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/model.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/optimizer.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/ssl_model.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/CKPT+2024-04-07+12-31-27+00/ssl_opt.ckpt  \n",
            "  inflating: train_with_wav2vec2/1993/save/label_encoder.txt  \n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/\n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/.locks/\n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/.locks/models--facebook--wav2vec2-base/\n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/\n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/.no_exist/\n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/.no_exist/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/\n",
            " extracting: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/.no_exist/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/model.safetensors  \n",
            " extracting: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/.no_exist/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/model.safetensors.index.json  \n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/blobs/\n",
            "  inflating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/blobs/3249fe98bfc62fcbc26067f724716a6ec49d12c4728a2af1df659013905dff21  \n",
            "  inflating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/blobs/3f24dc078fcba55ee1d417a413847ead40c093a3  \n",
            "  inflating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/blobs/47d7dc533f6d412e1c021eb181615d006e403bed  \n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/refs/\n",
            "  inflating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/refs/main  \n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/snapshots/\n",
            "   creating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/\n",
            "  inflating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/config.json  \n",
            "  inflating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/preprocessor_config.json  \n",
            "  inflating: train_with_wav2vec2/1993/save/ssl_checkpoint/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/pytorch_model.bin  \n",
            "  inflating: train_with_wav2vec2/1993/train_log.txt  \n",
            "  inflating: train_with_wav2vec2/1993/train_selfsupervised_mlp.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file selfsupervised_mlp_inference/hyperparams.yaml\n",
        "\n",
        "sample_rate: 16000\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: /content/ssl_checkpoint\n",
        "\n",
        "freeze_ssl: False\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "encoder_dim: 768\n",
        "out_n_neurons: 5\n",
        "\n",
        "label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    avg_pool: !ref <avg_pool>\n",
        "    log_softmax: !ref <log_softmax>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "\n",
        "# pretrained_path: content/selfsupervised_mlp_inference/\n",
        "\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "  loadables:\n",
        "      ssl_model: !ref <ssl_model>\n",
        "      model: !ref <model>\n",
        "      label_encoder: !ref <label_encoder>\n",
        "  # paths:\n",
        "  #     ssl_model: ssl_model.ckpt\n",
        "  #     model: model.ckpt\n",
        "  #     label_encoder: label_encoder.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwY6jgMajQgI",
        "outputId": "acfb1730-de80-4406-fb28-195921a01d73"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting selfsupervised_mlp_inference/hyperparams.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"output_mlp\",\n",
        "        \"log_softmax\",\n",
        "        \"avg_pool\",\n",
        "        \"output_mlp\",\n",
        "    ]\n",
        "\n",
        "    def resample_waveform(self, waveform, orig_sample_rate):\n",
        "        \"\"\"\n",
        "        Resample the waveform to a new sample rate.\n",
        "        \"\"\"\n",
        "        if orig_sample_rate != self.sample_rate:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=self.sample_rate)\n",
        "            waveform = resample_transform(waveform)\n",
        "        return waveform\n",
        "\n",
        "    def merge_overlapping_segments(self, segments):\n",
        "      if not segments:\n",
        "          return []\n",
        "      merged = [segments[0]]\n",
        "      for current in segments[1:]:\n",
        "          prev = merged[-1]\n",
        "          if current[0] <= prev[1]:\n",
        "              if current[2] == prev[2]:\n",
        "                  merged[-1] = (prev[0], max(prev[1], current[1]), prev[2])\n",
        "              else:\n",
        "                  merged.append(current)\n",
        "          else:\n",
        "              merged.append(current)\n",
        "      return merged\n",
        "\n",
        "    def refine_transitions(self, aggregated_predictions):\n",
        "        \"\"\"\n",
        "        Refines transition times by potentially adjusting them to be at the start\n",
        "        or end of segments, aiming to make the transitions smoother and more accurate.\n",
        "        \"\"\"\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label = aggregated_predictions[i-1]\n",
        "\n",
        "            if current_start - prev_end <= 1.0:\n",
        "                new_start = prev_end\n",
        "            else:\n",
        "                new_start = current_start\n",
        "\n",
        "            refined_predictions.append((new_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "    def refine_transitions_with_confidence(self, aggregated_predictions, segment_confidences):\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label, prev_confidence = refined_predictions[-1] + (segment_confidences[i-1],)\n",
        "\n",
        "            current_confidence = segment_confidences[i]\n",
        "\n",
        "            if current_label != prev_label:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    transition_point = current_start\n",
        "                else:\n",
        "                    transition_point = prev_end\n",
        "                refined_predictions[-1] = (prev_start, transition_point, prev_label)\n",
        "                refined_predictions.append((transition_point, current_end, current_label))\n",
        "            else:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    refined_predictions[-1] = (prev_start, current_end, current_label)\n",
        "                else:\n",
        "                    refined_predictions.append((current_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "\n",
        "\n",
        "    def aggregate_segments_with_overlap(self, segment_predictions):\n",
        "        aggregated_predictions = []\n",
        "        last_start, last_end, last_label = segment_predictions[0]\n",
        "\n",
        "        for start, end, label in segment_predictions[1:]:\n",
        "            if label == last_label and start <= last_end:\n",
        "                last_end = max(last_end, end)\n",
        "            else:\n",
        "                aggregated_predictions.append((last_start, last_end, last_label))\n",
        "                last_start, last_end, last_label = start, end, label\n",
        "\n",
        "        aggregated_predictions.append((last_start, last_end, last_label))\n",
        "\n",
        "        merged = self.merge_overlapping_segments(aggregated_predictions)\n",
        "        return merged\n",
        "\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        # feats = self.mods.compute_features(wavs) #For\n",
        "        feats = self.mods.ssl_model(wavs, wav_lens) #For selfsupervised model\n",
        "        outputs = self.mods.avg_pool(feats, wav_lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "        # feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        # embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return outputs\n",
        "\n",
        "    def create_segments(self, waveform, segment_length, overlap):\n",
        "        num_samples = waveform.shape[1]\n",
        "        segment_samples = int(segment_length * self.sample_rate)\n",
        "        overlap_samples = int(overlap * self.sample_rate)\n",
        "        step_samples = segment_samples - overlap_samples\n",
        "        segments = []\n",
        "        segment_times = []\n",
        "\n",
        "        for start in range(0, num_samples - segment_samples + 1, step_samples):\n",
        "            end = start + segment_samples\n",
        "            segments.append(waveform[:, start:end])\n",
        "            start_time = start / self.sample_rate\n",
        "            end_time = end / self.sample_rate\n",
        "            segment_times.append((start_time, end_time))\n",
        "\n",
        "        return segments, segment_times\n",
        "\n",
        "    def classify_file(self, path, segment_length=2.0, overlap=1.47, **kwargs):\n",
        "        \"\"\"Adjusted to handle overlapped segment predictions and refining transitions\"\"\"\n",
        "        waveform, osr = torchaudio.load(path)\n",
        "        waveform = self.resample_waveform(waveform, osr)\n",
        "\n",
        "\n",
        "        \"\"\" Attempt - Overlap Segments \"\"\"\n",
        "        segments, segment_times = self.create_segments(waveform, segment_length, overlap)\n",
        "        segment_predictions = []\n",
        "\n",
        "        for segment, (start_time, end_time) in zip(segments, segment_times):\n",
        "            rel_length = torch.tensor([1.0])\n",
        "            outputs = self.encode_batch(segment, rel_length)\n",
        "            outputs = self.mods.output_mlp(outputs)\n",
        "            out_prob = self.mods.log_softmax(outputs)\n",
        "            # out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "            score, index = torch.max(out_prob, dim=-1)\n",
        "            text_lab = index.item()\n",
        "            segment_predictions.append((start_time, end_time, text_lab))\n",
        "\n",
        "        aggregated_predictions = self.aggregate_segments_with_overlap(segment_predictions)\n",
        "        refined_predictions = self.refine_transitions(aggregated_predictions)\n",
        "        preds = self.refine_transitions_with_confidence(aggregated_predictions , refined_predictions)\n",
        "\n",
        "\n",
        "        with open(\"sample_segment_predictions.txt\", \"w\") as file:\n",
        "            for start_time, end_time, prediction in preds:\n",
        "                speaker_text = \"no speech\" if str(prediction) == \"0\" else (\"1 speaker\" if str(prediction) == \"1\" else f\"{prediction} speakers\")\n",
        "                print(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\")\n",
        "                file.write(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\\n\")\n",
        "\n",
        "        \"\"\" End of Attempt - Overlap Segments \"\"\"\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_file(wavs, wav_lens)"
      ],
      "metadata": {
        "id": "yjYuyk_8lcfy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_0_spk_1_mixture.wav\"\n",
        "save_dir = \"/content/local_dir/\"\n",
        "model_path = \"/content/selfsupervised_mlp_inference\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wldw2Le9mgS9",
        "outputId": "bb3b9241-95cf-48c0-fc69-861347bd0570"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:speechbrain.lobes.models.huggingface_transformers.wav2vec2:speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-127.61 has 1 speaker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sD-7uo9mgX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#wav2vec2"
      ],
      "metadata": {
        "id": "ltvGJDdaSDHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file hyperparams_selfsupervised_xvector.yaml\n",
        "\n",
        "sample_rate: 16000\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: /content/ssl_checkpoint\n",
        "\n",
        "freeze_ssl: False\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "encoder_dim: 768\n",
        "emb_dim: 128\n",
        "out_n_neurons: 5\n",
        "\n",
        "label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "# Mean and std normalization of the input features\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <encoder_dim>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 3\n",
        "    tdnn_channels: [ 64, 64, 64 ]\n",
        "    tdnn_kernel_sizes: [ 5, 2, 3 ]\n",
        "    tdnn_dilations: [ 1, 2, 3 ]\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, !ref <emb_dim>]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    embedding_model:  !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <embedding_model>, !ref <classifier>]\n",
        "\n",
        "\n",
        "pretrained_path: /content/\n",
        "\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "  loadables:\n",
        "      embedding_model: !ref <embedding_model>\n",
        "      classifier: !ref <classifier>\n",
        "      ssl_model: !ref <ssl_model>\n",
        "      model: !ref <model>\n",
        "      label_encoder: !ref <label_encoder>\n",
        "  paths:\n",
        "      embedding_model: !ref <pretrained_path>/embedding_model.ckpt\n",
        "      classifier: !ref <pretrained_path>/classifier.ckpt\n",
        "      ssl_model: !ref <pretrained_path>/ssl_model.ckpt\n",
        "      model: !ref <pretrained_path>/model.ckpt\n",
        "      label_encoder: !ref <pretrained_path>/label_encoder.txt"
      ],
      "metadata": {
        "id": "NULGrCcUezds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc1d092-5f5f-45de-862e-80df56de5808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hyperparams_selfsupervised_xvector.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "\n",
        "    def resample_waveform(self, waveform, orig_sample_rate):\n",
        "        \"\"\"\n",
        "        Resample the waveform to a new sample rate.\n",
        "        \"\"\"\n",
        "        if orig_sample_rate != self.sample_rate:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=self.sample_rate)\n",
        "            waveform = resample_transform(waveform)\n",
        "        return waveform\n",
        "\n",
        "    def merge_overlapping_segments(self, segments):\n",
        "      if not segments:\n",
        "          return []\n",
        "      merged = [segments[0]]\n",
        "      for current in segments[1:]:\n",
        "          prev = merged[-1]\n",
        "          if current[0] <= prev[1]:\n",
        "              if current[2] == prev[2]:\n",
        "                  merged[-1] = (prev[0], max(prev[1], current[1]), prev[2])\n",
        "              else:\n",
        "                  merged.append(current)\n",
        "          else:\n",
        "              merged.append(current)\n",
        "      return merged\n",
        "\n",
        "    def refine_transitions(self, aggregated_predictions):\n",
        "        \"\"\"\n",
        "        Refines transition times by potentially adjusting them to be at the start\n",
        "        or end of segments, aiming to make the transitions smoother and more accurate.\n",
        "        \"\"\"\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label = aggregated_predictions[i-1]\n",
        "\n",
        "            if current_start - prev_end <= 1.0:\n",
        "                new_start = prev_end\n",
        "            else:\n",
        "                new_start = current_start\n",
        "\n",
        "            refined_predictions.append((new_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "    def refine_transitions_with_confidence(self, aggregated_predictions, segment_confidences):\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label, prev_confidence = refined_predictions[-1] + (segment_confidences[i-1],)\n",
        "\n",
        "            current_confidence = segment_confidences[i]\n",
        "\n",
        "            if current_label != prev_label:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    transition_point = current_start\n",
        "                else:\n",
        "                    transition_point = prev_end\n",
        "                refined_predictions[-1] = (prev_start, transition_point, prev_label)\n",
        "                refined_predictions.append((transition_point, current_end, current_label))\n",
        "            else:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    refined_predictions[-1] = (prev_start, current_end, current_label)\n",
        "                else:\n",
        "                    refined_predictions.append((current_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "\n",
        "\n",
        "    def aggregate_segments_with_overlap(self, segment_predictions):\n",
        "        aggregated_predictions = []\n",
        "        last_start, last_end, last_label = segment_predictions[0]\n",
        "\n",
        "        for start, end, label in segment_predictions[1:]:\n",
        "            if label == last_label and start <= last_end:\n",
        "                last_end = max(last_end, end)\n",
        "            else:\n",
        "                aggregated_predictions.append((last_start, last_end, last_label))\n",
        "                last_start, last_end, last_label = start, end, label\n",
        "\n",
        "        aggregated_predictions.append((last_start, last_end, last_label))\n",
        "\n",
        "        merged = self.merge_overlapping_segments(aggregated_predictions)\n",
        "        return merged\n",
        "\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        # feats = self.mods.compute_features(wavs) #For\n",
        "        feats = self.mods.ssl_model(wavs, wav_lens) #For selfsupervised model\n",
        "        outputs = self.hparams.avg_pool(feats, wav_lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "        # feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        # embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return outputs\n",
        "\n",
        "    def create_segments(self, waveform, segment_length, overlap):\n",
        "        num_samples = waveform.shape[1]\n",
        "        segment_samples = int(segment_length * self.sample_rate)\n",
        "        overlap_samples = int(overlap * self.sample_rate)\n",
        "        step_samples = segment_samples - overlap_samples\n",
        "        segments = []\n",
        "        segment_times = []\n",
        "\n",
        "        for start in range(0, num_samples - segment_samples + 1, step_samples):\n",
        "            end = start + segment_samples\n",
        "            segments.append(waveform[:, start:end])\n",
        "            start_time = start / self.sample_rate\n",
        "            end_time = end / self.sample_rate\n",
        "            segment_times.append((start_time, end_time))\n",
        "\n",
        "        return segments, segment_times\n",
        "\n",
        "    def classify_file(self, path, segment_length=2.0, overlap=1.47, **kwargs):\n",
        "        \"\"\"Adjusted to handle overlapped segment predictions and refining transitions\"\"\"\n",
        "        waveform, osr = torchaudio.load(path)\n",
        "        waveform = self.resample_waveform(waveform, osr)\n",
        "\n",
        "\n",
        "        \"\"\" Attempt - Overlap Segments \"\"\"\n",
        "        segments, segment_times = self.create_segments(waveform, segment_length, overlap)\n",
        "        segment_predictions = []\n",
        "\n",
        "        for segment, (start_time, end_time) in zip(segments, segment_times):\n",
        "            rel_length = torch.tensor([1.0])\n",
        "            feats = self.encode_batch(segment, rel_length)\n",
        "            outputs = self.mods.output_mlp(feats)\n",
        "            out_prob = self.mods.log_softmax(outputs)\n",
        "            # out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "            score, index = torch.max(out_prob, dim=-1)\n",
        "            text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "            segment_predictions.append((start_time, end_time, text_lab[0]))\n",
        "\n",
        "        aggregated_predictions = self.aggregate_segments_with_overlap(segment_predictions)\n",
        "        refined_predictions = self.refine_transitions(aggregated_predictions)\n",
        "        preds = self.refine_transitions_with_confidence(aggregated_predictions , refined_predictions)\n",
        "\n",
        "\n",
        "        with open(\"sample_segment_predictions.txt\", \"w\") as file:\n",
        "            for start_time, end_time, prediction in preds:\n",
        "                speaker_text = \"no speech\" if str(prediction) == \"0\" else (\"1 speaker\" if str(prediction) == \"1\" else f\"{prediction} speakers\")\n",
        "                print(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\")\n",
        "                file.write(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\\n\")\n",
        "\n",
        "        \"\"\" End of Attempt - Overlap Segments \"\"\"\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_file(wavs, wav_lens)"
      ],
      "metadata": {
        "id": "EiTnAGxEnxVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_4_spk_1_mixture_004_segment.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "id": "WaxiStnRlhoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHx6Ey8Slhyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base"
      ],
      "metadata": {
        "id": "b7t8XHAUez7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KV97OetbYED"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "\n",
        "    def resample_waveform(self, waveform, orig_sample_rate):\n",
        "        \"\"\"\n",
        "        Resample the waveform to a new sample rate.\n",
        "        \"\"\"\n",
        "        if orig_sample_rate != self.sample_rate:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=self.sample_rate)\n",
        "            waveform = resample_transform(waveform)\n",
        "        return waveform\n",
        "\n",
        "    def merge_overlapping_segments(self, segments):\n",
        "      if not segments:\n",
        "          return []\n",
        "      merged = [segments[0]]\n",
        "      for current in segments[1:]:\n",
        "          prev = merged[-1]\n",
        "          if current[0] <= prev[1]:\n",
        "              if current[2] == prev[2]:\n",
        "                  merged[-1] = (prev[0], max(prev[1], current[1]), prev[2])\n",
        "              else:\n",
        "                  merged.append(current)\n",
        "          else:\n",
        "              merged.append(current)\n",
        "      return merged\n",
        "\n",
        "    def refine_transitions(self, aggregated_predictions):\n",
        "        \"\"\"\n",
        "        Refines transition times by potentially adjusting them to be at the start\n",
        "        or end of segments, aiming to make the transitions smoother and more accurate.\n",
        "        \"\"\"\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label = aggregated_predictions[i-1]\n",
        "\n",
        "            if current_start - prev_end <= 1.0:\n",
        "                new_start = prev_end\n",
        "            else:\n",
        "                new_start = current_start\n",
        "\n",
        "            refined_predictions.append((new_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "    def refine_transitions_with_confidence(self, aggregated_predictions, segment_confidences):\n",
        "        refined_predictions = []\n",
        "        for i in range(len(aggregated_predictions)):\n",
        "            if i == 0:\n",
        "                refined_predictions.append(aggregated_predictions[i])\n",
        "                continue\n",
        "\n",
        "            current_start, current_end, current_label = aggregated_predictions[i]\n",
        "            prev_start, prev_end, prev_label, prev_confidence = refined_predictions[-1] + (segment_confidences[i-1],)\n",
        "\n",
        "            current_confidence = segment_confidences[i]\n",
        "\n",
        "            if current_label != prev_label:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    transition_point = current_start\n",
        "                else:\n",
        "                    transition_point = prev_end\n",
        "                refined_predictions[-1] = (prev_start, transition_point, prev_label)\n",
        "                refined_predictions.append((transition_point, current_end, current_label))\n",
        "            else:\n",
        "                if prev_confidence < current_confidence:\n",
        "                    refined_predictions[-1] = (prev_start, current_end, current_label)\n",
        "                else:\n",
        "                    refined_predictions.append((current_start, current_end, current_label))\n",
        "\n",
        "        return refined_predictions\n",
        "\n",
        "\n",
        "\n",
        "    def aggregate_segments_with_overlap(self, segment_predictions):\n",
        "        aggregated_predictions = []\n",
        "        last_start, last_end, last_label = segment_predictions[0]\n",
        "\n",
        "        for start, end, label in segment_predictions[1:]:\n",
        "            if label == last_label and start <= last_end:\n",
        "                last_end = max(last_end, end)\n",
        "            else:\n",
        "                aggregated_predictions.append((last_start, last_end, last_label))\n",
        "                last_start, last_end, last_label = start, end, label\n",
        "\n",
        "        aggregated_predictions.append((last_start, last_end, last_label))\n",
        "\n",
        "        merged = self.merge_overlapping_segments(aggregated_predictions)\n",
        "        return merged\n",
        "\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs) #For\n",
        "        # feats = self.mods.ssl_model(wavs, wav_lens) #For selfsupervised model\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def create_segments(self, waveform, segment_length, overlap):\n",
        "        num_samples = waveform.shape[1]\n",
        "        segment_samples = int(segment_length * self.sample_rate)\n",
        "        overlap_samples = int(overlap * self.sample_rate)\n",
        "        step_samples = segment_samples - overlap_samples\n",
        "        segments = []\n",
        "        segment_times = []\n",
        "\n",
        "        for start in range(0, num_samples - segment_samples + 1, step_samples):\n",
        "            end = start + segment_samples\n",
        "            segments.append(waveform[:, start:end])\n",
        "            start_time = start / self.sample_rate\n",
        "            end_time = end / self.sample_rate\n",
        "            segment_times.append((start_time, end_time))\n",
        "\n",
        "        return segments, segment_times\n",
        "\n",
        "    def classify_file(self, path, segment_length=2.0, overlap=1.47, **kwargs):\n",
        "        \"\"\"Adjusted to handle overlapped segment predictions and refining transitions\"\"\"\n",
        "        waveform, osr = torchaudio.load(path)\n",
        "        waveform = self.resample_waveform(waveform, osr)\n",
        "\n",
        "\n",
        "        \"\"\" Attempt - Overlap Segments \"\"\"\n",
        "        segments, segment_times = self.create_segments(waveform, segment_length, overlap)\n",
        "        segment_predictions = []\n",
        "\n",
        "        for segment, (start_time, end_time) in zip(segments, segment_times):\n",
        "            rel_length = torch.tensor([1.0])\n",
        "            emb = self.encode_batch(segment, rel_length)\n",
        "            out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "            score, index = torch.max(out_prob, dim=-1)\n",
        "            text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "            segment_predictions.append((start_time, end_time, text_lab[0]))\n",
        "\n",
        "        aggregated_predictions = self.aggregate_segments_with_overlap(segment_predictions)\n",
        "        refined_predictions = self.refine_transitions(aggregated_predictions)\n",
        "        preds = self.refine_transitions_with_confidence(aggregated_predictions , refined_predictions)\n",
        "\n",
        "\n",
        "        with open(\"sample_segment_predictions.txt\", \"w\") as file:\n",
        "            for start_time, end_time, prediction in preds:\n",
        "                speaker_text = \"no speech\" if str(prediction) == \"0\" else (\"1 speaker\" if str(prediction) == \"1\" else f\"{prediction} speakers\")\n",
        "                print(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\")\n",
        "                file.write(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\\n\")\n",
        "\n",
        "        \"\"\" End of Attempt - Overlap Segments \"\"\"\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_file(wavs, wav_lens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_4_spk_1_mixture_004_segment.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmb-5p4ab-Yu",
        "outputId": "3859e44e-f53a-493d-960d-c2454cb8118e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-2.00 has 1 speaker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import re\n",
        "\n",
        "# # Define a function to parse the log file\n",
        "# def parse_log_file(file_name):\n",
        "#     epochs = []\n",
        "#     valid_losses = []\n",
        "#     valid_error_rates = []\n",
        "#     train_losses = []\n",
        "#     with open(file_name, 'r') as file:\n",
        "#       for line in file:\n",
        "#           match = re.search(r'Epoch: (\\d+).*train loss: (\\d+(?:\\.\\d+)?(?:e[+-]?\\d+)?).*valid loss: (\\d+(?:\\.\\d+)?(?:e[+-]?\\d+)?).*valid error_rate: (\\d+(?:\\.\\d+)?(?:e[+-]?\\d+)?)', line)\n",
        "#           if match:\n",
        "#             epoch = int(match.group(1))\n",
        "#             train_loss = float(match.group(2))\n",
        "#             valid_loss = float(match.group(3))\n",
        "#             valid_error_rate = float(match.group(4))\n",
        "#             epochs.append(epoch)\n",
        "#             train_losses.append(train_loss)\n",
        "#             valid_losses.append(valid_loss)\n",
        "#             valid_error_rates.append(valid_error_rate)\n",
        "#     return epochs, train_losses, valid_losses, valid_error_rates\n",
        "\n",
        "# # Define the log file names\n",
        "# file_names = [\n",
        "#     # \"/content/ecapa_tdnn_augmented_train_log.txt\",\n",
        "#     # \"/content/ecapa_tdnn_unaugmented_train_log.txt\",\n",
        "#     # \"/content/self_supervised_mlp_train_log.txt\",\n",
        "#     # \"/content/selfsupervised_xvector_train_log.txt\",\n",
        "#     # \"/content/xvector_augmented_train_log.txt\",\n",
        "#     # \"/content/xvector_unaugmented_train_log.txt\"\n",
        "# ]\n",
        "\n",
        "# # Plot the data from each log file\n",
        "# for file_name in file_names:\n",
        "#     epochs, train_losses, valid_losses, valid_error_rates = parse_log_file(file_name)\n",
        "#     label = file_name.split(\"/\")[2].split(\".\")[0].split(\"_\")[:-2]\n",
        "#     label = '_'.join(label)\n",
        "#     # plt.plot(epochs, train_losses, label=label)\n",
        "#     # plt.plot(epochs, valid_losses, label=label)\n",
        "#     plt.plot(epochs, valid_error_rates, label=label)\n",
        "\n",
        "# # Add labels and legend\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss / Error Rate')\n",
        "# plt.title('Error Rate')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SCPh-f8gT9aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UePYM0IeeyIG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}