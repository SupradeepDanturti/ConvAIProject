{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnYsqSQJQ4pXl4z9Fy+SUZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/main/Sample_model_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "i65D8moWu9s3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nb0UFu-xuuJk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import json\n",
        "import speechbrain as sb\n",
        "import os, sys\n",
        "from speechbrain.utils.data_utils import get_all_files\n",
        "import torch\n",
        "from speechbrain.augment.preparation import write_csv\n",
        "from speechbrain.augment.time_domain import AddNoise, AddReverb\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "import random\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!tar -xvzf /content/drive/MyDrive/ConvAI/Project/maindata.tar.gz"
      ],
      "metadata": {
        "id": "OhyKWQEIu6Ew"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_json_data(json_info, file_name=\"train\"):\n",
        "    data = {}\n",
        "    for sess_file in json_info:\n",
        "        if sess_file.endswith('.json'):\n",
        "            wav_path = sess_file.replace('_metadata.json', '_mixture.wav')\n",
        "\n",
        "            with open(sess_file) as f:\n",
        "                session_data = json.load(f)\n",
        "\n",
        "            sess_id = os.path.basename(sess_file).split('_')[:-1]\n",
        "            sess_id = '_'.join(sess_id)\n",
        "\n",
        "            item_data = {\n",
        "                \"wav_path\": wav_path,\n",
        "                \"num_speakers\": f\"{session_data.get('num_speakers',sess_id.split('_')[-1])}\",\n",
        "            }\n",
        "\n",
        "            for key, value in session_data.items():\n",
        "                if key != 'num_speakers':\n",
        "                    item_data[key] = value\n",
        "\n",
        "            # Assign the aggregated data to this session ID\n",
        "            data[sess_id] = item_data\n",
        "\n",
        "    # Save combined metadata to JSON\n",
        "    metadata_path = os.path.join(f\"{file_name}_data.json\")\n",
        "    with open(metadata_path, \"w\") as jsonfile:\n",
        "        json.dump(data, jsonfile, indent=4)"
      ],
      "metadata": {
        "id": "HGlJRHlZu6G1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_json_info = get_all_files(\"./train\", match_and=['.json'])\n",
        "dev_json_info = get_all_files(\"./dev\", match_and=['.json'])\n",
        "eval_json_info = get_all_files(\"./eval\", match_and=['.json'])\n",
        "\n",
        "get_all_json_data(train_json_info, file_name = \"train\")\n",
        "get_all_json_data(dev_json_info, file_name = \"dev\")\n",
        "get_all_json_data(eval_json_info, file_name = \"eval\")\n",
        "print(\"Gathered all json data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQHLf9Mru6I_",
        "outputId": "c768f42c-0764-4dc3-d32f-954d31248762"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gathered all json data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file hparams_xvector_fbanks.yaml\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1986\n",
        "__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "output_folder: !ref ./results/TIMIT_tiny/Xvector/FBanks/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Path where data manifest files are stored\n",
        "train_annotation: train_data.json\n",
        "valid_annotation: dev_data.json\n",
        "test_annotation: eval_data.json\n",
        "\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 40\n",
        "\n",
        "# Training Parameters\n",
        "sample_rate: 16000\n",
        "number_of_epochs: 25\n",
        "batch_size: 16\n",
        "lr_start: 0.001\n",
        "lr_final: 0.0001\n",
        "n_classes: 5\n",
        "emb_dim: 128 # dimensionality of the embeddings\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "# Feature extraction\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "# Mean and std normalization of the input features\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: global\n",
        "\n",
        "# Embedding model: from variable size digits gets a fixed size embedding vector\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 3\n",
        "    tdnn_channels: [128, 128, 128]\n",
        "    tdnn_kernel_sizes: [5, 3, 1]\n",
        "    tdnn_dilations: [1, 3, 1]\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "\n",
        "# Clasifier applied on top of the embeddings\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, !ref <emb_dim>]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "    out_neurons: !ref <n_classes>\n",
        "\n",
        "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
        "# which is saved by the Checkpointer so that training can be resumed\n",
        "# if it gets interrupted at any point.\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
        "# device, as well as having train()/eval() called on them by the Brain class.\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "\n",
        "# This optimizer will be constructed by the Brain class after all parameters\n",
        "# are moved to the correct device. Then it will be added to the checkpointer.\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_start>\n",
        "\n",
        "# This function manages learning rate annealing over the epochs.\n",
        "# We here use the simple lr annealing method that linearly decreases\n",
        "# the lr from the initial value to the final one.\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr_start>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# This object is used for saving the state of training both so that it\n",
        "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
        "# can be later loaded for evaluation or inference.\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOKkkP27vlrp",
        "outputId": "5eaecf0a-a0bc-43db-b65f-221903033196"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hparams_xvector_fbanks.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train_fbanks.py\n",
        "# Your code here\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"Recipe for training a spk classification system.\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "\n",
        "# Brain class for speech enhancement training\n",
        "class DigitBrain(sb.Brain):\n",
        "    \"\"\"Class that manages the training loop. See speechbrain.core.Brain.\"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Runs all the computations that transforms the input into the\n",
        "        output probabilities over the N classes.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : Tensor\n",
        "            Tensor that contains the posterior probabilities over the N classes.\n",
        "        \"\"\"\n",
        "        # Your code here. Aim for 7-8 lines\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "        feats = self.modules.compute_features(wavs)\n",
        "        feats = self.modules.mean_var_norm(feats, lens)\n",
        "        embeddings = self.modules.embedding_model(feats, lens)\n",
        "        predictions = self.modules.classifier(embeddings)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        predictions : tensor\n",
        "            The output tensor from `compute_forward`.\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            A one-element tensor used for backpropagating the gradient.\n",
        "        \"\"\"\n",
        "\n",
        "        # Your code here. Aim for 7-8 lines\n",
        "        _, lens = batch['sig']\n",
        "        num_speakers_encoded = batch[\"num_speakers_encoded\"].data\n",
        "\n",
        "        loss = sb.nnet.losses.nll_loss(predictions, num_speakers_encoded, lens)\n",
        "        self.loss_metric.append(\n",
        "            batch.id, predictions, num_speakers_encoded, lens, reduction=\"batch\"\n",
        "        )\n",
        "\n",
        "        # Compute classification error at test time\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(batch.id, predictions, num_speakers_encoded, lens)\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of each epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up statistics trackers for this stage\n",
        "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
        "            metric=sb.nnet.losses.nll_loss\n",
        "        )\n",
        "\n",
        "        # Set up evaluation-only statistics trackers\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
        "        stage_loss : float\n",
        "            The average loss for all of the data processed in this stage.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store the train loss until the validation stage.\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_loss = stage_loss\n",
        "\n",
        "        # Summarize the statistics from the stage for record-keeping.\n",
        "        else:\n",
        "            stats = {\n",
        "                \"loss\": stage_loss,\n",
        "                \"error\": self.error_metrics.summarize(\"average\"),\n",
        "            }\n",
        "\n",
        "        # At the end of validation...\n",
        "        if stage == sb.Stage.VALID:\n",
        "\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            # The train_logger writes a summary to stdout and to the logfile.\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats={\"loss\": self.train_loss},\n",
        "                valid_stats=stats,\n",
        "            )\n",
        "\n",
        "            # Save the current checkpoint and delete previous checkpoints,\n",
        "            self.checkpointer.save_and_keep_only(meta=stats, min_keys=[\"error\"])\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "    We expect `prepare_mini_librispeech` to have been called before this,\n",
        "    so that the `train.json`, `valid.json`,  and `valid.json` manifest files\n",
        "    are available.\n",
        "    Arguments\n",
        "    ---------\n",
        "    hparams : dict\n",
        "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
        "        all the hyperparameters needed for dataset construction and loading.\n",
        "    Returns\n",
        "    -------\n",
        "    datasets : dict\n",
        "        Contains two keys, \"train\" and \"valid\" that correspond\n",
        "        to the appropriate DynamicItemDataset object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization of the label encoder. The label encoder assigns to each\n",
        "    # of the observed label a unique index (e.g, 'digit0': 0, 'digit1': 1, ..)\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "    print(label_encoder)\n",
        "\n",
        "    # Define audio pipeline\n",
        "    @sb.utils.data_pipeline.takes(\"wav_path\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav_path):\n",
        "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
        "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
        "        sig, fs = torchaudio.load(wav_path)\n",
        "\n",
        "        # Resampling\n",
        "        sig = torchaudio.functional.resample(sig, fs, 16000).squeeze(0)\n",
        "        return sig\n",
        "\n",
        "    # Define label pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"num_speakers\")\n",
        "    @sb.utils.data_pipeline.provides(\"num_speakers\", \"num_speakers_encoded\")\n",
        "    def label_pipeline(num_speakers):\n",
        "        \"\"\"Defines the pipeline to process the spk labels.\n",
        "        Note that we have to assign a different integer to each class\n",
        "        through the label encoder.\n",
        "        \"\"\"\n",
        "        yield num_speakers\n",
        "        num_speakers_encoded = label_encoder.encode_label_torch(num_speakers)\n",
        "        yield num_speakers_encoded\n",
        "\n",
        "    # Define datasets. We also connect the dataset with the data processing\n",
        "    # functions defined above.\n",
        "    datasets = {}\n",
        "    data_info = {\n",
        "        \"train\": hparams[\"train_annotation\"],\n",
        "        \"valid\": hparams[\"valid_annotation\"],\n",
        "        \"test\": hparams[\"test_annotation\"],\n",
        "    }\n",
        "    hparams[\"dataloader_options\"][\"shuffle\"] = True\n",
        "    for dataset in data_info:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=data_info[dataset],\n",
        "            dynamic_items=[audio_pipeline, label_pipeline],\n",
        "            output_keys=[\"id\", \"sig\", \"num_speakers_encoded\"],\n",
        "        )\n",
        "\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    # Please, take a look into the lab_enc_file to see the label to index\n",
        "    # mapping.\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[datasets[\"train\"]],\n",
        "        output_key=\"num_speakers\",\n",
        "    )\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Recipe begins!\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Reading command line arguments.\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides.\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin,  overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
        "    datasets = dataio_prep(hparams)\n",
        "\n",
        "    # Initialize the Brain object to prepare for mask training.\n",
        "    digit_brain = DigitBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    digit_brain.fit(\n",
        "        epoch_counter=digit_brain.hparams.epoch_counter,\n",
        "        train_set=datasets[\"train\"],\n",
        "        valid_set=datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = digit_brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"error\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgRaCf5dvnGH",
        "outputId": "af87e7e8-e416-46a6-adc7-de3156e3451a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_fbanks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py hparams_xvector_fbanks.yaml --device=\"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uZRJ1ePxZ87",
        "outputId": "500fff79-089b-42e2-d76f-25f37337ce89"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ./results/TIMIT_tiny/Xvector/FBanks/1986\n",
            "<speechbrain.dataio.encoder.CategoricalEncoder object at 0x7a419bab2350>\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - 142.9k trainable parameters in DigitBrain\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 1/1 [00:07<00:00,  7.97s/it, train_loss=1.76]\n",
            "100% 1/1 [00:04<00:00,  4.31s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.001 to 0.00096\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-03 - train loss: 1.76 - valid loss: 1.61, valid error: 8.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-13-54+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 1/1 [00:06<00:00,  6.82s/it, train_loss=0.852]\n",
            "100% 1/1 [00:04<00:00,  4.42s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00096 to 0.00093\n",
            "speechbrain.utils.train_logger - Epoch: 2, lr: 9.63e-04 - train loss: 8.52e-01 - valid loss: 1.61, valid error: 8.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-06+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-13-54+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 1/1 [00:08<00:00,  8.85s/it, train_loss=0.363]\n",
            "100% 1/1 [00:03<00:00,  3.41s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00093 to 0.00089\n",
            "speechbrain.utils.train_logger - Epoch: 3, lr: 9.25e-04 - train loss: 3.63e-01 - valid loss: 1.61, valid error: 8.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-18+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-06+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 1/1 [00:07<00:00,  7.76s/it, train_loss=0.193]\n",
            "100% 1/1 [00:03<00:00,  3.32s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00089 to 0.00085\n",
            "speechbrain.utils.train_logger - Epoch: 4, lr: 8.88e-04 - train loss: 1.93e-01 - valid loss: 1.60, valid error: 8.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-29+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-18+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 1/1 [00:07<00:00,  7.65s/it, train_loss=0.121]\n",
            "100% 1/1 [00:03<00:00,  3.72s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00085 to 0.00081\n",
            "speechbrain.utils.train_logger - Epoch: 5, lr: 8.50e-04 - train loss: 1.21e-01 - valid loss: 1.60, valid error: 8.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-41+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100% 1/1 [00:07<00:00,  7.41s/it, train_loss=0.0841]\n",
            "100% 1/1 [00:03<00:00,  3.81s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00081 to 0.00078\n",
            "speechbrain.utils.train_logger - Epoch: 6, lr: 8.13e-04 - train loss: 8.41e-02 - valid loss: 1.59, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-52+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-41+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100% 1/1 [00:06<00:00,  6.59s/it, train_loss=0.063]\n",
            "100% 1/1 [00:04<00:00,  4.61s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00078 to 0.00074\n",
            "speechbrain.utils.train_logger - Epoch: 7, lr: 7.75e-04 - train loss: 6.30e-02 - valid loss: 1.58, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-03+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-14-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100% 1/1 [00:06<00:00,  6.67s/it, train_loss=0.0521]\n",
            "100% 1/1 [00:03<00:00,  3.52s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00074 to 0.0007\n",
            "speechbrain.utils.train_logger - Epoch: 8, lr: 7.38e-04 - train loss: 5.21e-02 - valid loss: 1.57, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-13+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100% 1/1 [00:07<00:00,  7.59s/it, train_loss=0.0445]\n",
            "100% 1/1 [00:03<00:00,  3.43s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0007 to 0.00066\n",
            "speechbrain.utils.train_logger - Epoch: 9, lr: 7.00e-04 - train loss: 4.45e-02 - valid loss: 1.56, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-25+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-13+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100% 1/1 [00:07<00:00,  7.70s/it, train_loss=0.0376]\n",
            "100% 1/1 [00:03<00:00,  3.37s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00066 to 0.00063\n",
            "speechbrain.utils.train_logger - Epoch: 10, lr: 6.63e-04 - train loss: 3.76e-02 - valid loss: 1.55, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-36+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-25+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100% 1/1 [00:07<00:00,  7.72s/it, train_loss=0.033]\n",
            "100% 1/1 [00:03<00:00,  3.40s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00063 to 0.00059\n",
            "speechbrain.utils.train_logger - Epoch: 11, lr: 6.25e-04 - train loss: 3.30e-02 - valid loss: 1.54, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-47+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100% 1/1 [00:08<00:00,  8.24s/it, train_loss=0.029]\n",
            "100% 1/1 [00:03<00:00,  3.81s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00059 to 0.00055\n",
            "speechbrain.utils.train_logger - Epoch: 12, lr: 5.88e-04 - train loss: 2.90e-02 - valid loss: 1.52, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-59+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-47+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100% 1/1 [00:06<00:00,  6.64s/it, train_loss=0.0262]\n",
            "100% 1/1 [00:04<00:00,  4.58s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00055 to 0.00051\n",
            "speechbrain.utils.train_logger - Epoch: 13, lr: 5.50e-04 - train loss: 2.62e-02 - valid loss: 1.50, valid error: 6.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-10+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-15-59+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100% 1/1 [00:06<00:00,  6.65s/it, train_loss=0.0228]\n",
            "100% 1/1 [00:03<00:00,  3.58s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00051 to 0.00048\n",
            "speechbrain.utils.train_logger - Epoch: 14, lr: 5.13e-04 - train loss: 2.28e-02 - valid loss: 1.48, valid error: 4.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-21+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-10+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100% 1/1 [00:07<00:00,  7.51s/it, train_loss=0.0207]\n",
            "100% 1/1 [00:03<00:00,  3.38s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00048 to 0.00044\n",
            "speechbrain.utils.train_logger - Epoch: 15, lr: 4.75e-04 - train loss: 2.07e-02 - valid loss: 1.46, valid error: 4.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-32+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-21+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100% 1/1 [00:07<00:00,  7.68s/it, train_loss=0.0188]\n",
            "100% 1/1 [00:03<00:00,  3.38s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00044 to 0.0004\n",
            "speechbrain.utils.train_logger - Epoch: 16, lr: 4.38e-04 - train loss: 1.88e-02 - valid loss: 1.43, valid error: 4.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-43+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-32+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100% 1/1 [00:07<00:00,  7.72s/it, train_loss=0.0175]\n",
            "100% 1/1 [00:03<00:00,  3.40s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0004 to 0.00036\n",
            "speechbrain.utils.train_logger - Epoch: 17, lr: 4.00e-04 - train loss: 1.75e-02 - valid loss: 1.41, valid error: 4.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-54+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-43+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100% 1/1 [00:06<00:00,  6.94s/it, train_loss=0.0164]\n",
            "100% 1/1 [00:04<00:00,  4.11s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00036 to 0.00033\n",
            "speechbrain.utils.train_logger - Epoch: 18, lr: 3.63e-04 - train loss: 1.64e-02 - valid loss: 1.38, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-16-54+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100% 1/1 [00:06<00:00,  6.62s/it, train_loss=0.0156]\n",
            "100% 1/1 [00:04<00:00,  4.44s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00033 to 0.00029\n",
            "speechbrain.utils.train_logger - Epoch: 19, lr: 3.25e-04 - train loss: 1.56e-02 - valid loss: 1.36, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100% 1/1 [00:06<00:00,  6.76s/it, train_loss=0.0149]\n",
            "100% 1/1 [00:03<00:00,  3.40s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00029 to 0.00025\n",
            "speechbrain.utils.train_logger - Epoch: 20, lr: 2.88e-04 - train loss: 1.49e-02 - valid loss: 1.34, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-27+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-16+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 21\n",
            "100% 1/1 [00:07<00:00,  7.75s/it, train_loss=0.0144]\n",
            "100% 1/1 [00:03<00:00,  3.79s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00025 to 0.00021\n",
            "speechbrain.utils.train_logger - Epoch: 21, lr: 2.50e-04 - train loss: 1.44e-02 - valid loss: 1.31, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-38+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-27+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 22\n",
            "100% 1/1 [00:08<00:00,  8.36s/it, train_loss=0.0139]\n",
            "100% 1/1 [00:03<00:00,  3.39s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00021 to 0.00017\n",
            "speechbrain.utils.train_logger - Epoch: 22, lr: 2.13e-04 - train loss: 1.39e-02 - valid loss: 1.29, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-50+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-38+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 23\n",
            "100% 1/1 [00:07<00:00,  7.63s/it, train_loss=0.0136]\n",
            "100% 1/1 [00:03<00:00,  3.43s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00017 to 0.00014\n",
            "speechbrain.utils.train_logger - Epoch: 23, lr: 1.75e-04 - train loss: 1.36e-02 - valid loss: 1.26, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-18-01+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-17-50+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 24\n",
            "100% 1/1 [00:07<00:00,  7.38s/it, train_loss=0.0133]\n",
            "100% 1/1 [00:03<00:00,  3.81s/it]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00014 to 0.0001\n",
            "speechbrain.utils.train_logger - Epoch: 24, lr: 1.37e-04 - train loss: 1.33e-02 - valid loss: 1.23, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-18-13+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-18-01+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 25\n",
            "100% 1/1 [00:06<00:00,  6.62s/it, train_loss=0.0131]\n",
            "100% 1/1 [00:04<00:00,  4.57s/it]\n",
            "speechbrain.utils.train_logger - Epoch: 25, lr: 1.00e-04 - train loss: 1.31e-02 - valid loss: 1.20, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-18-24+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-18-13+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/TIMIT_tiny/Xvector/FBanks/1986/save/CKPT+2024-03-16+01-18-24+00\n",
            "100% 1/1 [00:03<00:00,  3.37s/it]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 25 - test loss: 1.25, test error: 2.00e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHwdmsz5xevq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}