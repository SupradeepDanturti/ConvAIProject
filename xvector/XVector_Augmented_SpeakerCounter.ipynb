{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-21T22:15:22.150013Z",
     "start_time": "2024-03-21T22:13:40.646346Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# \n",
    "# import json\n",
    "# import speechbrain as sb\n",
    "# import os, sys\n",
    "# from speechbrain.utils.data_utils import get_all_files\n",
    "# import torch\n",
    "# from speechbrain.dataio.dataio import read_audio\n",
    "# import random\n",
    "# import torchaudio\n",
    "# \n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "# import torchaudio\n",
    "# from joblib import Parallel, delayed\n",
    "# \n",
    "# def process_file(path):\n",
    "#     # Optimized path operations\n",
    "#     parts = path.split(\"/\")[-1].split(\"\\\\\")[-1].split(\"_\")\n",
    "#     id = \"_\".join(parts[:-1])\n",
    "#     num_speakers = parts[3]\n",
    "#     info = torchaudio.info(path)\n",
    "#     length = info.num_frames / 16000\n",
    "# \n",
    "#     return id, {\n",
    "#         \"wav_path\": path.replace(\"\\\\\",\"/\"),\n",
    "#         \"num_speakers\": num_speakers,\n",
    "#         \"length\": length\n",
    "#     }\n",
    "# \n",
    "# def load_json(json_paths, save_file=\"train\"):\n",
    "#     data = {}\n",
    "# \n",
    "#     # Parallel processing\n",
    "#     results = Parallel(n_jobs=-1, verbose=10)(\n",
    "#         delayed(process_file)(path) for path in json_paths\n",
    "#     )\n",
    "# \n",
    "#     for id, path_data in results:\n",
    "#         data[id] = path_data\n",
    "# \n",
    "#     with open(f\"../data/{save_file}_data.json\", 'w') as json_file:\n",
    "#         json.dump(data, json_file, indent=4)\n",
    "# \n",
    "# \n",
    "# # Example usage\n",
    "# train_files = get_all_files(\"../data/train\", match_and=['_segment.wav'])\n",
    "# test_files = get_all_files(\"../data/dev\", match_and=['_segment.wav'])\n",
    "# valid_files = get_all_files(\"../data/eval\", match_and=['_segment.wav'])\n",
    "# \n",
    "# load_json(train_files, save_file=\"train\")\n",
    "# load_json(test_files, save_file=\"test\")\n",
    "# load_json(valid_files, save_file=\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b253db18f2090e7b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hparams_xvector_augmentation.yaml\n"
     ]
    }
   ],
   "source": [
    "%%file hparams_xvector_augmentation.yaml\n",
    "\n",
    "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
    "seed: 1986\n",
    "__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "output_folder: !ref ../results/XVector/Augmented/<seed>\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "# Path where data manifest files will be stored\n",
    "# The data manifest files are created by the data preparation script.\n",
    "data_folder: ../data\n",
    "train_annotation: !ref <data_folder>/train_data.json\n",
    "valid_annotation: !ref <data_folder>/valid_data.json\n",
    "test_annotation: !ref <data_folder>/test_data.json\n",
    "\n",
    "\n",
    "# NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
    "# rirs_noises_root: !ref <data_folder>/RIRS_NOISES\n",
    "# data_folder_noise:\n",
    "#   - !ref <rirs_noises_root>/simulated_rirs/\n",
    "#   - !ref <rirs_noises_root>/real_rirs_isotropic_noises/\n",
    "# \n",
    "# data_folder_rir: !ref <rirs_noises_root>/pointsource_noises/ \n",
    "\n",
    "noise_annotation: !ref <data_folder>/noises.csv\n",
    "rir_annotation: !ref <data_folder>/simulated_rirs.csv\n",
    "\n",
    "# The train logger writes training statistics to a file, as well as stdout.\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
    "    metric: !name:speechbrain.nnet.losses.classification_error\n",
    "        reduction: batch\n",
    "\n",
    "ckpt_enable: True\n",
    "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
    "\n",
    "n_mels: 40\n",
    "deltas: True\n",
    "\n",
    "# Training Parameters\n",
    "sample_rate: 16000\n",
    "number_of_epochs: 50\n",
    "batch_size: 64\n",
    "lr_start: 0.001\n",
    "lr_final: 0.0001\n",
    "weight_decay: 0.00002\n",
    "\n",
    "tdnn_channels: 64\n",
    "tdnn_channels_out: 128\n",
    "n_classes: 5 \n",
    "emb_dim: 128 \n",
    "\n",
    "num_workers: 0\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>\n",
    "    num_workers: !ref <num_workers>\n",
    "\n",
    "##################################\n",
    "####### Data Augmentation ########\n",
    "##################################\n",
    "# Download and prepare the dataset of noisy sequences for augmentation\n",
    "\n",
    "skip_prep: True\n",
    "# prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
    "#     URL: !ref <NOISE_DATASET_URL>\n",
    "#     dest_folder: !ref <data_folder_noise>\n",
    "#     ext: wav\n",
    "#     csv_file: !ref <noise_annotation>\n",
    "\n",
    "\n",
    "# Add noise to input signal\n",
    "snr_low: 0  # Min SNR for noise augmentation\n",
    "snr_high: 15  # Max SNR for noise augmentation\n",
    "\n",
    "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
    "    csv_file: !ref <noise_annotation>\n",
    "    snr_low: !ref <snr_low>\n",
    "    snr_high: !ref <snr_high>\n",
    "    noise_sample_rate: !ref <sample_rate>\n",
    "    clean_sample_rate: !ref <sample_rate>\n",
    "    num_workers: !ref <num_workers>\n",
    "\n",
    "# Speed perturbation\n",
    "speed_changes: [95, 100, 105]  # List of speed changes for time-stretching\n",
    "\n",
    "speed_perturb: !new:speechbrain.augment.time_domain.SpeedPerturb\n",
    "    orig_freq: !ref <sample_rate>\n",
    "    speeds: !ref <speed_changes>\n",
    "\n",
    "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
    "drop_freq_low: 0  # Min frequency band dropout probability\n",
    "drop_freq_high: 1  # Max frequency band dropout probability\n",
    "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
    "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
    "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
    "\n",
    "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
    "    drop_freq_low: !ref <drop_freq_low>\n",
    "    drop_freq_high: !ref <drop_freq_high>\n",
    "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
    "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
    "    drop_freq_width: !ref <drop_freq_width>\n",
    "\n",
    "# Time drop: randomly drops a number of temporal chunks.\n",
    "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
    "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
    "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
    "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
    "\n",
    "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
    "    drop_length_low: !ref <drop_chunk_length_low>\n",
    "    drop_length_high: !ref <drop_chunk_length_high>\n",
    "    drop_count_low: !ref <drop_chunk_count_low>\n",
    "    drop_count_high: !ref <drop_chunk_count_high>\n",
    "\n",
    "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
    "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
    "    parallel_augment: False\n",
    "    concat_original: True\n",
    "    repeat_augment: 1\n",
    "    shuffle_augmentations: False\n",
    "    min_augmentations: 4\n",
    "    max_augmentations: 4\n",
    "    augment_prob: 1.0\n",
    "    augmentations: [\n",
    "        !ref <add_noise>,\n",
    "        !ref <speed_perturb>,\n",
    "        !ref <drop_freq>,\n",
    "        !ref <drop_chunk>]\n",
    "        \n",
    "##################################\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "# Feature extraction\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    n_mels: !ref <n_mels>\n",
    "    # deltas: !ref <deltas>\n",
    "\n",
    "# Mean and std normalization of the input features\n",
    "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: sentence\n",
    "    std_norm: False\n",
    "\n",
    "# To design a custom model, either just edit the simple CustomModel\n",
    "# class that's listed here, or replace this `!new` call with a line\n",
    "# pointing to a different file you've defined.\n",
    "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
    "    in_channels: !ref <n_mels>\n",
    "    activation: !name:torch.nn.LeakyReLU\n",
    "    tdnn_blocks: 5\n",
    "    tdnn_channels:\n",
    "        - !ref <tdnn_channels>\n",
    "        - !ref <tdnn_channels>\n",
    "        - !ref <tdnn_channels>\n",
    "        - !ref <tdnn_channels>\n",
    "        - !ref <tdnn_channels_out>\n",
    "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
    "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
    "    lin_neurons: !ref <emb_dim>\n",
    "\n",
    "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
    "    input_shape: [null, null, !ref <emb_dim>]\n",
    "    activation: !name:torch.nn.LeakyReLU\n",
    "    lin_blocks: 1\n",
    "    lin_neurons: !ref <emb_dim>\n",
    "    out_neurons: !ref <n_classes>\n",
    "\n",
    "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
    "# which is saved by the Checkpointer so that training can be resumed\n",
    "# if it gets interrupted at any point.\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs>\n",
    "\n",
    "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
    "# device, as well as having train()/eval() called on them by the Brain class.\n",
    "modules:\n",
    "    compute_features: !ref <compute_features>\n",
    "    embedding_model: !ref <embedding_model>\n",
    "    classifier: !ref <classifier>\n",
    "    mean_var_norm: !ref <mean_var_norm>\n",
    "\n",
    "# This optimizer will be constructed by the Brain class after all parameters\n",
    "# are moved to the correct device. Then it will be added to the checkpointer.\n",
    "opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr_start>\n",
    "    weight_decay: !ref <weight_decay>\n",
    "\n",
    "# This function manages learning rate annealing over the epochs.\n",
    "# We here use the simple lr annealing method that linearly decreases\n",
    "# the lr from the initial value to the final one.\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
    "    initial_value: !ref <lr_start>\n",
    "    final_value: !ref <lr_final>\n",
    "    epoch_count: !ref <number_of_epochs>\n",
    "\n",
    "# This object is used for saving the state of training both so that it\n",
    "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
    "# can be later loaded for evaluation or inference.\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        embedding_model: !ref <embedding_model>\n",
    "        classifier: !ref <classifier>\n",
    "        normalizer: !ref <mean_var_norm>\n",
    "        counter: !ref <epoch_counter>"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T22:15:22.165005Z",
     "start_time": "2024-03-21T22:15:22.151006Z"
    }
   },
   "id": "d68f18b7e21adf1e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_xvector_augmentation.py\n"
     ]
    }
   ],
   "source": [
    "%%file train_xvector_augmentation.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from speechbrain.utils import hpopt as hp\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "# Brain class for speech enhancement training\n",
    "class SpkIdBrain(sb.Brain):\n",
    "    \"\"\"Class that manages the training loop. See speechbrain.core.Brain.\"\"\"\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Runs all the computation of that transforms the input into the\n",
    "        output probabilities over the N classes.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Tensor\n",
    "            Tensor that contains the posterior probabilities over the N classes.\n",
    "        \"\"\"\n",
    "\n",
    "        # We first move the batch to the appropriate device.\n",
    "        batch = batch.to(self.device)\n",
    "        # Compute features, embeddings, and predictions\n",
    "        feats, lens = self.prepare_features(batch.sig, stage)\n",
    "        embeddings = self.modules.embedding_model(feats, lens)\n",
    "        predictions = self.modules.classifier(embeddings)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def prepare_features(self, wavs, stage):\n",
    "        \"\"\"Prepare the features for computation, including augmentation.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        wavs : tuple\n",
    "            Input signals (tensor) and their relative lengths (tensor).\n",
    "        stage : sb.Stage\n",
    "            The current stage of training.\n",
    "        \"\"\"\n",
    "        wavs, lens = wavs\n",
    "\n",
    "        # Add waveform augmentation if specified.\n",
    "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
    "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
    "\n",
    "        # Feature extraction and normalization\n",
    "        feats = self.modules.compute_features(wavs)\n",
    "        feats = self.modules.mean_var_norm(feats, lens)\n",
    "\n",
    "        return feats, lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : tensor\n",
    "            The output tensor from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        _, lens = batch.sig\n",
    "        spks, _ = batch.num_speakers_encoded\n",
    "\n",
    "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
    "            spks = self.hparams.wav_augment.replicate_labels(spks)\n",
    "            lens = self.hparams.wav_augment.replicate_labels(lens)\n",
    "\n",
    "        # Compute the cost function\n",
    "        loss = sb.nnet.losses.nll_loss(predictions, spks, lens)\n",
    "\n",
    "        # Append this batch of losses to the loss metric for easy\n",
    "        self.loss_metric.append(\n",
    "            batch.id, predictions, spks, lens, reduction=\"batch\"\n",
    "        )\n",
    "\n",
    "        # Compute classification error at test time\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, predictions, spks, lens)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\"Gets called at the beginning of each epoch.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up statistics trackers for this stage\n",
    "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=sb.nnet.losses.nll_loss\n",
    "        )\n",
    "\n",
    "        # Set up evaluation-only statistics trackers\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics = self.hparams.error_stats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\"Gets called at the end of an epoch.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage.\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "\n",
    "        # Summarize the statistics from the stage for record-keeping.\n",
    "        else:\n",
    "            stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"error\": self.error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # At the end of validation...\n",
    "        if stage == sb.Stage.VALID:\n",
    "\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch\": epoch, \"lr\": old_lr},\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats=stats,\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints,\n",
    "            if self.hparams.ckpt_enable:\n",
    "                self.checkpointer.save_and_keep_only(\n",
    "                    meta=stats, min_keys=[\"error\"]\n",
    "                )\n",
    "            hp.report_result(stats)\n",
    "\n",
    "        # We also write statistics about test data to stdout and to the logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stats,\n",
    "            )\n",
    "\n",
    "\n",
    "def dataio_prep(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "    We expect `prepare_mini_librispeech` to have been called before this,\n",
    "    so that the `train.json`, `valid.json`,  and `valid.json` manifest files\n",
    "    are available.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : dict\n",
    "        Contains two keys, \"train\" and \"valid\" that correspond\n",
    "        to the appropriate DynamicItemDataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization of the label encoder. The label encoder assigns to each\n",
    "    # of the observed label a unique index (e.g, 'spk01': 0, 'spk02': 1, ..)\n",
    "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
    "\n",
    "    # Define audio pipeline\n",
    "    @sb.utils.data_pipeline.takes(\"wav_path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav_path):\n",
    "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
    "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig, fs = torchaudio.load(wav_path)\n",
    "\n",
    "        # Resampling\n",
    "        sig = torchaudio.functional.resample(sig, fs, 16000).squeeze(0)\n",
    "        return sig\n",
    "\n",
    "    # Define label pipeline:\n",
    "    @sb.utils.data_pipeline.takes(\"num_speakers\")\n",
    "    @sb.utils.data_pipeline.provides(\"num_speakers\", \"num_speakers_encoded\")\n",
    "    def label_pipeline(num_speakers):\n",
    "        \"\"\"Defines the pipeline to process the input speaker label.\"\"\"\n",
    "        yield num_speakers\n",
    "        num_speakers_encoded = label_encoder.encode_label_torch(num_speakers)\n",
    "        yield num_speakers_encoded\n",
    "\n",
    "    # Define datasets. We also connect the dataset with the data processing\n",
    "    # functions defined above.\n",
    "    datasets = {}\n",
    "    data_info = {\n",
    "        \"train\": hparams[\"train_annotation\"],\n",
    "        \"valid\": hparams[\"valid_annotation\"],\n",
    "        \"test\": hparams[\"test_annotation\"],\n",
    "    }\n",
    "    hparams[\"dataloader_options\"][\"shuffle\"] = True\n",
    "    for dataset in data_info:\n",
    "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=data_info[dataset],\n",
    "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
    "            dynamic_items=[audio_pipeline, label_pipeline],\n",
    "            output_keys=[\"id\", \"sig\", \"num_speakers_encoded\"],\n",
    "        )\n",
    "\n",
    "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
    "    # Please, take a look into the lab_enc_file to see the label to index\n",
    "    # mapping.\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[datasets[\"train\"]],\n",
    "        output_key=\"num_speakers\",\n",
    "    )\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Recipe begins!\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with hp.hyperparameter_optimization(objective_key=\"error\") as hp_ctx:\n",
    "\n",
    "        # Reading command line arguments\n",
    "        hparams_file, run_opts, overrides = hp_ctx.parse_arguments(\n",
    "            sys.argv[1:], pass_trial_id=False\n",
    "        )\n",
    "\n",
    "        # Initialize ddp (useful only for multi-GPU DDP training).\n",
    "        # sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "        # Load hyperparameters file with command-line overrides.\n",
    "        with open(hparams_file) as fin:\n",
    "            hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "        # Create experiment directory\n",
    "        sb.create_experiment_directory(\n",
    "            experiment_directory=hparams[\"output_folder\"],\n",
    "            hyperparams_to_save=hparams_file,\n",
    "            overrides=overrides,\n",
    "        )\n",
    "\n",
    "        # # Data preparation, to be run on only one process.\n",
    "        # if not hparams[\"skip_prep\"]:\n",
    "        #     sb.utils.distributed.run_on_main(\n",
    "        #         kwargs={\n",
    "        #             \"data_folder\": hparams[\"data_folder\"],\n",
    "        #             \"save_json_train\": hparams[\"train_annotation\"],\n",
    "        #             \"save_json_valid\": hparams[\"valid_annotation\"],\n",
    "        #             \"save_json_test\": hparams[\"test_annotation\"],\n",
    "        #             \"split_ratio\": hparams[\"split_ratio\"],\n",
    "        #         },\n",
    "        #     )\n",
    "        # sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
    "\n",
    "        # Create dataset objects \"train\", \"valid\", and \"test\".\n",
    "        datasets = dataio_prep(hparams)\n",
    "\n",
    "        # Initialize the Brain object to prepare for mask training.\n",
    "        spk_id_brain = SpkIdBrain(\n",
    "            modules=hparams[\"modules\"],\n",
    "            opt_class=hparams[\"opt_class\"],\n",
    "            hparams=hparams,\n",
    "            run_opts=run_opts,\n",
    "            checkpointer=hparams[\"checkpointer\"],\n",
    "        )\n",
    "\n",
    "        # The `fit()` method iterates the training loop, calling the methods\n",
    "        # necessary to update the parameters of the model. Since all objects\n",
    "        # with changing state are managed by the Checkpointer, training can be\n",
    "        # stopped at any point, and will be resumed on next call.\n",
    "        spk_id_brain.fit(\n",
    "            epoch_counter=spk_id_brain.hparams.epoch_counter,\n",
    "            train_set=datasets[\"train\"],\n",
    "            valid_set=datasets[\"valid\"],\n",
    "            train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "            valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "        )\n",
    "        if not hp_ctx.enabled:\n",
    "            # Load the best checkpoint for evaluation\n",
    "            test_stats = spk_id_brain.evaluate(\n",
    "                test_set=datasets[\"test\"],\n",
    "                min_key=\"error\",\n",
    "                test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T22:15:22.181005Z",
     "start_time": "2024-03-21T22:15:22.166006Z"
    }
   },
   "id": "580f3168852615e5",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T22:15:22.245065Z",
     "start_time": "2024-03-21T22:15:22.182006Z"
    }
   },
   "id": "b5fb738a26e144aa",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !python train_xvector_augmentation.py hparams_xvector_augmentation.yaml"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T22:16:29.432046Z",
     "start_time": "2024-03-21T22:16:29.420046Z"
    }
   },
   "id": "c79599295cd070f6",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/XVector/Augmented/1986/train_log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 15\u001B[0m\n\u001B[0;32m     11\u001B[0m learning_rates \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     13\u001B[0m pattern \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: (\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+), lr: (\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+e-\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m) - train loss: (\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+e-\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m) - valid loss: (\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+e-\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m), valid error: (\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+e-\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlog_file_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m file:\n\u001B[0;32m     17\u001B[0m         match \u001B[38;5;241m=\u001B[39m pattern\u001B[38;5;241m.\u001B[39mmatch(line)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ConversationalAI\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './results/XVector/Augmented/1986/train_log.txt'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "log_file_path = './results/XVector/Augmented/1986/train_log.txt'\n",
    "# log_file_path = './results_backup_reduceonplateau/TIMIT_tiny/Xvector/FBanks/1986/train_log.txt'\n",
    "\n",
    "epochs = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_errors = []\n",
    "learning_rates = []\n",
    "\n",
    "pattern = re.compile(r'Epoch: (\\d+), lr: (\\d\\.\\d+e-\\d{2}) - train loss: (\\d\\.\\d+e-\\d{2}) - valid loss: (\\d\\.\\d+e-\\d{2}), valid error: (\\d\\.\\d+e-\\d{2})')\n",
    "\n",
    "with open(log_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            epoch, lr, train_loss, valid_loss, valid_error = match.groups()\n",
    "            epochs.append(int(epoch))\n",
    "            learning_rates.append(float(lr))\n",
    "            train_losses.append(float(train_loss))\n",
    "            valid_losses.append(float(valid_loss))\n",
    "            valid_errors.append(float(valid_error))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Train Loss', marker='o', linestyle='-')\n",
    "plt.plot(epochs, valid_losses, label='Valid Loss', marker='x', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T00:12:49.619980Z",
     "start_time": "2024-03-28T00:12:48.644106Z"
    }
   },
   "id": "753b4700a1a18eac",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T17:12:32.050716Z",
     "start_time": "2024-03-22T17:12:32.036188Z"
    }
   },
   "id": "8e5baaff9b4d5efb",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "90074009f1db81b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
