{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3da702e422e4418a8f6f5462d5c4116d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f5d7f306e3d47bb9ae4c47e381cabc4",
              "IPY_MODEL_3cefde077edd4cbab5602519a6548b5d",
              "IPY_MODEL_1c9a46cbf98f4a1fa9b71eed84b32aab"
            ],
            "layout": "IPY_MODEL_1d86196e5cb742aba6b04cd5dee7eb2e"
          }
        },
        "4f5d7f306e3d47bb9ae4c47e381cabc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb359e286b594b4eb312c63ff94b78e8",
            "placeholder": "​",
            "style": "IPY_MODEL_592a5eaeddf24442b907f26e4118a69a",
            "value": "config.json: 100%"
          }
        },
        "3cefde077edd4cbab5602519a6548b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d65d80495b4bd0a759579ee4b81f6d",
            "max": 1842,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e85128f3bacb48ff9ee5e9bff9b9cb2d",
            "value": 1842
          }
        },
        "1c9a46cbf98f4a1fa9b71eed84b32aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_094e33fe2f8743108151cfdecd00ac56",
            "placeholder": "​",
            "style": "IPY_MODEL_478b951a518f415a88b2ded140cada61",
            "value": " 1.84k/1.84k [00:00&lt;00:00, 73.6kB/s]"
          }
        },
        "1d86196e5cb742aba6b04cd5dee7eb2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb359e286b594b4eb312c63ff94b78e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592a5eaeddf24442b907f26e4118a69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73d65d80495b4bd0a759579ee4b81f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e85128f3bacb48ff9ee5e9bff9b9cb2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "094e33fe2f8743108151cfdecd00ac56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "478b951a518f415a88b2ded140cada61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9927672340e44352887d7a8586ec8a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ee6c4e92c4848d88cb270b385404e0d",
              "IPY_MODEL_1458cc2962054689b690f263ecdcd5de",
              "IPY_MODEL_7d57ab510da44e299c3ff563c7df2fb5"
            ],
            "layout": "IPY_MODEL_1672c66a588546bfac6b4f860e9c8b31"
          }
        },
        "6ee6c4e92c4848d88cb270b385404e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5240f334961a4ba98481776571711d6f",
            "placeholder": "​",
            "style": "IPY_MODEL_1eda62223f6a4649832f68e6b20bf0ab",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "1458cc2962054689b690f263ecdcd5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa8043e4e6344cbc90df63d7ceb44393",
            "max": 380267417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f18b9f474bac46629cd394ba08be7b7e",
            "value": 380267417
          }
        },
        "7d57ab510da44e299c3ff563c7df2fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2960ff8c1d7d493fa51f3a4910b2965a",
            "placeholder": "​",
            "style": "IPY_MODEL_c95c9bdaea004a95973cf168b6b1a266",
            "value": " 380M/380M [00:01&lt;00:00, 240MB/s]"
          }
        },
        "1672c66a588546bfac6b4f860e9c8b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5240f334961a4ba98481776571711d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eda62223f6a4649832f68e6b20bf0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa8043e4e6344cbc90df63d7ceb44393": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f18b9f474bac46629cd394ba08be7b7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2960ff8c1d7d493fa51f3a4910b2965a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c95c9bdaea004a95973cf168b6b1a266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bea0ce075e249a38e4a4133b418eb27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a5c4a0eeeee44248ee0b34346bb7946",
              "IPY_MODEL_659971f7949a402fbca383ad1b8d47eb",
              "IPY_MODEL_d8a646470bb24cb6990d3e72bf7f6921"
            ],
            "layout": "IPY_MODEL_b10a446ee4674611bd2ae922233a8ba9"
          }
        },
        "7a5c4a0eeeee44248ee0b34346bb7946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b23f39079f02425a8c8de43896afb3d3",
            "placeholder": "​",
            "style": "IPY_MODEL_a11570c496b24b90a07c53a9fa824268",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "659971f7949a402fbca383ad1b8d47eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dffc9a1f8b54401b897bc131f84a60a1",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ced59e4df78c450493270357dcbc30c4",
            "value": 159
          }
        },
        "d8a646470bb24cb6990d3e72bf7f6921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c942bdbdabf24e0e888e0a22e9fed218",
            "placeholder": "​",
            "style": "IPY_MODEL_8270f4a44a8544299f157e4315e6be48",
            "value": " 159/159 [00:00&lt;00:00, 5.25kB/s]"
          }
        },
        "b10a446ee4674611bd2ae922233a8ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23f39079f02425a8c8de43896afb3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a11570c496b24b90a07c53a9fa824268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dffc9a1f8b54401b897bc131f84a60a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced59e4df78c450493270357dcbc30c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c942bdbdabf24e0e888e0a22e9fed218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8270f4a44a8544299f157e4315e6be48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/main/ConvAI_Project_submission_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Project 7: Speaker Counter and Overlap Detector](#scrollTo=TZzv25vDL9LQ)\n",
        "  - [Abstract](#scrollTo=MP7fynuIMGlg)\n",
        "  - [Introduction](#scrollTo=wBGDBctaMYYQ)\n",
        "  - [Methodology](#scrollTo=XtzOOJDnMpWp)\n",
        "    - [Preparation & Preprocessing](#scrollTo=XtzOOJDnMpWp)\n",
        "    - [Data Augmentation](#scrollTo=XtzOOJDnMpWp)\n",
        "    - [Model Development and Optimization](#scrollTo=XtzOOJDnMpWp)\n",
        "      - [ECAPA TDNN](#scrollTo=XtzOOJDnMpWp)\n",
        "      - [XVector](#scrollTo=XtzOOJDnMpWp)\n",
        "      - [Selfsupervised - MLP (Multi-Layer Perceptron)](#scrollTo=XtzOOJDnMpWp)\n",
        "      - [Selfsupervised XVector](#scrollTo=XtzOOJDnMpWp)\n",
        "  - [Experimental Setup](#scrollTo=YIQOcLeaPq3v)\n",
        "    - [Hyperparameters Used](#scrollTo=YIQOcLeaPq3v)\n",
        "  - [Model Performance Analysis - Results](#scrollTo=BRL5KR20QWKu)\n",
        "    - [X-Vector Model](#scrollTo=BRL5KR20QWKu)\n",
        "    - [ECAPA-TDNN Model](#scrollTo=BRL5KR20QWKu)\n",
        "    - [Self-Supervised MLP Model](#scrollTo=BRL5KR20QWKu)\n",
        "    - [Self-Supervised X-Vector Model](#scrollTo=BRL5KR20QWKu)\n",
        "    - [Classwise Error Rate](#scrollTo=BRL5KR20QWKu)\n",
        "  - [Setup and Training Instructions](#scrollTo=OTaD2SZ2cvsR)\n",
        "  - [Inference Interface](#scrollTo=vpsZKEbeko19)\n",
        "  - [Conclusions](#scrollTo=b4Jyn3BcQDpf)\n",
        "  - [References](#scrollTo=yaxqlm6kRcmb)"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "E0d6HODOSHIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 7: Speaker Counter and Overlap Detector**"
      ],
      "metadata": {
        "id": "TZzv25vDL9LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Abstract**\n",
        "\n",
        "This project addresses the challenge of accurately counting speakers in meeting recordings where speech may overlap. This is essential for improving the accuracy of automated meeting transcriptions. To generate realistic training data, a simulator was developed that combines clean speech (LibriSpeech-clean-100) with noise and reverberation effects (Open-RIR dataset).\n",
        "\n",
        "Two established speaker recognition models (x-vector and ECAPA-TDNN) were tested alongside a novel approach. This new method integrated a pretrained Wav2Vec 2.0 model with a linear classifier and XVector. The system analyzes short audio segments, providing timestamps and the detected number of speakers.\n",
        "\n",
        "Crucially, the Wav2Vec 2.0 hybrid model significantly outperformed the other approaches. This demonstrates its power in handling complex meeting environments.  This work pushes the boundaries of speaker counting technology and offers a valuable tool for the SpeechBrain project, ultimately benefiting a wide range of speech-related applications."
      ],
      "metadata": {
        "id": "MP7fynuIMGlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "Speaker diarization, a critical component in speech processing, identifies \"who spoke when\" within audio recordings. One of the primary challenges in speaker diarization arises from overlapping speech, where multiple individuals speak simultaneously, often leading to errors in speaker identification. Traditional clustering-based diarization methods typically falter under these conditions, resulting in mislabelled speakers.\n",
        "\n",
        "To mitigate these issues, contemporary project has focused on the integration of a speaker counting module within the diarization framework. This module, by estimating the number of speakers active in any audio segment, enhances the diarization system's capability to accurately segment and label overlapping speech segments.\n",
        "\n",
        "The current project advances this methodology, aiming to engineer a resilient system for the detection and enumeration of speakers in conditions of overlapping dialogue. Drawing on the methodologies presented in \"Overlapped Speech Detection and Speaker Counting using Distant Microphone Arrays\", this project utilizes a data simulation process that amalgamates clean speech data from the LibriSpeech-Clean-100 corpus with artificial noise and reverberation from the Open-RIR dataset, thereby generating realistic scenarios for training.\n",
        "\n",
        "The investigation explores various models for counting speakers, including established techniques such as x-vectors and ECAPA-TDNN, and introduces an innovative method that employs a pre-trained self-supervised Wav2Vec 2.0 model. This proposed system processes audio recordings in brief segments, ascertains the number of speakers, and records the findings with corresponding timestamps.\n",
        "\n",
        "Preliminary results indicate that this hybrid approach, which combines the strengths of self-supervised learning with traditional speaker embedding techniques, shows promise in surpassing traditional diarization models. This finding is supported by project illustrated in \"Count And Separate: Incorporating Speaker Counting For Continuous Speaker Separation\", highlighting the significant advantages of integrating speaker counting into complex speech processing tasks."
      ],
      "metadata": {
        "id": "wBGDBctaMYYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Methodology**\n",
        "\n",
        "My project methodology comprised several significant steps that enabled me to effectively address the complexities of speaker counting within overlapping speech segments. Here, we focus on the crucial aspects of the approach that contributed significantly to the project's success.\n",
        "\n",
        "#### **Preparation & Preprocessing**\n",
        "- Used the [LibriSpeech-Clean-100](https://www.openslr.org/12) dataset for clean speech samples and [Open-RIR](https://www.openslr.org/28/) dataset for realistic noise and reverberation, creating a challenging environment for the models. For Evaluation used [LibriSpeech-Dev-Clean](http://www.openslr.org/resources/12/dev-clean.tar.gz) and tested on [LibriSpeech-test-Clean](http://www.openslr.org/resources/12/test-clean.tar.gz).  Each audio file was segmented into 1-2 second clips, which were then annotated with the number of speakers present, ranging from 0 to 4.\n",
        "\n",
        "The metadata of each file will be first divided into segments based on time and stored in a JSON like the one below. These JSON metadata files will then be combined to create mixtures, and those mixtures will be transformed into .wav files, resulting in the final data structure used for training\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    \"session_0_spk_0\": {\n",
        "        \"0\": [\n",
        "            {\n",
        "                \"start\": 0,\n",
        "                \"stop\": 120,\n",
        "                \"words\": [],\n",
        "                \"file\": \"generate_silence\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"session_0_spk_1\": {\n",
        "        \"7447\": [\n",
        "            {\n",
        "                \"start\": 0,\n",
        "                \"stop\": 8.88,\n",
        "                \"words\": \"ALTHOUGH HIS MODE OF EXPRESSION WAS PECULIARLY HIS OWN HE HAD RECEIVED A STRONG IMPULSE FROM THE POPULAR MUSIC OF POLAND\",\n",
        "                \"file\": \"train-clean-100\\\\7447\\\\91187\\\\7447-91187-0015.flac\"\n",
        "            },\n",
        "            {\n",
        "                \"start\": 8.88,\n",
        "                \"stop\": 25.160000000000004,\n",
        "                \"words\": \"HAVE EXTOLLED HIM FOR THE BEAUTY OF HIS MELODIES AND HARMONIES THE EXPRESSIVENESS OF HIS MODULATIONS THE WEALTH SPONTANEITY AND LOGICAL CLEARNESS OF HIS IDEAS AND THE SUPERB ARCHITECTURE OF HIS PRODUCTIONS\",\n",
        "                \"file\": \"train-clean-100\\\\7447\\\\91186\\\\7447-91186-0036.flac\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "  .\n",
        "  .\n",
        "  .\n",
        "          \n",
        "```\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Zuz4uSL1w0f1cYsQ5CnLN3WUNqt6r8Cl\" alt=\"Image description\" width=\"500\"/>\n",
        "<figcaption>Fig.1 - Data Structure</figcaption>\n",
        "</center>\n",
        "\n",
        "Which finally looks this way before training:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"session_0_spk_0_mixture_000\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_000_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "    \"session_0_spk_0_mixture_001\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_001_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "    \"session_0_spk_0_mixture_002\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_002_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "    \"session_0_spk_0_mixture_003\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_003_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "```\n",
        "\n",
        "#### **Data Augmentation**\n",
        "- Data Augmentation played a critical role in enhancing the robustness of the models. Implemented techniques such as noise injection, varying speed, and pitch modification to ensure that the models could generalize well across different acoustic environments and speaker variations.\n",
        "\n",
        "#### **Model Development and Optimization**\n",
        "- I have developed and tested four models: X-Vector, ECAPA-TDNN, and a combination of Pretrained Wav2Vec 2.0 with MLP and with Xvectors. Each model was chosen based on its proven efficacy in related tasks such as speaker verification and identification.\n",
        "\n",
        "#### 1. ECAPA TDNN\n",
        "(Emphasized Channel Attention, Propagation and Aggregation Time Delay Neural Network)\n",
        "\n",
        "Leverages a special type of neural network layer called a Time Delay Neural Network (TDNN) to capture temporal information in speech audio.\n",
        "\n",
        "Incorporates an \"attention\" mechanism that emphasizes informative channels within the network, potentially improving speaker differentiation, especially in overlapping scenarios.\n",
        "\n",
        "Well-suited for speaker identification and verification tasks, making it a strong candidate for speaker counting as well.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1IZHqF86vornEw9Ib-N46JMVyhMZ7cctZ\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.2 - ECAPA-TDNN</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 2. XVector\n",
        "Employs a convolutional neural network (CNN) architecture to learn speaker embeddings, which are compressed representations that encode speaker identity.\n",
        "Designed for speaker identification and verification, and its strength lies in its ability to capture speaker-specific characteristics even in noisy or varying environments.\n",
        "\n",
        "This makes it a viable model for speaker counting, where identification of individual speakers often precedes counting.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1n6mRBqFzJNfzbmzQjcKQmDjZBTxr0hbN\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.3 - XVector</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 3. Selfsupervised - MLP(Multi-Layer Perceptron)\n",
        "\n",
        "Wav2Vec 2.0 is a powerful self-supervised model, meaning it learns representations from vast amounts of unlabeled speech data.\n",
        "These learned representations are highly effective at capturing intricate speech features.\n",
        "\n",
        "The MLP acts as a classifier, taking the Wav2Vec 2.0 output and mapping it to the predicted number of speakers.\n",
        "\n",
        "This approach leverages the strengths of self-supervised learning for feature extraction and a traditional MLP for classification, potentially offering robustness in speaker counting.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1enhfLUxl3v-FWZ3bLNa0tFmGLvF6VAvq\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.4 - wav2vec2 with linear classifier</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 4. Selfsupervised XVector\n",
        "Combines the power of Wav2Vec 2.0's self-supervised feature learning with X-vector's speaker identification capability.\n",
        "\n",
        "Wav2Vec 2.0 extracts rich features, and X-vectors can potentially isolate speaker-specific information within those features.\n",
        "\n",
        "This combined approach may lead to more accurate speaker counting, especially in challenging overlapping speech conditions.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1en7t4gevL-3QqA5ru6Mz7VlfpGx7cby3\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.5 - wav2vec2 with Xvector</figcaption>\n",
        "</center>"
      ],
      "metadata": {
        "id": "XtzOOJDnMpWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experimental Setup**\n",
        "\n",
        "The LibriSpeech-Clean-100 dataset was used for training, while the LibriSpeech-Dev-Clean set was used for validation, and the LibriSpeech-Test-Clean set was used for final testing. This split ensures generalization is evaluated on unseen data. All models were trained using the Adam optimizer with varying learning rates and weight decay as indicated in the table below. Experiments were conducted on a system with GTX 4050 6GB GPU.\n",
        "\n",
        "### Hyperparameters Used\n",
        "\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Hyperparams</th>\n",
        "    <th>GitHub Link</th>\n",
        "    <th>Model</th>\n",
        "    <th>Hyperparams</th>\n",
        "    <th>GitHub Link</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ECAPA-TDNN</td>\n",
        "    <td><pre><code>\n",
        "    sample_rate: 16000\n",
        "    number_of_epochs: 20\n",
        "    batch_size: 64\n",
        "    lr_start: 0.001\n",
        "    lr_final: 0.0001\n",
        "    weight_decay: 0.00002\n",
        "    num_workers: 0 # For windows or 4 for linux\n",
        "    n_classes: 5\n",
        "    dim: 192\n",
        "    num_attention_channels: 128\n",
        "    n_mels: 80\n",
        "    channels: [256, 256, 256, 256, 768]\n",
        "    kernel_sizes: [5, 3, 3, 3, 1]\n",
        "    dilations: [1, 2, 3, 4, 1]\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/ecapa_tdnn/hparams_ecapa_tdnn_augmentation.yaml\">View Complete file</a></td>\n",
        "    <td>SelfSupervised <br>\n",
        "    Linear Classifier</td>\n",
        "    <td><pre><code>\n",
        "    number_of_epochs: 5\n",
        "    batch_size: 64\n",
        "    lr: 0.001\n",
        "    lr_ssl: 0.0001\n",
        "    freeze_ssl: False\n",
        "    freeze_ssl_conv: True\n",
        "    encoder_dim: 768\n",
        "    out_n_neurons: 5\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/selfsupervised/hparams_selfsupervised_mlp.yaml\">View Complete file</a></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>XVector</td>\n",
        "    <td><pre><code>\n",
        "    sample_rate: 16000\n",
        "    number_of_epochs: 50\n",
        "    batch_size: 64\n",
        "    lr_start: 0.001\n",
        "    lr_final: 0.0001\n",
        "    weight_decay: 0.00002\n",
        "    num_workers: 0 # For windows or 4 for linux\n",
        "    n_mels: 4\n",
        "    n_classes: 5\n",
        "    emb_dim: 128\n",
        "    tdnn_channels: 64\n",
        "    tdnn_channels_out: 128\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/xvector/hparams_xvector_augmentation.yaml\">View Complete file</a></td>\n",
        "    <td>SelfSupervised <br>\n",
        "    XVector</td>\n",
        "    <td><pre><code>\n",
        "    number_of_epochs: 15\n",
        "    batch_size: 128\n",
        "    lr: 0.001\n",
        "    lr_final: 0.0001\n",
        "    lr_ssl: 0.00001\n",
        "    freeze_ssl: False\n",
        "    freeze_ssl_conv: True\n",
        "    encoder_dim: 768\n",
        "    emb_dim: 128\n",
        "    out_n_neurons: 5\n",
        "    tdnn_channels: [ 64, 64, 64 ]\n",
        "    tdnn_kernel_sizes: [ 5, 2, 3 ]\n",
        "    tdnn_dilations: [ 1, 2, 3 ]\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/selfsupervised/hparams_selfsupervised_xvector.yaml\">View Complete file</a></td>\n",
        "  </tr>\n",
        "</table></center>"
      ],
      "metadata": {
        "id": "YIQOcLeaPq3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Performance Analysis - Results**\n",
        "\n",
        "Across all models tested, the Self-supervised approaches (Wav2Vec2.0 + MLP and Wav2Vec2.0 + X-vector) demonstrated superior performance in the speaker counting task, achieving lower overall error rates compared to the ECAPA-TDNN and X-vector models.\n",
        "\n",
        "Although the specific error rates were close, the Wav2Vec2.0 + X-vector hybrid model offered a slight advantage. This advantage was particularly noticeable when handling overlapping speech from multiple speakers, as illustrated in Fig.7,9 & 12.\n",
        "\n",
        "An interesting and positive finding from the analysis was the perfect accuracy achieved by all models in identifying segments with no speakers present. This capability is crucial for any diarization system, as it ensures the accurate detection of silent intervals, preventing unnecessary processing and improving overall system efficiency.\n",
        "\n",
        "A common trend across all models was an increase in error rate as the true number of speakers increased.  This difficulty in handling three or more overlapping speakers highlights a core challenge for speaker counting systems.  The class-wise error rates [See Table 1 below] illustrate this struggle, with the highest error rates occurring for classes with 3 or 4 speakers.\n",
        "\n",
        "Notably, all models achieved perfect accuracy in identifying segments containing no speakers. This indicates strong performance in silence detection, a valuable component of speaker diarization systems.\n",
        "\n",
        "But During inference XVector and ECAPA-TDNN are much faster when compared to SelfSupervised models.\n",
        "\n",
        "#### **X-Vector Model**\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1t1VZhDyG8awIU0keoyrw54yRLETa7r9B\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.6 - XVector Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1qVfJx5yzxa_UKE3XVAveicqboy69l2kV\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.7 - Error rate of Both XVector Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "\n",
        "#### **ECAPA-TDNN Model**\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1VeKXeO3-aXcTyBz6Ztq-WH3M40njXEkq\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.8 - ECAPA-TDNN Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1TQ3qK56wQr3lxTEh9-NUh5s-NtRkqtyB\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.9 - Error rate of Both ECAPA-TDNN Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "\n",
        "#### **Self-Supervised MLP Model**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1IgCQqGb1HLbyrhYwUMPuV8fjY-Xy4aUy\" alt=\"Selfsupervised MLP Train and Valid Loss\" width=\"300\"/>\n",
        "<figcaption>Fig.10 - Self-Supervised MLP Train and Valid Loss</figcaption>\n",
        "</center>\n",
        "\n",
        "#### **Self-Supervised X-Vector Model**\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1K6vvfvfoIZShz8Y1lV-epE2waoPgdMIs\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.11 - Self-Supervised X-Vector Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1cZDKGp0VBPOE9TKhWkUXCnd7bgrXV4IQ\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.12 - Error rate of Both Selfsupervised Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "#### **Classwise Error Rate**\n",
        "<center>\n",
        "<table>\n",
        " <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Class</th>\n",
        "    <th>Error Rate</th>\n",
        "    <th>Model</th>\n",
        "    <th>Class</th>\n",
        "    <th>Error Rate</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td rowspan=\"6\">XVector</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.29e-01</td>\n",
        "    <td rowspan=\"6\">ECAPA-TDNN</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.40e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>4.06e-01</td>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>4.12e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>3.20e-02</td>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>4.10e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>2.15e-01</td>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>3.26e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>4.67e-01</td>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>5.23e-01</td>\n",
        "  </tr>\n",
        "  <tr><td colspan=\"6\"></td></tr>\n",
        "  <tr>\n",
        "    <td rowspan=\"6\">Selfsupervised MLP</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.00e-01</td>\n",
        "    <td rowspan=\"6\">Selfsupervised XVector</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.10e-01</td>\n",
        "  </tr>\n",
        "   <tr>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>8.67e-03</td>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>1.64e-02</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1.14e-01</td>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1.39e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>4.08e-01</td>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>3.34e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>4.49e-01</td>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>5.41e-01</td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>"
      ],
      "metadata": {
        "id": "BRL5KR20QWKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup and Training Instructions**\n",
        "\n",
        "To reproduce the results of this project, follow these steps.\n",
        "\n",
        "```\n",
        "!git clone https://github.com/SupradeepDanturti/ConvAIProject\n",
        "%cd ConvAIProject\n",
        "```\n",
        "Download the project code from the GitHub repository and navigate into the project directory.\n",
        "\n",
        "\n",
        "```\n",
        "!python prepare_dataset/download_required_data.py --output_folder <destination_folder_path>\n",
        "```\n",
        "Download the necessary datasets (LibriSpeech etc.), specifying the desired destination folder.\n",
        "\n",
        "```\n",
        "!python prepare_dataset/create_custom_dataset.py prepare_dataset/dataset.yaml\n",
        "```\n",
        "Create custom dataset based on set parameters as shown in the sample below\n",
        "\n",
        "Sample of dataset.yaml:\n",
        "```\n",
        "n_sessions:\n",
        "  train: 1000 # Creates 1000 sessions per class\n",
        "  dev: 200 # Creates 200 sessions per class\n",
        "  eval: 200 # Creates 200 sessions per class\n",
        "n_speakers: 4 # max number of speakers. In this case the total classes will be 5 (0-4 speakers)\n",
        "max_length: 120 # max length in seconds for each session/utterance.\n",
        "```\n",
        "<center>Sample of dataset.yaml</center>\n",
        "\n",
        "To train the XVector model run the following command.\n",
        "```\n",
        "%cd xvector\n",
        "!python train_xvector_augmentation.py hparams_xvector_augmentation.yaml\n",
        "```\n",
        "\n",
        "To train the ECAPA-TDNN model run the following command.\n",
        "\n",
        "```\n",
        "%cd ecapa_tdnn\n",
        "!python train_ecapa_tdnn.py hparams_ecapa_tdnn_augmentation.yaml\n",
        "```\n",
        "\n",
        "To train the SelfSupervised MLP model run the following command.\n",
        "```\n",
        "%cd selfsupervised\n",
        "!python selfsupervised_mlp.py hparams_selfsupervised_mlp.yaml\n",
        "```\n",
        "\n",
        "To train the SelfSupervised XVector model run the following command.\n",
        "```\n",
        "%cd selfsupervised\n",
        "!python selfsupervised_xvector.py hparams_selfsupervised_xvector.yaml\n",
        "```"
      ],
      "metadata": {
        "id": "OTaD2SZ2cvsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference Interface**\n",
        "\n",
        "To run each models inference pull the interface directory as shown in the cell below\n"
      ],
      "metadata": {
        "id": "vpsZKEbeko19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample result\n",
        "```\n",
        "0.00-1.06 has 1 speaker\n",
        "1.06-2.65 has 2 speakers\n",
        "2.65-5.30 has 1 speaker\n",
        "5.30-6.36 has 3 speakers\n",
        "6.36-7.42 has 1 speaker\n",
        "7.42-9.01 has 4 speakers\n",
        "9.01-10.07 has 1 speaker\n",
        "10.07-12.19 has 3 speakers\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1RqkPjLTr2Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "OZawEScuQ67L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\"\"\"This might not work because I reached my git lfs limits please use the google drive one mentioned below.\"\"\"\n",
        "# !git clone --filter=blob:none --no-checkout https://github.com/SupradeepDanturti/ConvAIProject\n",
        "# %cd ConvAIProject\n",
        "# !git sparse-checkout init --cone\n",
        "# !git sparse-checkout set interface\n",
        "# !git checkout\n",
        "\n",
        "\"\"\" Download from google drive \"\"\"\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown 1oVQuzHXNNPNxQ6WqR0mUEvMptx15SZQG\n",
        "!unzip interface.zip"
      ],
      "metadata": {
        "id": "zU93F9O8Pxfg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the inference for XVector or ECAPA-TDNN Models run the run_inference_xvector_ecapa_tdnn.py file or run the cell below."
      ],
      "metadata": {
        "id": "4nL5i9fEQB21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "PkpZRymCQ0R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fc8e80-f47a-4922-98bf-63899aaae1f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This is used for Both XVector and ECAPA-TDNN \"\"\"\n",
        "\n",
        "from interface.SpeakerCounter import SpeakerCounter\n",
        "\n",
        "wav_path = \"interface/sample_audio1.wav\"  # Path to your audio file\n",
        "save_dir = \"interface/sample_inference_run2/\" # Where to save results\n",
        "model_path = \"interface/xvector\" # /ecapa_tdnn  # Path of the trained model\n",
        "\n",
        "# Create classifier object\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "# Run inference on the audio file\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "id": "a_R3QP5cQhPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f569111d-74b3-4730-dc7e-35f0fdf0bd19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-0.53 has 1 speaker\n",
            "0.53-4.77 has 2 speakers\n",
            "4.77-65.72 has 1 speaker\n",
            "65.72-66.78 has 2 speakers\n",
            "66.78-85.86 has 1 speaker\n",
            "85.86-86.92 has 2 speakers\n",
            "86.92-87.45 has 1 speaker\n",
            "87.45-91.16 has 2 speakers\n",
            "91.16-94.34 has 1 speaker\n",
            "94.34-94.87 has 4 speakers\n",
            "94.87-95.40 has 2 speakers\n",
            "95.40-103.35 has 1 speaker\n",
            "103.35-104.94 has 4 speakers\n",
            "104.94-106.53 has 1 speaker\n",
            "106.53-107.06 has 4 speakers\n",
            "107.06-107.59 has 3 speakers\n",
            "107.59-108.65 has 1 speaker\n",
            "108.65-109.18 has 2 speakers\n",
            "109.18-110.24 has 4 speakers\n",
            "110.24-115.54 has 1 speaker\n",
            "115.54-117.13 has 4 speakers\n",
            "117.13-123.49 has 1 speaker\n",
            "123.49-124.02 has 2 speakers\n",
            "124.02-128.79 has 1 speaker\n",
            "128.79-129.32 has 3 speakers\n",
            "129.32-129.85 has 1 speaker\n",
            "129.85-130.38 has 3 speakers\n",
            "130.38-130.91 has 4 speakers\n",
            "130.91-131.97 has 3 speakers\n",
            "131.97-133.03 has 1 speaker\n",
            "133.03-133.56 has 3 speakers\n",
            "133.56-134.62 has 4 speakers\n",
            "134.62-136.21 has 1 speaker\n",
            "136.21-136.74 has 4 speakers\n",
            "136.74-137.27 has 2 speakers\n",
            "137.27-146.81 has 1 speaker\n",
            "146.81-148.40 has 2 speakers\n",
            "148.40-149.99 has 3 speakers\n",
            "149.99-151.05 has 2 speakers\n",
            "151.05-190.27 has 1 speaker\n",
            "190.27-190.80 has 4 speakers\n",
            "190.80-206.17 has 1 speaker\n",
            "206.17-206.70 has 2 speakers\n",
            "206.70-217.83 has 1 speaker\n",
            "217.83-218.89 has 2 speakers\n",
            "218.89-219.95 has 3 speakers\n",
            "219.95-224.72 has 2 speakers\n",
            "224.72-233.73 has 1 speaker\n",
            "233.73-234.26 has 4 speakers\n",
            "234.26-299.33 has 1 speaker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the inference for Selfsupervised Models run the run_inference_selfsupervised.py file or run the cell below.\n",
        "\n",
        "Note- This usually takes a bit more time than the XVector or the ECAPA-TDNN Models but given more accurate result. Which is the only disadvantage it has."
      ],
      "metadata": {
        "id": "QQiGZUdGS4t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from interface.SpeakerCounterSelfsupervisedMLP import SpeakerCounter\n",
        "\n",
        "wav_path = \"interface/sample_audio1.wav\" # Path to your audio file\n",
        "save_dir = \"interface/sample_inference_run\" # Where to save results\n",
        "model_path = \"interface/selfsupervised_mlp\" # Path of the trained model\n",
        "\n",
        "# Create classifier object\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "# Run inference on the audio file\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "id": "2yC62PIRSj1Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "3da702e422e4418a8f6f5462d5c4116d",
            "4f5d7f306e3d47bb9ae4c47e381cabc4",
            "3cefde077edd4cbab5602519a6548b5d",
            "1c9a46cbf98f4a1fa9b71eed84b32aab",
            "1d86196e5cb742aba6b04cd5dee7eb2e",
            "bb359e286b594b4eb312c63ff94b78e8",
            "592a5eaeddf24442b907f26e4118a69a",
            "73d65d80495b4bd0a759579ee4b81f6d",
            "e85128f3bacb48ff9ee5e9bff9b9cb2d",
            "094e33fe2f8743108151cfdecd00ac56",
            "478b951a518f415a88b2ded140cada61",
            "9927672340e44352887d7a8586ec8a88",
            "6ee6c4e92c4848d88cb270b385404e0d",
            "1458cc2962054689b690f263ecdcd5de",
            "7d57ab510da44e299c3ff563c7df2fb5",
            "1672c66a588546bfac6b4f860e9c8b31",
            "5240f334961a4ba98481776571711d6f",
            "1eda62223f6a4649832f68e6b20bf0ab",
            "fa8043e4e6344cbc90df63d7ceb44393",
            "f18b9f474bac46629cd394ba08be7b7e",
            "2960ff8c1d7d493fa51f3a4910b2965a",
            "c95c9bdaea004a95973cf168b6b1a266",
            "5bea0ce075e249a38e4a4133b418eb27",
            "7a5c4a0eeeee44248ee0b34346bb7946",
            "659971f7949a402fbca383ad1b8d47eb",
            "d8a646470bb24cb6990d3e72bf7f6921",
            "b10a446ee4674611bd2ae922233a8ba9",
            "b23f39079f02425a8c8de43896afb3d3",
            "a11570c496b24b90a07c53a9fa824268",
            "dffc9a1f8b54401b897bc131f84a60a1",
            "ced59e4df78c450493270357dcbc30c4",
            "c942bdbdabf24e0e888e0a22e9fed218",
            "8270f4a44a8544299f157e4315e6be48"
          ]
        },
        "outputId": "acb31fb4-ac6d-4360-e6eb-50a69ac9e665"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3da702e422e4418a8f6f5462d5c4116d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9927672340e44352887d7a8586ec8a88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bea0ce075e249a38e4a4133b418eb27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:speechbrain.lobes.models.huggingface_transformers.wav2vec2:speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-0.53 has 1 speaker\n",
            "0.53-4.77 has 2 speakers\n",
            "4.77-65.72 has 1 speaker\n",
            "65.72-67.31 has 2 speakers\n",
            "67.31-85.33 has 1 speaker\n",
            "85.33-90.63 has 2 speakers\n",
            "90.63-146.81 has 1 speaker\n",
            "146.81-151.58 has 2 speakers\n",
            "151.58-217.83 has 1 speaker\n",
            "217.83-224.72 has 2 speakers\n",
            "224.72-299.33 has 1 speaker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions**\n",
        "\n",
        "The project's investigation into the development of a speaker counting and overlap detection system has led to several important conclusions and insights. The utilization of various modeling approaches such as ECAPA-TDNN, X-Vector, and innovative self-supervised techniques involving Wav2Vec 2.0, have each contributed differently to the system's overall performance and efficiency.\n",
        "\n",
        "**Model Performance**: The Self-Supervised models leveraging Wav2Vec 2.0 technology demonstrated a significant improvement over traditional models (ECAPA-TDNN and X-Vector) in terms of error rates, particularly in complex auditory environments with overlapping speech. These findings confirm the advantage of integrating self-supervised learning frameworks for feature extraction in speaker diarization tasks.\n",
        "\n",
        "**Error Rates and Challenges**: Despite advancements, the project revealed increasing challenges with higher speaker counts. Error rates escalated as the number of overlapping speakers increased, indicating a persistent difficulty in distinguishing multiple simultaneous speakers. This suggests a need for further refinement in model architectures or training strategies to handle such scenarios more effectively.\n",
        "\n",
        "**Inference Speed**: While self-supervised models showed superior accuracy, they lagged in inference speed compared to traditional models like ECAPA-TDNN and X-Vector. This trade-off between accuracy and speed is crucial for real-time applications, implying that depending on the use case, a balance might need to be struck between the two metrics.\n",
        "\n",
        "**Data Augmentation**: The use of a data augmentation strategy involving noise injection, speed variation, and pitch modification proved beneficial in enhancing model robustness. This approach allowed the models to generalize better across diverse acoustic settings and speaker variations, which is vital for practical deployments.\n",
        "\n",
        "**Limitations**: The project encountered limitations in handling extreme noise conditions and highly dynamic speech activities, which occasionally led to misclassifications. These aspects underscore the necessity for ongoing research and adaptation of the models to accommodate a broader spectrum of real-world conditions."
      ],
      "metadata": {
        "id": "b4Jyn3BcQDpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**\n",
        "You can add here the citations of books, websites, or academic papers, etc.\n",
        "\n",
        "[1] Cornella, S., Omologo, M., Squartini, S., & Vincent, E. (2020). Overlapped Speech Detection and Speaker Counting using Distant Microphone Arrays.\n",
        "\n",
        "[2] Duong, T. T. H., Nguyen, P. L., Nguyen, H. S., & Duong, N. Q. K. (2023). Investigating the Role of Speaker Counter in Handling Overlapping Speeches in Speaker Diarization Systems. Authorea.\n",
        "\n",
        "[3] Wang, Z. Q., & Wang, D. (2022). Count And Separate: Incorporating Speaker Counting For Continuous Speaker Separation.\n",
        "\n",
        "[4] Andrei, V., Cucu, H., & Burileanu, C. (2020). Overlapped Speech Detection and Competing Speaker Counting Humans Versus Deep Learning. IEEE.\n",
        "\n",
        "[5] Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J., Yeh, S., Fu, S., Bengio, Y., & Mohamed, C. (2021). SpeechBrain: A General-Purpose Speech Toolkit. arXiv:2106.04624."
      ],
      "metadata": {
        "id": "yaxqlm6kRcmb"
      }
    }
  ]
}