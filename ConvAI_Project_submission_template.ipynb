{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a849787f1114ee8b22141cf9507ae86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bee48e64bfe4a96895e1fba19d8fcd0",
              "IPY_MODEL_ddc0ecc0d57743c99c7a9be1806e17e9",
              "IPY_MODEL_71cf94d187f94bc9be7217706eacd1d8"
            ],
            "layout": "IPY_MODEL_5a0535cb94fc42b98acf73976cc4bcb9"
          }
        },
        "7bee48e64bfe4a96895e1fba19d8fcd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0303c0d7bdec4034bf2b6597df4ef3b2",
            "placeholder": "​",
            "style": "IPY_MODEL_885c30b1204f4f94a1fc2c5c49f9011e",
            "value": "config.json: 100%"
          }
        },
        "ddc0ecc0d57743c99c7a9be1806e17e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0074e8023dce4bd7b638938022c3fec0",
            "max": 1842,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37ae1fc55f4e4b84a5af73045b9208df",
            "value": 1842
          }
        },
        "71cf94d187f94bc9be7217706eacd1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2193e99bc188483491212064b70c8d4e",
            "placeholder": "​",
            "style": "IPY_MODEL_306df962c1e84b6aa9906f578e0c8bc1",
            "value": " 1.84k/1.84k [00:00&lt;00:00, 86.0kB/s]"
          }
        },
        "5a0535cb94fc42b98acf73976cc4bcb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0303c0d7bdec4034bf2b6597df4ef3b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885c30b1204f4f94a1fc2c5c49f9011e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0074e8023dce4bd7b638938022c3fec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ae1fc55f4e4b84a5af73045b9208df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2193e99bc188483491212064b70c8d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "306df962c1e84b6aa9906f578e0c8bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9292e26dd704c6fb6c988c9c6b8587a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf1364cfff2b46aebe7f6d08c90c81f3",
              "IPY_MODEL_2dbf9263aeed4df2835e6529d4bc66ba",
              "IPY_MODEL_1b18b2bd31ca483eb7d1e6ae360e98e4"
            ],
            "layout": "IPY_MODEL_8b15b8c2dbd84ddc8d633682cb4f7d79"
          }
        },
        "bf1364cfff2b46aebe7f6d08c90c81f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94fc2bc39a1b4cf38eb02dc3505acf95",
            "placeholder": "​",
            "style": "IPY_MODEL_5d902d98bc29484fac62825cdfdf714a",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "2dbf9263aeed4df2835e6529d4bc66ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b50cbd7540614769aae8c47cfa6d1791",
            "max": 380267417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff5da714da294944876133f306100f3a",
            "value": 380267417
          }
        },
        "1b18b2bd31ca483eb7d1e6ae360e98e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bc156a32d0c4f40b206ef6cd17d2cf5",
            "placeholder": "​",
            "style": "IPY_MODEL_e5b38f6f9e8b47038b7ed28b4a6eb3bb",
            "value": " 380M/380M [00:03&lt;00:00, 104MB/s]"
          }
        },
        "8b15b8c2dbd84ddc8d633682cb4f7d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94fc2bc39a1b4cf38eb02dc3505acf95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d902d98bc29484fac62825cdfdf714a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b50cbd7540614769aae8c47cfa6d1791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5da714da294944876133f306100f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bc156a32d0c4f40b206ef6cd17d2cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b38f6f9e8b47038b7ed28b4a6eb3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "810d3856983d4519b62f9fad3f0c243c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31ac56fc3d62428493a23419fb65e6e2",
              "IPY_MODEL_4d5b6f0595c249448a4f5d639835c6f1",
              "IPY_MODEL_39388634e7c145aa8f32c2494c606460"
            ],
            "layout": "IPY_MODEL_8182979f9aa54fed96e93cd9161a1a41"
          }
        },
        "31ac56fc3d62428493a23419fb65e6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fee159435aa44ffab7ce098fb4f3d8a",
            "placeholder": "​",
            "style": "IPY_MODEL_3caa1c99f6814a00a25df737719b4b37",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "4d5b6f0595c249448a4f5d639835c6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75d3ab02ab6c4942a46cc64368115d26",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5814ff5d0f6f4cd78eb9620eba2fda2e",
            "value": 159
          }
        },
        "39388634e7c145aa8f32c2494c606460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb4bdf78f383432ca2a3ecfeda2f495c",
            "placeholder": "​",
            "style": "IPY_MODEL_fb7be5df48f04188a4399dbd1fdc73c4",
            "value": " 159/159 [00:00&lt;00:00, 4.74kB/s]"
          }
        },
        "8182979f9aa54fed96e93cd9161a1a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fee159435aa44ffab7ce098fb4f3d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3caa1c99f6814a00a25df737719b4b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75d3ab02ab6c4942a46cc64368115d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5814ff5d0f6f4cd78eb9620eba2fda2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb4bdf78f383432ca2a3ecfeda2f495c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb7be5df48f04188a4399dbd1fdc73c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/main/ConvAI_Project_submission_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Project 7: Speaker Counter and Overlap Detector](#scrollTo=TZzv25vDL9LQ)\n",
        "\n",
        ">>[Abstract](#scrollTo=MP7fynuIMGlg)\n",
        "\n",
        ">>[Introduction](#scrollTo=wBGDBctaMYYQ)\n",
        "\n",
        ">>[Methodology](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Preparation & Preprocessing](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Data Augmentation](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Model Development and Optimization](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[ECAPA TDNN](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[XVector](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Selfsupervised - MLP](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Selfsupervised XVector](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Original Aspects and Relation to Existing Work](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>[Experimental Setup](#scrollTo=YIQOcLeaPq3v)\n",
        "\n",
        ">>>[Model Details](#scrollTo=YIQOcLeaPq3v)\n",
        "\n",
        ">>[Model Performance Analysis - Results](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[X-Vector Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[ECAPA-TDNN Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[Self-Supervised MLP Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[Self-Supervised X-Vector Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[Classwise Error Rate](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>[Steps Download, Create and Train all models](#scrollTo=OTaD2SZ2cvsR)\n",
        "\n",
        ">>[Inference Interface](#scrollTo=vpsZKEbeko19)\n",
        "\n",
        ">>[Conclusions](#scrollTo=b4Jyn3BcQDpf)\n",
        "\n",
        ">>[References](#scrollTo=yaxqlm6kRcmb)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "E0d6HODOSHIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 7: Speaker Counter and Overlap Detector**"
      ],
      "metadata": {
        "id": "TZzv25vDL9LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Abstract**\n",
        "\n",
        "This project tackles the challenge of counting speakers in meeting recordings with overlapping speech, a crucial task for improving automated meeting transcriptions. A data simulator was developed, combining clean speech from LibriSpeech-clean-100 with noise and reverberation (Open-RIR dataset) to mimic realistic meeting conditions.  Two established models (x-vector and ECAPA-TDNN) were evaluated alongside a novel approach integrating a pretrained Wav2Vec 2.0 model with a linear classifier and XVector. The system analyzes short audio segments, providing timestamps and speaker counts. Results demonstrate that the Wav2Vec 2.0 hybrid model significantly outperforms the benchmarks, offering a robust tool for speaker counting in complex environments. This work advances the field and contributes a valuable asset to the SpeechBrain project.\n",
        "\n",
        "To create training data, I developed a simulator that combines clean speech from the LibriSpeech-Clean-100 dataset with noise and reverberation from the Open-RIR dataset, based on a specified probability. I evaluated two conventional models, the X-Vector and ECAPA-TDNN, along with a novel approach that integrates a pretrained self-supervised Wav2Vec 2.0 model with a linear classifier and XVector. Each model processed recordings in 1-2 second chunks to determine the number of speakers present, cataloging the results alongside timestamps in a text file format.\n",
        "\n",
        "The findings reveal that the hybrid Wav2Vec 2.0 and linear classifier model significantly outperforms traditional models, achieving lower error rates in distinguishing between zero to three or more speakers. This research not only advances the capability of speaker counting in complex auditory environments but also contributes a robust tool to the SpeechBrain project, enhancing its utility for diverse speech technology applications."
      ],
      "metadata": {
        "id": "MP7fynuIMGlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "Speaker diarization, a critical component in speech processing, identifies \"who spoke when\" within audio recordings. One of the primary challenges in speaker diarization arises from overlapping speech, where multiple individuals speak simultaneously, often leading to errors in speaker identification. Traditional clustering-based diarization methods typically falter under these conditions, resulting in mislabelled speakers.\n",
        "\n",
        "To mitigate these issues, contemporary research has focused on the integration of a speaker counting module within the diarization framework. This module, by estimating the number of speakers active in any audio segment, enhances the diarization system's capability to accurately segment and label overlapping speech segments.\n",
        "\n",
        "The current project advances this methodology, aiming to engineer a resilient system for the detection and enumeration of speakers in conditions of overlapping dialogue. Drawing on the methodologies presented in \"Overlapped Speech Detection and Speaker Counting using Distant Microphone Arrays\", this project utilizes a data simulation process that amalgamates clean speech data from the LibriSpeech-Clean-100 corpus with artificial noise and reverberation from the Open-RIR dataset, thereby generating realistic scenarios for training.\n",
        "\n",
        "The investigation explores various models for counting speakers, including established techniques such as x-vectors and ECAPA-TDNN, and introduces an innovative method that employs a pre-trained self-supervised Wav2Vec 2.0 model. This proposed system processes audio recordings in brief segments, ascertains the number of speakers, and records the findings with corresponding timestamps.\n",
        "\n",
        "Preliminary results indicate that this hybrid approach, which combines the strengths of self-supervised learning with traditional speaker embedding techniques, shows promise in surpassing traditional diarization models. This finding is supported by research illustrated in \"Count And Separate: Incorporating Speaker Counting For Continuous Speaker Separation\", highlighting the significant advantages of integrating speaker counting into complex speech processing tasks."
      ],
      "metadata": {
        "id": "wBGDBctaMYYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Methodology**\n",
        "\n",
        "My project methodology comprised several significant steps that enabled me to effectively address the complexities of speaker counting within overlapping speech segments. Here, we focus on the crucial aspects of the approach that contributed significantly to the project's success.\n",
        "\n",
        "#### **Preparation & Preprocessing**\n",
        "- Used the [LibriSpeech-Clean-100](https://www.openslr.org/12) dataset for clean speech samples and [Open-RIR](https://www.openslr.org/28/) dataset for realistic noise and reverberation, creating a challenging environment for the models. For Evaluation used [LibriSpeech-Dev-Clean](http://www.openslr.org/resources/12/dev-clean.tar.gz) and tested on [LibriSpeech-test-Clean](http://www.openslr.org/resources/12/test-clean.tar.gz).  Each audio file was segmented into 1-2 second clips, which were then annotated with the number of speakers present, ranging from 0 to 4.\n",
        "\n",
        "The metadata of each file will be first divided into segments based on time and stored in a JSON like the one below. These JSON metadata files will then be combined to create mixtures, and those mixtures will be transformed into .wav files, resulting in the final data structure used for training\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    \"session_0_spk_0\": {\n",
        "        \"0\": [\n",
        "            {\n",
        "                \"start\": 0,\n",
        "                \"stop\": 120,\n",
        "                \"words\": [],\n",
        "                \"file\": \"generate_silence\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"session_0_spk_1\": {\n",
        "        \"7447\": [\n",
        "            {\n",
        "                \"start\": 0,\n",
        "                \"stop\": 8.88,\n",
        "                \"words\": \"ALTHOUGH HIS MODE OF EXPRESSION WAS PECULIARLY HIS OWN HE HAD RECEIVED A STRONG IMPULSE FROM THE POPULAR MUSIC OF POLAND\",\n",
        "                \"file\": \"train-clean-100\\\\7447\\\\91187\\\\7447-91187-0015.flac\"\n",
        "            },\n",
        "            {\n",
        "                \"start\": 8.88,\n",
        "                \"stop\": 25.160000000000004,\n",
        "                \"words\": \"HAVE EXTOLLED HIM FOR THE BEAUTY OF HIS MELODIES AND HARMONIES THE EXPRESSIVENESS OF HIS MODULATIONS THE WEALTH SPONTANEITY AND LOGICAL CLEARNESS OF HIS IDEAS AND THE SUPERB ARCHITECTURE OF HIS PRODUCTIONS\",\n",
        "                \"file\": \"train-clean-100\\\\7447\\\\91186\\\\7447-91186-0036.flac\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "  .\n",
        "  .\n",
        "  .\n",
        "          \n",
        "```\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Zuz4uSL1w0f1cYsQ5CnLN3WUNqt6r8Cl\" alt=\"Image description\" width=\"500\"/>\n",
        "<figcaption>Fig.1 - Data Structure</figcaption>\n",
        "</center>\n",
        "\n",
        "Which finally looks this way before training:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"session_0_spk_0_mixture_000\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_000_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "    \"session_0_spk_0_mixture_001\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_001_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "    \"session_0_spk_0_mixture_002\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_002_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "    \"session_0_spk_0_mixture_003\": {\n",
        "        \"wav_path\": \"../data/train/session_0_spk_0/session_0_spk_0_mixture_003_segment.wav\",\n",
        "        \"num_speakers\": \"0\",\n",
        "        \"length\": 2.0\n",
        "    },\n",
        "```\n",
        "\n",
        "#### **Data Augmentation**\n",
        "- Data Augmentation played a critical role in enhancing the robustness of the models. Implemented techniques such as noise injection, varying speed, and pitch modification to ensure that the models could generalize well across different acoustic environments and speaker variations.\n",
        "\n",
        "#### **Model Development and Optimization**\n",
        "- I have developed and tested four models: X-Vector, ECAPA-TDNN, and a combination of Pretrained Wav2Vec 2.0 with MLP and with Xvectors. Each model was chosen based on its proven efficacy in related tasks such as speaker verification and identification.\n",
        "\n",
        "#### 1. ECAPA TDNN\n",
        "(Emphasized Channel Attention, Propagation and Aggregation Time Delay Neural Network)\n",
        "\n",
        "Leverages a special type of neural network layer called a Time Delay Neural Network (TDNN) to capture temporal information in speech audio.\n",
        "\n",
        "Incorporates an \"attention\" mechanism that emphasizes informative channels within the network, potentially improving speaker differentiation, especially in overlapping scenarios.\n",
        "\n",
        "Well-suited for speaker identification and verification tasks, making it a strong candidate for speaker counting as well.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1IZHqF86vornEw9Ib-N46JMVyhMZ7cctZ\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.2 - ECAPA-TDNN</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 2. XVector\n",
        "Employs a convolutional neural network (CNN) architecture to learn speaker embeddings, which are compressed representations that encode speaker identity.\n",
        "Designed for speaker identification and verification, and its strength lies in its ability to capture speaker-specific characteristics even in noisy or varying environments.\n",
        "\n",
        "This makes it a viable model for speaker counting, where identification of individual speakers often precedes counting.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1n6mRBqFzJNfzbmzQjcKQmDjZBTxr0hbN\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.3 - XVector</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 3. Selfsupervised - MLP(Multi-Layer Perceptron)\n",
        "\n",
        "Wav2Vec 2.0 is a powerful self-supervised model, meaning it learns representations from vast amounts of unlabeled speech data.\n",
        "These learned representations are highly effective at capturing intricate speech features.\n",
        "\n",
        "The MLP acts as a classifier, taking the Wav2Vec 2.0 output and mapping it to the predicted number of speakers.\n",
        "\n",
        "This approach leverages the strengths of self-supervised learning for feature extraction and a traditional MLP for classification, potentially offering robustness in speaker counting.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1enhfLUxl3v-FWZ3bLNa0tFmGLvF6VAvq\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.4 - wav2vec2 with linear classifier</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 4. Selfsupervised XVector\n",
        "Combines the power of Wav2Vec 2.0's self-supervised feature learning with X-vector's speaker identification capability.\n",
        "\n",
        "Wav2Vec 2.0 extracts rich features, and X-vectors can potentially isolate speaker-specific information within those features.\n",
        "\n",
        "This combined approach may lead to more accurate speaker counting, especially in challenging overlapping speech conditions.\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1en7t4gevL-3QqA5ru6Mz7VlfpGx7cby3\" alt=\"Image description\" width=\"600\"/>\n",
        "<figcaption>Fig.5 - wav2vec2 with Xvector</figcaption>\n",
        "</center>"
      ],
      "metadata": {
        "id": "XtzOOJDnMpWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experimental Setup**\n",
        "\n",
        "The LibriSpeech-Clean-100 dataset was used for training, while the LibriSpeech-Dev-Clean set was used for validation, and the LibriSpeech-Test-Clean set was used for final testing. This split ensures generalization is evaluated on unseen data. All models were trained using the Adam optimizer with varying learning rates and weight decay as indicated in the table below. Experiments were conducted on a system with GTX 4050 6GB GPU.\n",
        "\n",
        "### Hyperparameters Used\n",
        "\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Hyperparams</th>\n",
        "    <th>GitHub Link</th>\n",
        "    <th>Model</th>\n",
        "    <th>Hyperparams</th>\n",
        "    <th>GitHub Link</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ECAPA-TDNN</td>\n",
        "    <td><pre><code>\n",
        "    sample_rate: 16000\n",
        "    number_of_epochs: 20\n",
        "    batch_size: 64\n",
        "    lr_start: 0.001\n",
        "    lr_final: 0.0001\n",
        "    weight_decay: 0.00002\n",
        "    num_workers: 0 # For windows or 4 for linux\n",
        "    n_classes: 5\n",
        "    dim: 192\n",
        "    num_attention_channels: 128\n",
        "    n_mels: 80\n",
        "    channels: [256, 256, 256, 256, 768]\n",
        "    kernel_sizes: [5, 3, 3, 3, 1]\n",
        "    dilations: [1, 2, 3, 4, 1]\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/ecapa_tdnn/hparams_ecapa_tdnn_augmentation.yaml\">View Complete file</a></td>\n",
        "    <td>SelfSupervised <br>\n",
        "    Linear Classifier</td>\n",
        "    <td><pre><code>\n",
        "    number_of_epochs: 5\n",
        "    batch_size: 64\n",
        "    lr: 0.001\n",
        "    lr_ssl: 0.0001\n",
        "    freeze_ssl: False\n",
        "    freeze_ssl_conv: True\n",
        "    encoder_dim: 768\n",
        "    out_n_neurons: 5\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/selfsupervised/hparams_selfsupervised_mlp.yaml\">View Complete file</a></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>XVector</td>\n",
        "    <td><pre><code>\n",
        "    sample_rate: 16000\n",
        "    number_of_epochs: 50\n",
        "    batch_size: 64\n",
        "    lr_start: 0.001\n",
        "    lr_final: 0.0001\n",
        "    weight_decay: 0.00002\n",
        "    num_workers: 0 # For windows or 4 for linux\n",
        "    n_mels: 4\n",
        "    n_classes: 5\n",
        "    emb_dim: 128\n",
        "    tdnn_channels: 64\n",
        "    tdnn_channels_out: 128\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/xvector/hparams_xvector_augmentation.yaml\">View Complete file</a></td>\n",
        "    <td>SelfSupervised <br>\n",
        "    XVector</td>\n",
        "    <td><pre><code>\n",
        "    number_of_epochs: 15\n",
        "    batch_size: 128\n",
        "    lr: 0.001\n",
        "    lr_final: 0.0001\n",
        "    lr_ssl: 0.00001\n",
        "    freeze_ssl: False\n",
        "    freeze_ssl_conv: True\n",
        "    encoder_dim: 768\n",
        "    emb_dim: 128\n",
        "    out_n_neurons: 5\n",
        "    tdnn_channels: [ 64, 64, 64 ]\n",
        "    tdnn_kernel_sizes: [ 5, 2, 3 ]\n",
        "    tdnn_dilations: [ 1, 2, 3 ]\n",
        "    </code></pre></td>\n",
        "    <td><a href=\"https://github.com/SupradeepDanturti/ConvAIProject/blob/main/selfsupervised/hparams_selfsupervised_xvector.yaml\">View Complete file</a></td>\n",
        "  </tr>\n",
        "</table></center>"
      ],
      "metadata": {
        "id": "YIQOcLeaPq3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Performance Analysis - Results**\n",
        "\n",
        "Across all models tested, the Self-supervised approaches (Wav2Vec2.0 + MLP and Wav2Vec2.0 + X-vector) demonstrated superior performance in the speaker counting task, achieving lower overall error rates compared to the ECAPA-TDNN and X-vector models.\n",
        "\n",
        "Although the specific error rates were close, the Wav2Vec2.0 + X-vector hybrid model offered a slight advantage. This advantage was particularly noticeable when handling overlapping speech from multiple speakers, as illustrated in Fig.7,9 & 12.\n",
        "\n",
        "An interesting and positive finding from the analysis was the perfect accuracy achieved by all models in identifying segments with no speakers present. This capability is crucial for any diarization system, as it ensures the accurate detection of silent intervals, preventing unnecessary processing and improving overall system efficiency.\n",
        "\n",
        "A common trend across all models was an increase in error rate as the true number of speakers increased.  This difficulty in handling three or more overlapping speakers highlights a core challenge for speaker counting systems.  The class-wise error rates [See Table 1 below] illustrate this struggle, with the highest error rates occurring for classes with 3 or 4 speakers.\n",
        "\n",
        "Notably, all models achieved perfect accuracy in identifying segments containing no speakers. This indicates strong performance in silence detection, a valuable component of speaker diarization systems.\n",
        "\n",
        "But During inference XVector and ECAPA-TDNN are much faster when compared to SelfSupervised models.\n",
        "\n",
        "#### **X-Vector Model**\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1t1VZhDyG8awIU0keoyrw54yRLETa7r9B\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.6 - XVector Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1qVfJx5yzxa_UKE3XVAveicqboy69l2kV\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.7 - Error rate of Both XVector Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "\n",
        "#### **ECAPA-TDNN Model**\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1VeKXeO3-aXcTyBz6Ztq-WH3M40njXEkq\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.8 - ECAPA-TDNN Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1TQ3qK56wQr3lxTEh9-NUh5s-NtRkqtyB\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.9 - Error rate of Both ECAPA-TDNN Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "\n",
        "#### **Self-Supervised MLP Model**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1IgCQqGb1HLbyrhYwUMPuV8fjY-Xy4aUy\" alt=\"Selfsupervised MLP Train and Valid Loss\" width=\"300\"/>\n",
        "<figcaption>Fig.10 - Self-Supervised MLP Train and Valid Loss</figcaption>\n",
        "</center>\n",
        "\n",
        "#### **Self-Supervised X-Vector Model**\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1K6vvfvfoIZShz8Y1lV-epE2waoPgdMIs\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.11 - Self-Supervised X-Vector Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1cZDKGp0VBPOE9TKhWkUXCnd7bgrXV4IQ\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.12 - Error rate of Both Selfsupervised Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "#### **Classwise Error Rate**\n",
        "<center>\n",
        "<table>\n",
        " <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Class</th>\n",
        "    <th>Error Rate</th>\n",
        "    <th>Model</th>\n",
        "    <th>Class</th>\n",
        "    <th>Error Rate</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td rowspan=\"6\">XVector</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.29e-01</td>\n",
        "    <td rowspan=\"6\">ECAPA-TDNN</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.40e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>4.06e-01</td>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>4.12e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>3.20e-02</td>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>4.10e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>2.15e-01</td>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>3.26e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>4.67e-01</td>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>5.23e-01</td>\n",
        "  </tr>\n",
        "  <tr><td colspan=\"6\"></td></tr>\n",
        "  <tr>\n",
        "    <td rowspan=\"6\">Selfsupervised MLP</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.00e-01</td>\n",
        "    <td rowspan=\"6\">Selfsupervised XVector</td>\n",
        "    <td>Overall</td>\n",
        "    <td>2.10e-01</td>\n",
        "  </tr>\n",
        "   <tr>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>0.00e+00</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>8.67e-03</td>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>1.64e-02</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1.14e-01</td>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1.39e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>4.08e-01</td>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>3.34e-01</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>4.49e-01</td>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>5.41e-01</td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>"
      ],
      "metadata": {
        "id": "BRL5KR20QWKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup and Training Instructions**\n",
        "To reproduce the results of this project, follow these steps.\n",
        "\n",
        "```\n",
        "!git clone https://github.com/SupradeepDanturti/ConvAIProject\n",
        "%cd ConvAIProject\n",
        "```\n",
        "Download the project code from the GitHub repository and navigate into the project directory.\n",
        "\n",
        "\n",
        "```\n",
        "!python prepare_dataset/download_required_data.py --output_folder <destination_folder_path>\n",
        "```\n",
        "Download the necessary datasets (LibriSpeech etc.), specifying the desired destination folder.\n",
        "\n",
        "```\n",
        "!python prepare_dataset/create_custom_dataset.py dataset.yaml\n",
        "```\n",
        "Create custom dataset based on set parameters as shown in the sample below\n",
        "\n",
        "Sample of dataset.yaml:\n",
        "```\n",
        "n_sessions:\n",
        "  train: 1000 # Creates 1000 sessions per class\n",
        "  dev: 200 # Creates 200 sessions per class\n",
        "  eval: 200 # Creates 200 sessions per class\n",
        "n_speakers: 4 # max number of speakers. In this case the total classes will be 5 (0-4 speakers)\n",
        "max_length: 120 # max length in seconds for each session/utterance.\n",
        "```\n",
        "<center>Sample of dataset.yaml</center>\n",
        "\n",
        "To train the XVector model run the following command.\n",
        "```\n",
        "!python xvector/train_xvector_augmentation.py xvector/hparams_xvector_augmentation.yaml\n",
        "```\n",
        "\n",
        "To train the ECAPA-TDNN model run the following command.\n",
        "\n",
        "```\n",
        "!python ecapa_tdnn/train_ecapa_tdnn.py ecapa_tdnn/hparams_ecapa_tdnn_augmentation.yaml\n",
        "```\n",
        "\n",
        "To train the SelfSupervised MLP model run the following command.\n",
        "```\n",
        "!python selfsupervised/train_selfsupervised_mlp.py selfsupervised/hparams_selfsupervised_mlp.yaml\n",
        "```\n",
        "\n",
        "To train the SelfSupervised XVector model run the following command.\n",
        "```\n",
        "!python selfsupervised/train_selfsupervised.py selfsupervised/hparams_selfsupervised_xvector.yaml\n",
        "```"
      ],
      "metadata": {
        "id": "OTaD2SZ2cvsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference Interface**\n",
        "\n",
        "To run each models inference pull the interface directory as shown in the cell below\n"
      ],
      "metadata": {
        "id": "vpsZKEbeko19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "OZawEScuQ67L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --filter=blob:none --no-checkout https://github.com/SupradeepDanturti/ConvAIProject\n",
        "%cd ConvAIProject\n",
        "!git sparse-checkout init --cone\n",
        "!git sparse-checkout set interface\n",
        "!git checkout"
      ],
      "metadata": {
        "id": "zU93F9O8Pxfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the inference for XVector or ECAPA-TDNN Models run the run_inference_xvector_ecapa_tdnn.py file or run the cell below.\n",
        "\n",
        "Note- Make sure you are in the ConvAIProject directory. or else you can run the cell below"
      ],
      "metadata": {
        "id": "4nL5i9fEQB21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkpZRymCQ0R4",
        "outputId": "0194a4fe-d178-445d-9909-b64c357d2798"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ConvAIProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This is used for Both XVector and ECAPA-TDNN \"\"\"\n",
        "from interface.SpeakerCounter import SpeakerCounter\n",
        "\n",
        "wav_path = \"interface/sample_audio1.wav\"\n",
        "save_dir = \"interface/sample_inference_run2/\"\n",
        "model_path = \"interface/xvector\" # ./ecapa_tdnn\n",
        "\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "id": "a_R3QP5cQhPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the inference for Selfsupervised Models run the run_inference_selfsupervised.py file or run the cell below.\n",
        "\n",
        "Note- This usually takes a bit more time than the XVector or the ECAPA-TDNN Models. Which is the only disadvantage it has."
      ],
      "metadata": {
        "id": "QQiGZUdGS4t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from interface.SpeakerCounterSelfsupervisedMLP import SpeakerCounter\n",
        "wav_path = \"interface/sample_audio1.wav\"\n",
        "save_dir = \"interface/sample_inference_run\"\n",
        "model_path = \"interface/selfsupervised_mlp\"\n",
        "\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224,
          "referenced_widgets": [
            "6a849787f1114ee8b22141cf9507ae86",
            "7bee48e64bfe4a96895e1fba19d8fcd0",
            "ddc0ecc0d57743c99c7a9be1806e17e9",
            "71cf94d187f94bc9be7217706eacd1d8",
            "5a0535cb94fc42b98acf73976cc4bcb9",
            "0303c0d7bdec4034bf2b6597df4ef3b2",
            "885c30b1204f4f94a1fc2c5c49f9011e",
            "0074e8023dce4bd7b638938022c3fec0",
            "37ae1fc55f4e4b84a5af73045b9208df",
            "2193e99bc188483491212064b70c8d4e",
            "306df962c1e84b6aa9906f578e0c8bc1",
            "d9292e26dd704c6fb6c988c9c6b8587a",
            "bf1364cfff2b46aebe7f6d08c90c81f3",
            "2dbf9263aeed4df2835e6529d4bc66ba",
            "1b18b2bd31ca483eb7d1e6ae360e98e4",
            "8b15b8c2dbd84ddc8d633682cb4f7d79",
            "94fc2bc39a1b4cf38eb02dc3505acf95",
            "5d902d98bc29484fac62825cdfdf714a",
            "b50cbd7540614769aae8c47cfa6d1791",
            "ff5da714da294944876133f306100f3a",
            "7bc156a32d0c4f40b206ef6cd17d2cf5",
            "e5b38f6f9e8b47038b7ed28b4a6eb3bb",
            "810d3856983d4519b62f9fad3f0c243c",
            "31ac56fc3d62428493a23419fb65e6e2",
            "4d5b6f0595c249448a4f5d639835c6f1",
            "39388634e7c145aa8f32c2494c606460",
            "8182979f9aa54fed96e93cd9161a1a41",
            "7fee159435aa44ffab7ce098fb4f3d8a",
            "3caa1c99f6814a00a25df737719b4b37",
            "75d3ab02ab6c4942a46cc64368115d26",
            "5814ff5d0f6f4cd78eb9620eba2fda2e",
            "eb4bdf78f383432ca2a3ecfeda2f495c",
            "fb7be5df48f04188a4399dbd1fdc73c4"
          ]
        },
        "id": "2yC62PIRSj1Y",
        "outputId": "0da4d7c4-83ad-4f55-9557-ff923fe03f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a849787f1114ee8b22141cf9507ae86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9292e26dd704c6fb6c988c9c6b8587a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "810d3856983d4519b62f9fad3f0c243c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:speechbrain.lobes.models.huggingface_transformers.wav2vec2:speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions**\n",
        "\n",
        "Summarize what you could and could not conclude based on your experiments.\n",
        "In this section, you can add **text**.\n",
        "\n"
      ],
      "metadata": {
        "id": "b4Jyn3BcQDpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**\n",
        "You can add here the citations of books, websites, or academic papers, etc.\n",
        "\n",
        "[1] Cornella, S., Omologo, M., Squartini, S., & Vincent, E. (2020). Overlapped Speech Detection and Speaker Counting using Distant Microphone Arrays.\n",
        "\n",
        "[2] Duong, T. T. H., Nguyen, P. L., Nguyen, H. S., & Duong, N. Q. K. (2023). Investigating the Role of Speaker Counter in Handling Overlapping Speeches in Speaker Diarization Systems. Authorea.\n",
        "\n",
        "[3] Wang, Z. Q., & Wang, D. (2022). Count And Separate: Incorporating Speaker Counting For Continuous Speaker Separation.\n",
        "\n",
        "[4] Andrei, V., Cucu, H., & Burileanu, C. (2020). Overlapped Speech Detection and Competing Speaker Counting Humans Versus Deep Learning. IEEE."
      ],
      "metadata": {
        "id": "yaxqlm6kRcmb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQx58wct_p5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}