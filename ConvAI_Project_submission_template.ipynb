{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/dev2/ConvAI_Project_submission_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Project 7: Speaker Counter and Overlap Detector](#scrollTo=TZzv25vDL9LQ)\n",
        "\n",
        ">>[Abstract](#scrollTo=MP7fynuIMGlg)\n",
        "\n",
        ">>[Introduction](#scrollTo=wBGDBctaMYYQ)\n",
        "\n",
        ">>[Methodology](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Preparation & Preprocessing](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Data Augmentation](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Model Development and Optimization](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[ECAPA TDNN](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[XVector](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Selfsupervised - MLP](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Selfsupervised XVector](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>>>[Original Aspects and Relation to Existing Work](#scrollTo=XtzOOJDnMpWp)\n",
        "\n",
        ">>[Experimental Setup](#scrollTo=YIQOcLeaPq3v)\n",
        "\n",
        ">>>[Model Details](#scrollTo=YIQOcLeaPq3v)\n",
        "\n",
        ">>[Model Performance Analysis - Results](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[X-Vector Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[ECAPA-TDNN Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[Self-Supervised MLP Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[Self-Supervised X-Vector Model](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>>>[Classwise Error Rate](#scrollTo=BRL5KR20QWKu)\n",
        "\n",
        ">>[Steps Download, Create and Train all models](#scrollTo=OTaD2SZ2cvsR)\n",
        "\n",
        ">>[Inference Interface](#scrollTo=vpsZKEbeko19)\n",
        "\n",
        ">>[Conclusions](#scrollTo=b4Jyn3BcQDpf)\n",
        "\n",
        ">>[References](#scrollTo=yaxqlm6kRcmb)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "E0d6HODOSHIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 7: Speaker Counter and Overlap Detector**"
      ],
      "metadata": {
        "id": "TZzv25vDL9LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Abstract**\n",
        "\n",
        " - In this project, we address the challenge of accurately counting speakers in meeting recordings, a task complicated by frequent instances of overlapping speech. Leveraging speech separation technologies such as SepFormer, implemented in SpeechBrain, the objective was to initially identify segments containing overlapping speech before applying speech separation. To achieve this, we undertook a comprehensive review of literature on speaker counting and developed a data simulator. This simulator generated overlapping speech signals by combining clean data from the LibriSpeech-Clean-100 dataset with noise and reverberation from the Open-RIR dataset, based on a specified probability.\n",
        "\n",
        " - We experimented with and evaluated two primary models for the task of speaker counting: X-Vector and ECAPA-TDNN, with the input being 1-2 second speech segments. Additionally, a pretrained Wav2Vec 2.0 model coupled with a multilayer perceptron (MLP) classifier was explored. The models were tasked with processing long recordings, segmented into 1-2 second chunks, and determining the number of speakers in each segment. The inference results were outputted as a text file, detailing the start and end times of each segment alongside the speaker count.\n",
        "\n",
        "-  The ECAPA-TDNN model demonstrated the lowest error rate among the tested models, showcasing its effectiveness in distinguishing between zero to three or more speakers within the overlapping speech segments. This research not only contributes a practical tool for speaker counting in complex audio environments but also integrates this functionality into the SpeechBrain project, enhancing its utility for speech technology applications.\n"
      ],
      "metadata": {
        "id": "MP7fynuIMGlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        " -In the era of digital communication, the ability to accurately process and interpret spoken language in audio recordings is becoming increasingly vital. One significant challenge in this field is the problem of speaker counting and diarization in environments where multiple individuals speak simultaneously. This issue is particularly prevalent in meeting scenarios, where overlapping speech can significantly complicate transcription and analysis tasks, leading to errors in speaker identification and speech recognition.\n",
        "\n",
        "- The importance of resolving this challenge cannot be overstated, as it directly impacts the effectiveness of automated systems in various applications, from virtual assistant technologies to legal and medical transcription services. The main challenges in tackling overlapping speech include the segmentation of speech, identification of individual speakers, and the subsequent processing of these segments to accurately count and distinguish between speakers.\n",
        "\n",
        "- Prior works have explored various aspects of speaker diarization and counting. Notably, **Bredin et al.** (2020) introduced *pyannote.audio*, a toolkit that provides neural building blocks specifically designed for speaker diarization tasks. This framework has facilitated significant advancements in the field by enabling researchers and developers to implement and experiment with complex neural network architectures for audio processing tasks.\n",
        "\n",
        "- Approach builds upon these foundational studies by employing advanced neural network models to address the complexities of speaker counting within overlapping speech segments. We focus particularly on the application of X-Vector and ECAPA-TDNN architectures, renowned for their efficacy in speaker verification tasks, and explore the integration of a pretrained Wav2Vec 2.0 model coupled with a multilayer perceptron (MLP) for classification.\n",
        "\n",
        "- To train and evaluate the models, we utilized a simulated dataset created by mixing clean speech samples from the LibriSpeech-Clean-100 dataset with noise and reverberation from the Open-RIR dataset. This setup allowed us to mimic real-world overlapping speech scenarios and provided a robust platform for testing the models' effectiveness.\n",
        "\n",
        "- Preliminary results have been promising, particularly with the use of pretrained(Wav2Vec2) model, which demonstrated the lowest error rate among the models tested. This outcome suggests that the approach could significantly enhance the accuracy and reliability of speaker counting systems in real-world applications.\n",
        "\n",
        "- The adoption of these advanced methodologies not only pushes the boundaries of what is achievable with current speech processing technology but also sets a new standard for future research and development in the field."
      ],
      "metadata": {
        "id": "wBGDBctaMYYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Methodology**\n",
        "\n",
        "My project methodology comprised several significant steps that enabled me to effectively address the complexities of speaker counting within overlapping speech segments. Here, we focus on the crucial aspects of the approach that contributed significantly to the project's success.\n",
        "\n",
        "#### **Preparation & Preprocessing**\n",
        "- Used the [LibriSpeech-Clean-100](https://www.openslr.org/12) dataset for clean speech samples and [Open-RIR](https://www.openslr.org/28/) dataset for realistic noise and reverberation, creating a challenging environment for the models. For Evaluation used [LibriSpeech-Dev-Clean](http://www.openslr.org/resources/12/dev-clean.tar.gz) and tested on [LibriSpeech-test-Clean](http://www.openslr.org/resources/12/test-clean.tar.gz).  Each audio file was segmented into 1-2 second clips, which were then annotated with the number of speakers present, ranging from 0 to 4.\n",
        "\n",
        "#### **Data Augmentation**\n",
        "- Data Augmentation played a critical role in enhancing the robustness of the models. Implemented techniques such as noise injection, varying speed, and pitch modification to ensure that the models could generalize well across different acoustic environments and speaker variations.\n",
        "\n",
        "#### **Model Development and Optimization**\n",
        "- I have developed and tested four models: X-Vector, ECAPA-TDNN, and a combination of Pretrained Wav2Vec 2.0 with MLP and with Xvectors. Each model was chosen based on its proven efficacy in related tasks such as speaker verification and identification.\n",
        "\n",
        "#### 1. ECAPA TDNN\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1IZHqF86vornEw9Ib-N46JMVyhMZ7cctZ\" alt=\"Image description\" width=\"500\"/>\n",
        "<figcaption>Fig.1 - ECAPA-TDNN</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 2. XVector\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1n6mRBqFzJNfzbmzQjcKQmDjZBTxr0hbN\" alt=\"Image description\" width=\"500\"/>\n",
        "<figcaption>Fig.2 - XVector</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 3. Selfsupervised - MLP\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1enhfLUxl3v-FWZ3bLNa0tFmGLvF6VAvq\" alt=\"Image description\" width=\"500\"/>\n",
        "<figcaption>Fig.3 - wav2vec2 with linear classifier</figcaption>\n",
        "</center>\n",
        "\n",
        "#### 4. Selfsupervised XVector\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1en7t4gevL-3QqA5ru6Mz7VlfpGx7cby3\" alt=\"Image description\" width=\"500\"/>\n",
        "<figcaption>Fig.4 - wav2vec2 with Xvector</figcaption>\n",
        "</center>\n",
        "\n",
        "#### **Original Aspects and Relation to Existing Work**\n",
        "- My approach extends existing methodologies by integrating robust data augmentation and advanced neural architectures specifically tuned for the task of speaker counting in overlapping speech conditions. This project not only adapts existing frameworks like those introduced in [pyannote.audio](https://huggingface.co/pyannote/overlapped-speech-detection) but also pushes forward the state-of-the-art by focusing on quantitative metrics such as error rates in realistic, noisy environments."
      ],
      "metadata": {
        "id": "XtzOOJDnMpWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experimental Setup**\n",
        "### Model Details\n",
        "\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Hyperparams</th>\n",
        "    <th>GitHub Link</th>\n",
        "    <th>Model</th>\n",
        "    <th>Hyperparams</th>\n",
        "    <th>GitHub Link</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ECAPA-TDNN</td>\n",
        "    <td><pre><code>\n",
        "    sample_rate: 16000\n",
        "    number_of_epochs: 20\n",
        "    batch_size: 64\n",
        "    lr_start: 0.001\n",
        "    lr_final: 0.0001\n",
        "    weight_decay: 0.00002\n",
        "    num_workers: 0 # For windows or 4 for linux\n",
        "    n_classes: 5\n",
        "    dim: 192\n",
        "    num_attention_channels: 128\n",
        "    n_mels: 80\n",
        "    channels: [256, 256, 256, 256, 768]\n",
        "    kernel_sizes: [5, 3, 3, 3, 1]\n",
        "    dilations: [1, 2, 3, 4, 1]\n",
        "    </code></pre></td>\n",
        "    <td>[view complete file]()</td>\n",
        "    <td>SelfSupervised <br>\n",
        "    Linear Classifier</td>\n",
        "    <td><pre><code>\n",
        "    number_of_epochs: 5\n",
        "    batch_size: 64\n",
        "    lr: 0.001\n",
        "    lr_ssl: 0.0001\n",
        "    freeze_ssl: False\n",
        "    freeze_ssl_conv: True\n",
        "    encoder_dim: 768\n",
        "    out_n_neurons: 5\n",
        "    </code></pre></td>\n",
        "    <td>[view complete file]()</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>XVector</td>\n",
        "    <td><pre><code>\n",
        "    sample_rate: 16000\n",
        "    number_of_epochs: 50\n",
        "    batch_size: 64\n",
        "    lr_start: 0.001\n",
        "    lr_final: 0.0001\n",
        "    weight_decay: 0.00002\n",
        "    num_workers: 0 # For windows or 4 for linux\n",
        "    n_mels: 4\n",
        "    n_classes: 5\n",
        "    emb_dim: 128\n",
        "    tdnn_channels: 64\n",
        "    tdnn_channels_out: 128\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    </code></pre></td>\n",
        "    <td>[view complete file]()</td>\n",
        "    <td>SelfSupervised <br>\n",
        "    XVector</td>\n",
        "    <td><pre><code>\n",
        "    number_of_epochs: 15\n",
        "    batch_size: 128\n",
        "    lr: 0.001\n",
        "    lr_final: 0.0001\n",
        "    lr_ssl: 0.00001\n",
        "    freeze_ssl: False\n",
        "    freeze_ssl_conv: True\n",
        "    encoder_dim: 768\n",
        "    emb_dim: 128\n",
        "    out_n_neurons: 5\n",
        "    tdnn_channels: [ 64, 64, 64 ]\n",
        "    tdnn_kernel_sizes: [ 5, 2, 3 ]\n",
        "    tdnn_dilations: [ 1, 2, 3 ]\n",
        "    </code></pre></td>\n",
        "    <td>[view complete file]()</td>\n",
        "  </tr>\n",
        "</table></center>"
      ],
      "metadata": {
        "id": "YIQOcLeaPq3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Performance Analysis - Results**\n",
        "\n",
        "#### **X-Vector Model**\n",
        "The X-Vector model's training showcased an initial decline in loss, with the augmented data presenting a consistent downward trend indicative of effective learning. However, this was coupled with fluctuations that raise concerns of possible overfitting to the augmented noises. The unaugmented dataset displayed a higher error rate that remained stable across epochs, suggesting a potential limitation in the model's ability to generalize to the augmented conditions without additional optimization strategies.\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1t1VZhDyG8awIU0keoyrw54yRLETa7r9B\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.5 - XVector Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1qVfJx5yzxa_UKE3XVAveicqboy69l2kV\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.6 - Error rate of Both XVector Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "\n",
        "#### **ECAPA-TDNN Model**\n",
        "The ECAPA-TDNN model demonstrated a downward trend in error rates for both the augmented and unaugmented datasets, with the augmented setup yielding notably lower error rates. This underscores the model's capacity to generalize from augmented training. However, fluctuations in the augmented validation loss suggest sensitivity to novel complex patterns, indicating potential overfitting and necessitating further model refinement.\n",
        "\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1VeKXeO3-aXcTyBz6Ztq-WH3M40njXEkq\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.7 - ECAPA-TDNN Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1TQ3qK56wQr3lxTEh9-NUh5s-NtRkqtyB\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.8 - Error rate of Both ECAPA-TDNN Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "\n",
        "#### **Self-Supervised MLP Model**\n",
        "The self-supervised MLP model, powered by Wav2Vec 2.0, exhibited a promising decline in training loss, though the reduction in validation loss plateaued early, suggesting a limit to its learning capacity in this context. While it did not reach the lowest error rates among the models, it demonstrates the effectiveness of self-supervised learning frameworks in achieving rapid initial improvements.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1IgCQqGb1HLbyrhYwUMPuV8fjY-Xy4aUy\" alt=\"Selfsupervised MLP Train and Valid Loss\" width=\"300\"/>\n",
        "<figcaption>Fig.9 - Self-Supervised MLP Train and Valid Loss</figcaption>\n",
        "</center>\n",
        "\n",
        "#### **Self-Supervised X-Vector Model**\n",
        "The self-supervised X-Vector model exhibited outstanding performance with the lowest error rates, validating the robust feature extraction capability of the Wav2Vec 2.0 framework. Despite some fluctuations in the error rate, the overall trend displayed an impressive adaptability to speaker nuances, highlighting the advantages of pretraining on a vast corpus of unlabeled data.\n",
        "\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1K6vvfvfoIZShz8Y1lV-epE2waoPgdMIs\" alt=\"Selfsupervised XVector Train and Valid Loss\" width=\"300\"/>\n",
        "      <figcaption>Fig.10 - Self-Supervised X-Vector Train and Valid Loss</figcaption>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1cZDKGp0VBPOE9TKhWkUXCnd7bgrXV4IQ\" alt=\"img\" width=\"300\"/>\n",
        "      <figcaption>Fig.11 - Error rate of Both Selfsupervised Models</figcaption>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "#### **Classwise Error Rate**\n",
        "<center>\n",
        "<table>\n",
        " <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Class</th>\n",
        "    <th>Error Rate</th>\n",
        "    <th>Model</th>\n",
        "    <th>Class</th>\n",
        "    <th>Error Rate</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td rowspan=\"5\">XVector</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td rowspan=\"5\">ECAPA-TDNN</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td rowspan=\"5\">Selfsupervised MLP</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td rowspan=\"5\">Selfsupervised XVector</td>\n",
        "    <td>No Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>1 Speaker</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>2 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>3 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "    <td>4 Speakers</td>\n",
        "    <td>1e-05</td>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>"
      ],
      "metadata": {
        "id": "BRL5KR20QWKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Steps Download, Create and Train all models**\n",
        "\n",
        "Pull codebase from github\n",
        "```\n",
        "!git clone --filter=blob:none --no-checkout https://github.com/SupradeepDanturti/ConvAIProject\n",
        "%cd ConvAIProject\n",
        "```\n",
        "\n",
        "Prepare the dataset\n",
        "\n",
        "1.   Download Dataset\n",
        "```\n",
        "!python prepare_dataset/download_required_data.py --output_folder <destination_folder_path>\n",
        "```\n",
        "2.   Create Damples\n",
        "```\n",
        "!python prepare_dataset/create_custom_dataset.py dataset.yaml\n",
        "```\n",
        "dataset.yaml\n",
        "```\n",
        "n_sessions:\n",
        "  train: 1000 # Creates 1000 sessions per class\n",
        "  dev: 200 # Creates 200 sessions per class\n",
        "  eval: 200 # Creates 200 sessions per class\n",
        "n_speakers: 4 # max number of speakers. In this case the total classes will be 5 (0-4 speakers)\n",
        "max_length: 120 # max length in seconds for each session/utterance.\n",
        "```\n",
        "3. Train XVector Model\n",
        "```\n",
        "!python xvector/train_xvector_augmentation.py xvector/hparams_xvector_augmentation.yaml\n",
        "```\n",
        "4. Train ECAPA-TDNN Model\n",
        "```\n",
        "!python ecapa_tdnn/train_ecapa_tdnn.py ecapa_tdnn/hparams_ecapa_tdnn_augmentation.yaml\n",
        "```\n",
        "5. Train Selfsupervised MLP(Linear Classifier)\n",
        "```\n",
        "!python selfsupervised/train_selfsupervised_mlp.py selfsupervised/hparams_selfsupervised_mlp.yaml\n",
        "```\n",
        "6. Train Selfsupervised XVector\n",
        "```\n",
        "!python selfsupervised/train_selfsupervised.py selfsupervised/hparams_selfsupervised_xvector.yaml\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OTaD2SZ2cvsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference Interface**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from SpeakerCounter import SpeakerCounter\n",
        "wav_path = wav_path\n",
        "save_dir = save_dir_path\n",
        "model_path = model_path\n",
        "\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "audio_classifier.classify_file(wav_path)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vpsZKEbeko19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions**\n",
        "\n",
        "Summarize what you could and could not conclude based on your experiments.\n",
        "In this section, you can add **text**.\n",
        "\n"
      ],
      "metadata": {
        "id": "b4Jyn3BcQDpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**\n",
        "You can add here the citations of books, websites, or academic papers, etc."
      ],
      "metadata": {
        "id": "yaxqlm6kRcmb"
      }
    }
  ]
}