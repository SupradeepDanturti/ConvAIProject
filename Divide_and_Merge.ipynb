{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_N7MrhDjkt3B"
   },
   "outputs": [],
   "source": [
    "# !pip install speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e0LveiDiktuf",
    "ExecuteTime": {
     "end_time": "2024-03-11T00:39:08.744578Z",
     "start_time": "2024-03-11T00:39:04.829052Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "import random\n",
    "import torchaudio\n",
    "import torch\n",
    "from speechbrain.augment.time_domain import AddNoise, AddReverb\n",
    "from speechbrain.dataio.dataio import read_audio, write_audio\n",
    "from speechbrain.augment.preparation import write_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T00:39:08.759578Z",
     "start_time": "2024-03-11T00:39:08.746579Z"
    }
   },
   "outputs": [],
   "source": [
    "#Windows\n",
    "base_path = r'D:\\Winter 24\\COMP 691 X- Conversational AI\\project'\n",
    "libri_root_path = os.path.join('content', 'miniLibriSpeech')\n",
    "miniLibriSpeechSegments_path = os.path.join('content',  'miniLibriSpeechSegments')\n",
    "output_directory = os.path.join('content',  'miniLibriSpeechOverlappedSegments') \n",
    "\n",
    "os.makedirs(miniLibriSpeechSegments_path, exist_ok=True)\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWxwHuGzktqP"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "\n",
    "# # !tar -xvzf /content/drive/MyDrive/ConvAI/Project/train-clean-100.tar.gz # Complete Dataset\n",
    "\n",
    "# !tar -xvzf /content/drive/MyDrive/ConvAI/Project/miniLibriSpeech.tar.gz # Mini Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zol1rB9wktlv",
    "ExecuteTime": {
     "end_time": "2024-03-09T19:11:42.080973Z",
     "start_time": "2024-03-09T19:11:42.075962Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Colab\n",
    "# libri_root_path = \"content/miniLibriSpeech/\"\n",
    "# librispeech_csv = \"/content/librispeech.csv\"\n",
    "# \n",
    "# miniLibriSpeechSegments_path = \"/content/miniLibriSpeechSegments/\"\n",
    "# \n",
    "# output_folder_corruptednoises = \"/content/corruptednoises/\"\n",
    "# corruptednoises_csv = '/content/corruptednoises.csv'\n",
    "# \n",
    "# openrir_folder = \"/content/\"\n",
    "# reverb_csv = \"reverb.csv\"\n",
    "# noise_csv = \"noise.csv\"\n",
    "# max_noise_len = 10.0\n",
    "# \n",
    "# output_directory = \"/content/miniLibriSpeechOverlappedSegments\"  # Update this path\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5lhmiNu6ldyn",
    "ExecuteTime": {
     "end_time": "2024-03-11T00:39:13.637621Z",
     "start_time": "2024-03-11T00:39:08.760587Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_audio(audio_path, output_folder, segment_info_list, start_offset=4.0):\n",
    "    # Load audio file\n",
    "    sample_rate = 16000\n",
    "    waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "    # Calculate the start position in samples to skip the initial offset\n",
    "    start_position = int(start_offset * sample_rate)\n",
    "\n",
    "    # Iterate over the audio waveform starting from the offset and extract segments with random durations between 1.5 and 2 seconds\n",
    "    current_position = start_position  # Start after the first 4 seconds\n",
    "    while current_position < waveform.size(1):\n",
    "        segment_duration = random.uniform(1.5, 2)  # Random duration between 1.5 and 2 seconds\n",
    "        segment_length = int(segment_duration * sample_rate)\n",
    "        \n",
    "        # Ensure the segment does not exceed the waveform length\n",
    "        end_position = min(current_position + segment_length, waveform.size(1))\n",
    "        \n",
    "        segment = waveform[:, current_position:end_position]\n",
    "\n",
    "        # Extract file information\n",
    "        file_path, file_name = os.path.split(audio_path)\n",
    "        file_name, _ = os.path.splitext(file_name)\n",
    "        speaker_id = file_name.split('-')[0]\n",
    "        chapter_id = file_name.split('-')[1]\n",
    "        audio_id = file_name.split('-')[2]\n",
    "\n",
    "        # Create output directory structure if not exists\n",
    "        output_subfolder = os.path.join(output_folder, speaker_id, chapter_id)\n",
    "        os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "        # Define output filename based on segment start time\n",
    "        start_time = current_position / sample_rate\n",
    "        end_time = end_position / sample_rate\n",
    "        output_filename = f\"{file_name}-{start_time:.2f}-{end_time:.2f}.flac\"\n",
    "        output_path = os.path.join(output_subfolder, output_filename)\n",
    "\n",
    "        # Save the segment as a separate audio file\n",
    "        torchaudio.save(output_path, segment, sample_rate)\n",
    "\n",
    "        # Add segment information to the list\n",
    "        segment_info_list.append({\n",
    "            \n",
    "            \"speakerID\": speaker_id,\n",
    "            \"chapter_id\": chapter_id,\n",
    "            \"audio_id\": audio_id,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"segment_length\": segment_length,\n",
    "            \"file_path\": output_path.replace(\"\\\\\", \"/\")  # Ensure consistent path format\n",
    "        })\n",
    "\n",
    "        # Move to the next position\n",
    "        current_position = end_position\n",
    "\n",
    "# Assuming 'libri_root_path' and 'miniLibriSpeechSegments_path' are defined as per your environment\n",
    "# Initialize list to store segment information\n",
    "all_segment_info = []\n",
    "\n",
    "# Assuming glob and os are correctly set up to find your FLAC files\n",
    "flac_files = glob.glob(os.path.join(libri_root_path, \"**/*.flac\"), recursive=True)\n",
    "\n",
    "# Iterate over FLAC files and segment them\n",
    "for audio_file in flac_files:\n",
    "    segment_audio(audio_file, miniLibriSpeechSegments_path, all_segment_info)\n",
    "\n",
    "# Write all segment information to a single JSON file\n",
    "json_output_path = os.path.join(miniLibriSpeechSegments_path, \"train-clean-100_segments.json\")\n",
    "with open(json_output_path, \"w\") as json_file:\n",
    "    json.dump(all_segment_info, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "json_file_path = os.path.join(base_path, 'content', 'miniLibriSpeechSegments', 'train-clean-100_segments.json')\n",
    "output_directory = os.path.join(base_path, 'content', 'miniLibriSpeechOverlappedSegments')\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "def load_segment_info(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "all_segment_info = load_segment_info(json_file_path)\n",
    "\n",
    "def mix_audio_segments(segments):\n",
    "    max_length = max(segment.size(1) for segment in segments)\n",
    "    mixed = torch.zeros(1, max_length)\n",
    "    for segment in segments:\n",
    "        length = min(segment.size(1), max_length)\n",
    "        mixed[:, :length] += segment[:, :length]\n",
    "    return mixed\n",
    "\n",
    "def process_combination(combo_info, segments_by_audio, output_directory):\n",
    "    combo, num_combinations = combo_info\n",
    "    segments_to_mix = [torchaudio.load(segments_by_audio[audio_key]['file_path'])[0] for audio_key in combo]\n",
    "    mixed_waveform = mix_audio_segments(segments_to_mix)\n",
    "    combo_id = \"_\".join(combo) + f\"_mixed_{num_combinations}\"\n",
    "    output_filename = os.path.join(output_directory, f\"{combo_id}.wav\")\n",
    "    torchaudio.save(output_filename, mixed_waveform, 16000)\n",
    "\n",
    "    metadata = {\n",
    "        \"num_speakers\": num_combinations,\n",
    "        \"path\": output_filename,\n",
    "        \"total_length\": mixed_waveform.size(1) / 16000  # Assuming a sample rate of 16000\n",
    "    }\n",
    "    # Adding speaker-specific metadata\n",
    "    for audio_key in combo:\n",
    "        speaker_id = audio_key.split('_')[0]\n",
    "        metadata[f\"Speaker{speaker_id}\"] = {\n",
    "            \"start_time\": segments_by_audio[audio_key]['start_time'],\n",
    "            \"end_time\": segments_by_audio[audio_key]['end_time']\n",
    "        }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def prepare_combinations_data(all_segment_info, num_combinations, max_combinations_per_audio):\n",
    "    audio_usage_counter = {}\n",
    "    segments_by_audio = {}\n",
    "    for seg_info in all_segment_info:\n",
    "        audio_key = f\"{seg_info['speakerID']}_{seg_info['chapter_id']}_{seg_info['audio_id']}\"\n",
    "        segments_by_audio[audio_key] = seg_info\n",
    "        audio_usage_counter[audio_key] = 0\n",
    "        \n",
    "    # # Create a list of audio keys and shuffle it to ensure diversity\n",
    "    audio_keys = list(segments_by_audio.keys())\n",
    "    random.shuffle(audio_keys)\n",
    "    \n",
    "    valid_combinations = []\n",
    "    for combo in itertools.combinations(segments_by_audio.keys(), num_combinations):\n",
    "        # Ensure unique speakers and within audio limit\n",
    "        speakers = set(key.split('_')[0] for key in combo)\n",
    "        if len(speakers) == num_combinations and all(audio_usage_counter[key] < max_combinations_per_audio for key in combo):\n",
    "            valid_combinations.append(combo)\n",
    "            for key in combo:\n",
    "                audio_usage_counter[key] += 1  \n",
    "\n",
    "    return valid_combinations, segments_by_audio\n",
    "\n",
    "def main(all_segment_info, max_combinations_per_audio=20, max_workers=10):\n",
    "    segments_by_audio = {f\"{seg_info['speakerID']}_{seg_info['chapter_id']}_{seg_info['audio_id']}\": seg_info for seg_info in all_segment_info}\n",
    "    all_metadata = {\"combos\": []}  # Initialize the metadata collection\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for num_combinations in range(2, 5):\n",
    "            valid_combinations, _ = prepare_combinations_data(all_segment_info, num_combinations, max_combinations_per_audio)\n",
    "            for combo in valid_combinations:\n",
    "                futures.append(executor.submit(process_combination, (combo, num_combinations), segments_by_audio, output_directory))\n",
    "        \n",
    "        # Collect metadata from futures\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing audio combos\"):\n",
    "            all_metadata[\"combos\"].append(future.result())\n",
    "\n",
    "    # Write collected metadata to a single JSON file\n",
    "    metadata_path = os.path.join(output_directory, \"all_combos_metadata.json\")\n",
    "    with open(metadata_path, 'w') as json_file:\n",
    "        json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(all_segment_info)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-11T00:39:13.639625Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### Add Noise and reverb\n",
    "\n",
    "\n",
    "# # Define the paths\n",
    "# noises_path = os.path.join(base_path, 'content', 'rirs_noises', 'noises')\n",
    "\n",
    "# os.makedirs(noises_path, exist_ok=True)\n",
    "\n",
    "# pointsource_noises_path = os.path.join(base_path, 'content', 'rirs_noises', 'pointsource_noises')\n",
    "# real_rirs_isotropic_noises_path = os.path.join(base_path, 'content', 'rirs_noises', 'real_rirs_isotropic_noises')\n",
    "\n",
    "# shutil.move(pointsource_noises_path, noises_path)\n",
    "# shutil.move(real_rirs_isotropic_noises_path, noises_path)\n",
    "\n",
    "\n",
    "Overlapped_audios = get_all_files(os.path.join(base_path, 'content', 'miniLibriSpeechOverlappedSegments'), match_and=['.wav'])\n",
    "rir_audios = get_all_files(os.path.join(base_path, 'content', 'rirs_noises', 'simulated_rirs'), match_and=['.wav'])\n",
    "noise_audios = get_all_files(os.path.join(base_path, 'content', 'rirs_noises', 'noises'), match_and=['.wav'])\n",
    "\n",
    "Overlapped_audios_csv = write_csv(Overlapped_audios, os.path.join(base_path, 'content', 'Overlapped_audios.csv'))\n",
    "rir_audios_csv = write_csv(rir_audios, os.path.join(base_path, 'content', 'rir_audios.csv'))\n",
    "noise_audios_csv = write_csv(noise_audios, os.path.join(base_path, 'content', 'noise_audios.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Overlapped_audios[1].split(\"\\\\\")[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import torchaudio\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "\n",
    "# Initialize your noise and reverb processors\n",
    "noisifier = AddNoise(csv_file=os.path.join(base_path, 'content', 'noise_audios.csv'), num_workers=8)\n",
    "reverb = AddReverb(csv_file=os.path.join(base_path, 'content', 'rir_audios.csv'), num_workers=8)\n",
    "\n",
    "batch_size = 10\n",
    "total_batches = len(Overlapped_audios) // batch_size + (1 if len(Overlapped_audios) % batch_size > 0 else 0)\n",
    "\n",
    "for i in tqdm(range(0, len(Overlapped_audios), batch_size), desc='Processing Batches', total=total_batches):\n",
    "    batch_paths = Overlapped_audios[i:i+batch_size]\n",
    "    \n",
    "    # Load and process each audio file in the batch\n",
    "    processed_audios = []\n",
    "    for audio_path in batch_paths:\n",
    "        audio_path = audio_path.replace(\"\\\\\", \"/\")\n",
    "        signal = read_audio(audio_path)\n",
    "        clean = signal.unsqueeze(0) # if signal.dim() == 1 else signal.transpose(0, 1).unsqueeze(0)\n",
    "\n",
    "        # Apply noise\n",
    "        noisy = noisifier(clean, torch.ones(clean.size(0)))\n",
    "\n",
    "        # Apply reverb\n",
    "        if noisy.dim() == 2:\n",
    "            noisy = noisy.unsqueeze(-1)  # Adding channel dimension for mono signals\n",
    "        reverbed = reverb(noisy)\n",
    "\n",
    "        # Ensure the output is in the correct shape for saving [time, channels]\n",
    "        processed_audio = reverbed.squeeze(0).transpose(0, 1)\n",
    "        processed_audios.append(processed_audio)\n",
    "\n",
    "    # Save processed audio\n",
    "    for j, processed_audio in enumerate(processed_audios):\n",
    "        output_path = batch_paths[j]\n",
    "        torchaudio.save(output_path, processed_audio, 16000)\n",
    "# noisifier = AddNoise(csv_file=os.path.join(base_path, 'content', 'noise_audios.csv'), num_workers=8)\n",
    "# reverb = AddReverb(csv_file=os.path.join(base_path, 'content', 'rir_audios.csv'), num_workers=8)\n",
    "# for audio in Overlapped_audios:\n",
    "#     audio = audio.replace(\"\\\\\", \"/\")\n",
    "#     signal  = read_audio(audio)\n",
    "#     clean = signal.unsqueeze(0)\n",
    "#     \n",
    "#     noisy = noisifier(clean, torch.ones(1))\n",
    "#     \n",
    "#     reberbed = reverb(noisy.unsqueeze(0)) \n",
    "#     \n",
    "#     torchaudio.save(audio, reberbed, 16000) "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Without saving json data\n",
    "\n",
    "json_file_path = os.path.join(base_path, 'content', 'miniLibriSpeechSegments', 'train-clean-100_segments.json')\n",
    "output_directory = os.path.join(base_path, 'content', 'miniLibriSpeechOverlappedSegments')\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "def load_segment_info(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "all_segment_info = load_segment_info(json_file_path)\n",
    "\n",
    "def mix_audio_segments(segments):\n",
    "    max_length = max(segment.size(1) for segment in segments)\n",
    "    mixed = torch.zeros(1, max_length)\n",
    "    for segment in segments:\n",
    "        length = min(segment.size(1), max_length)\n",
    "        mixed[:, :length] += segment[:, :length]\n",
    "    return mixed\n",
    "\n",
    "def process_combination(combo_info):\n",
    "    combo, segments_by_audio, output_directory, num_combinations = combo_info\n",
    "    segments_to_mix = [torchaudio.load(segments_by_audio[audio_key]['file_path'])[0] for audio_key in combo]\n",
    "    mixed_waveform = mix_audio_segments(segments_to_mix)\n",
    "    combo_id = \"_\".join(combo) + f\"_mixed_{num_combinations}\"\n",
    "    output_filename = f\"{combo_id}.wav\"\n",
    "    torchaudio.save(os.path.join(output_directory, output_filename), mixed_waveform, 16000)\n",
    "    return combo_id\n",
    "\n",
    "def prepare_combinations_data(all_segment_info, num_combinations, max_combinations_per_audio):\n",
    "    audio_usage_counter = {}\n",
    "    segments_by_audio = {}\n",
    "    for seg_info in all_segment_info:\n",
    "        audio_key = f\"{seg_info['speakerID']}_{seg_info['chapter_id']}_{seg_info['audio_id']}\"\n",
    "        segments_by_audio[audio_key] = seg_info\n",
    "        audio_usage_counter[audio_key] = 0\n",
    "        \n",
    "    # # Create a list of audio keys and shuffle it to ensure diversity\n",
    "    audio_keys = list(segments_by_audio.keys())\n",
    "    random.shuffle(audio_keys)\n",
    "    \n",
    "    valid_combinations = []\n",
    "    for combo in itertools.combinations(segments_by_audio.keys(), num_combinations):\n",
    "        # Ensure unique speakers and within audio limit\n",
    "        speakers = set(key.split('_')[0] for key in combo)\n",
    "        if len(speakers) == num_combinations and all(audio_usage_counter[key] < max_combinations_per_audio for key in combo):\n",
    "            valid_combinations.append(combo)\n",
    "            for key in combo:\n",
    "                audio_usage_counter[key] += 1  \n",
    "\n",
    "    return valid_combinations, segments_by_audio\n",
    "\n",
    "def main(all_segment_info, max_combinations_per_audio=50, max_workers=10):\n",
    "    segments_by_audio = {f\"{seg_info['speakerID']}_{seg_info['chapter_id']}_{seg_info['audio_id']}\": seg_info for seg_info in all_segment_info}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for num_combinations in tqdm(range(2, 5), desc=\"Generating combinations\"):\n",
    "            valid_combinations, _ = prepare_combinations_data(all_segment_info, num_combinations, max_combinations_per_audio)\n",
    "            tasks = [(combo, segments_by_audio, output_directory, num_combinations) for combo in valid_combinations]\n",
    "            futures.extend(executor.map(process_combination, tasks))\n",
    "\n",
    "        tqdm(futures, total=len(futures), desc=\"Processing audio combos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(all_segment_info)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
