# ConvAIProject
 Project 7: Overlap detector + speaker counter The objective of this project  is to build a speaker counter for meeting recordings using speech technology. Meetings often have overlapping speech and speech separation technologies such as SepFormer (implemented in SpeechBrain) can separate the speech into individual tracks for each speaker. However, before speech separation can be applied, the segments of the recording that contain overlapping speech must be identified. This can be done with a neural network that inputs a short speech segment and outputs the number of speakers present in the segment. The output could be 0, 1, 2, or 3, representing no speakers, one speaker, two speakers, or three or more speakers, respectively.  The student is tasked with the following steps to achieve this goal:  Reviewing the literature on speaker counting. Implementing a data simulator that creates overlapping speech signals by sampling clean data from a large dataset (e.g. librispeech-clean-100) and adding noise and reverberation from the open-rir dataset with a specified probability (e.g. 0.5). Implementing and testing at least two models for speaker counting, such as x-vectors or ECAPA-TDNN (or any other model proposed by the students), with input being a 1-2 second speech segment. Implementing the inference stage where the model can process long recordings by chunking them into 1-2 second segments and making a decision on the number of speakers present in each segment. The output should be in the form of a text file indicating the start and end times of each segment and the decision made by the model. For instance: 0.00 1.00 0 (mo speech) 1.00 5.50 1 (1 speaker) 5.50 6.55 2 (two speakers) 6.55 10.34  1 (1 speaker)  Where each line contains begin_second, end_second, classifier decision.  Integrating the code into the main SpeechBrain project and making the best model available on the SpeechBrain repository for use by others. Develop and interface for inference purposes, similar to the one available for the VAD.
