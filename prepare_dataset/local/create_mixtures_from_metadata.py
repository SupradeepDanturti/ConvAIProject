"""
Creates mixtures from json files generated by create_mixtures_metadata.py.
"""

import os
import torch
import numpy as np
import torchaudio
import json


def create_mixture(session_n, output_dir, params, metadata):
    """
    Constructs an audio mixture from multiple source files based on session metadata and saves the resulting mixture
    along with the updated metadata.

    Parameters:
        - session_n (str): Identifier for the session, used to name the output directory and files.
        - output_dir (str): Base directory to store the generated audio mixtures and metadata.
        - params (dict): Parameters for audio processing, including 'max_length', 'samplerate', and
                        'librispeech_root'. 'max_length' specifies the maximum length of the mixture
                        in seconds, 'samplerate' is the sample rate for the audio files, and 'librispeech_root'
                        is the path to the directory containing source audio files.
        - metadata (dict): A nested dictionary where the outer key is the session identifier, and the inner keys
                           are speaker IDs with each containing a list of utterances. Each utterance is a dictionary
                           detailing the start and stop times, file path, and optionally words spoken.
    Returns:
        None. The function saves the audio mixture as a WAV file and the session metadata as a JSON file in the
        specified output directory.
    """

    os.makedirs(os.path.join(output_dir, session_n), exist_ok=True)
    session_meta = {}

    # Initialize mixture length based on max_length parameter
    max_length_sec = params['max_length']
    tot_length_samples = int(np.ceil(max_length_sec * params["samplerate"]))
    mixture = torch.zeros(tot_length_samples)

    # Placeholder for tracking utterance start and stop times for overlap management
    active_utterances = []

    for speaker_id, utterances in metadata[session_n].items():

        if speaker_id not in session_meta:
            session_meta[speaker_id] = []

        for utterance in utterances:

            if speaker_id == "0":
                """We create 0 speaker utterances using torch.zeros"""
                silence_duration_samples = int(np.ceil((utterance["stop"] - utterance["start"]) * params["samplerate"]))
                silence_start_sample = int(utterance["start"] * params["samplerate"])
                silence_stop_sample = silence_start_sample + silence_duration_samples

                if silence_stop_sample > mixture.shape[0]:
                    additional_length = silence_stop_sample - mixture.shape[0]
                    mixture = torch.cat((mixture, torch.zeros(additional_length)), 0)

                session_meta[speaker_id].append({
                    "file": utterance["file"],
                    "start": utterance["start"],
                    "stop": utterance["stop"],
                    "words": utterance["words"]
                })

            else:
                """We create 1- n-speaker utterances"""
                audio_path = os.path.join(params["librispeech_root"], utterance["file"])
                audio, _ = torchaudio.load(audio_path)

                # Mono conversion and channel selection if needed
                if audio.shape[0] > 1:
                    audio = audio[utterance["channel"]]

                start_sample = int(utterance["start"] * params["samplerate"])
                stop_sample = start_sample + audio.shape[1]

                # Ensure mixture can accommodate current utterance
                if stop_sample > mixture.shape[0]:
                    additional_length = stop_sample - mixture.shape[0]
                    mixture = torch.cat((mixture, torch.zeros(additional_length)), 0)

                # Make sure audio is 1D before adding it to the mixture
                if audio.dim() > 1:
                    audio = audio.squeeze()  # Squeeze audio to ensure it's 1D

                # Add current utterance to the mixture
                mixture[start_sample:stop_sample] += audio

                # Update active utterances for overlap management
                active_utterances.append((start_sample, stop_sample))

                # Append utterance details to the corresponding speaker ID in session_meta
                session_meta[speaker_id].append({
                    "file": utterance["file"],
                    "start": utterance["start"],
                    "stop": utterance["stop"],
                    "words": utterance["words"]
                })

    # Clamp mixture to avoid clipping
    mixture = torch.clamp(mixture, min=-1.0, max=1.0)

    # Save the mixture
    mixture_path = os.path.join(output_dir, session_n, f"{session_n}_mixture.wav")
    torchaudio.save(mixture_path, mixture.unsqueeze(0), params["samplerate"])

    # Save session metadata to JSON
    metadata_path = os.path.join(output_dir, session_n, f"{session_n}_metadata.json")
    with open(metadata_path, "w") as jsonfile:
        json.dump(session_meta, jsonfile, indent=4)
