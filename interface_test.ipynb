{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5wWRCW203P6MqFLpxIMRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/main/interface_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "L8_uFkn4omQK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P1afa3l2nzNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e901d392-5ec7-4514-d68f-a16659760587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting interface_hyperparams.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file interface_hyperparams.yaml\n",
        "\n",
        "#Feature parameters\n",
        "sample_rate: 16000\n",
        "n_mels: 80\n",
        "\n",
        "#Model parameters\n",
        "n_classes: 5\n",
        "\n",
        "#model\n",
        "\n",
        "label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder\n",
        "\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN\n",
        "    input_size: !ref <n_mels>\n",
        "    channels: [256, 256, 256, 256, 768]\n",
        "    kernel_sizes: [5, 3, 3, 3, 1]\n",
        "    dilations: [1, 2, 3, 4, 1]\n",
        "    attention_channels: 128\n",
        "    lin_neurons: 192\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier\n",
        "    input_size: 192\n",
        "    out_neurons: !ref <n_classes>\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "\n",
        "pretrained_path: /content\n",
        "\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "    loadables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        label_encoder: !ref <label_encoder>\n",
        "    paths:\n",
        "        embedding_model: !ref <pretrained_path>/embedding_model.ckpt\n",
        "        classifier: !ref <pretrained_path>/classifier.ckpt\n",
        "        label_encoder: !ref <pretrained_path>/label_encoder.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file SpeakerCounter.py\n",
        "\n",
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        # Manage single waveforms in input\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        # Assign full length if wav_lens is not assigned\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        # Storing waveform in the specified device\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs)\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def classify_batch(self, wavs, wav_lens=None):\n",
        "        emb = self.encode_batch(wavs, wav_lens)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        # text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index\n",
        "        # return out_prob, score, index, text_lab\n",
        "\n",
        "    def classify_file(self, path, **kwargs):\n",
        "        waveform = self.load_audio(path, **kwargs)\n",
        "        # Fake a batch:\n",
        "        batch = waveform.unsqueeze(0)\n",
        "        rel_length = torch.tensor([1.0])\n",
        "        emb = self.encode_batch(batch, rel_length)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index, text_lab\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_batch(wavs, wav_lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnalQbNvn3JI",
        "outputId": "4584e7df-eac0-4119-8503-92cb01642288"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting SpeakerCounter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_1_spk_3_mixture.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "pred = audio_classifier.classify_file(wav_path)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cslpc5n4n41Y",
        "outputId": "c26a29ea-43d8-495e-ecfc-c93b692ec9dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[0.3709, 0.7996, 0.9666, 0.9996, 0.9995]]), tensor([0.9996]), tensor([3]), ['3'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjAKjKoLVxlC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}