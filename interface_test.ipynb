{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgo9rg5xiItmtnaetQFYBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/main/interface_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6bCpuKZqJrDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "fFO7-WNnbdjH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!unzip /content/drive/MyDrive/ConvAI/interface_files.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6R9k1rJ7Yp1",
        "outputId": "5ad2b01f-1fa6-48d9-e3dc-cd90921a9925"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Archive:  /content/drive/MyDrive/ConvAI/interface_files.zip\n",
            "  inflating: classifier.ckpt         \n",
            "  inflating: embedding_model.ckpt    \n",
            "  inflating: hyperparams.yaml        \n",
            "  inflating: label_encoder.txt       \n",
            "  inflating: session_2_spk_2_mixture.wav  \n",
            "  inflating: session_4_spk_1_mixture_004_segment.wav  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4KV97OetbYED"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "\n",
        "    def resample_waveform(self, waveform, orig_sample_rate, new_sample_rate=16000):\n",
        "        \"\"\"\n",
        "        Resample the waveform to a new sample rate.\n",
        "        \"\"\"\n",
        "        if orig_sample_rate != new_sample_rate:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=new_sample_rate)\n",
        "            waveform = resample_transform(waveform)\n",
        "        return waveform\n",
        "\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        # Manage single waveforms in input\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        # Assign full length if wav_lens is not assigned\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        # Storing waveform in the specified device\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs)\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def create_segments(self, waveform, segment_length=2.0):\n",
        "        num_samples = waveform.shape[1]\n",
        "        segment_samples = int(segment_length * self.sample_rate)\n",
        "        segments = []\n",
        "        segment_times = []\n",
        "\n",
        "        for start in range(0, num_samples, segment_samples):\n",
        "            end = start + segment_samples\n",
        "            if end > num_samples:\n",
        "                end = num_samples\n",
        "            segments.append(waveform[:, start:end])\n",
        "            segment_times.append((start / self.sample_rate, end / self.sample_rate))\n",
        "\n",
        "        return segments, segment_times\n",
        "\n",
        "    def classify_file(self, path, segment_length=2.0, **kwargs):\n",
        "        waveform, osr = torchaudio.load(path)\n",
        "        waveform = self.resample_waveform(waveform, osr)\n",
        "\n",
        "        segments, segment_times = self.create_segments(waveform, segment_length)\n",
        "        with open(\"segment_predictions.txt\", \"w\") as file:\n",
        "          for segment, (start_time, end_time) in zip(segments, segment_times):\n",
        "\n",
        "            rel_length = torch.tensor([1.0])\n",
        "\n",
        "            emb = self.encode_batch(segment, rel_length)\n",
        "            out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "            score, index = torch.max(out_prob, dim=-1)\n",
        "            text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "            file.write(f\"{start_time:.2f} {end_time:.2f} {text_lab}\\n\")\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_batch(wavs, wav_lens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_2_spk_2_mixture.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "segments  = audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "id": "Jmb-5p4ab-Yu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "    if len(wavs.shape) == 1:\n",
        "        wavs = wavs.unsqueeze(0)\n",
        "\n",
        "    wavs = wavs.to(self.device).float()\n",
        "\n",
        "    # Compute features without passing `lengths` if it causes issues\n",
        "    feats = self.mods.compute_features(wavs)\n",
        "    # Use `lengths` here if the layer expects it, otherwise adjust as needed\n",
        "    feats = self.mods.mean_var_norm(feats, wav_lens) if wav_lens is not None else feats\n",
        "    embeddings = self.mods.embedding_model(feats)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "T7P0rOU_M0fW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUjFS_gQO_E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "fJgLbyjBLvYM",
        "outputId": "f95d11a1-6004-463b-c0a2-55e9da4e43ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "encode_batch() missing 1 required positional argument: 'self'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5225224f95a2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: encode_batch() missing 1 required positional argument: 'self'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/SaveECAPAsampleinterface"
      ],
      "metadata": {
        "id": "kv1jU8pG-PDj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file SpeakerCounter.py\n",
        "\n",
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs)\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def classify_batch(self, wavs, wav_lens=None):\n",
        "        emb = self.encode_batch(wavs, wav_lens)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        # text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index\n",
        "        # return out_prob, score, index, text_lab\n",
        "\n",
        "    def classify_file(self, path, **kwargs):\n",
        "        waveform = self.load_audio(path, **kwargs)\n",
        "        # Fake a batch:\n",
        "        batch = waveform.unsqueeze(0)\n",
        "        rel_length = torch.tensor([1.0])\n",
        "        emb = self.encode_batch(batch, rel_length)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index, text_lab\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_batch(wavs, wav_lens)"
      ],
      "metadata": {
        "id": "8qwhaMK5i-DS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef12809b-7df1-4a3b-ecaa-339359593cbb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting SpeakerCounter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SpeakerCounter import SpeakerCounter\n",
        "wav_path = \"/content/session_2_spk_2_mixture.wav\"\n",
        "# wav_path = \"/content/session_4_spk_1_mixture_004_segment.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "# Instantiate your class using from_hparams\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "# audio_classifier.hparams.label_encoder.ignore_len()\n",
        "# signal, fs = torchaudio.load(wav_path)\n",
        "# # pred = audio_classifier.classify_file(wav_path)\n",
        "# embeddings = audio_classifier.encode_batch(signal)\n",
        "# prediction = audio_classifier.classify_batch(signal)\n",
        "# print(prediction)\n",
        "\n",
        "\"\"\"or \"\"\"\n",
        "audio_classifier.classify_file(wav_path)"
      ],
      "metadata": {
        "id": "a9H0xhvyjBy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b14dc6a4-b180-4064-8c10-65346e8dcac9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"session_2_spk_2_mixture.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7da782cced87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7da782c7f75f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7da7828ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7da7828cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7da6bdc3a58e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7da6bdc32147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5bebc993410e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5bebc9942c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5bebc993f087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5bebc992ae2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7da7bf592244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5bebc9929c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5bebc993ea64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5bebc992aa1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #24: <unknown function> + 0x16893e (0x5bebc994293e in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #26: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #27: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #28: <unknown function> + 0x13f9c6 (0x5bebc99199c6 in /usr/bin/python3)\nframe #29: PyEval_EvalCode + 0x86 (0x5bebc9a0f256 in /usr/bin/python3)\nframe #30: <unknown function> + 0x23ae2d (0x5bebc9a14e2d in /usr/bin/python3)\nframe #31: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #33: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #35: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #36: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #37: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #38: <unknown function> + 0x2557af (0x5bebc9a2f7af in /usr/bin/python3)\nframe #39: <unknown function> + 0x1662ca (0x5bebc99402ca in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #43: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #45: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #46: PyObject_Call + 0x122 (0x5bebc9943492 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #48: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #49: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #50: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #51: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #52: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #53: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #54: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #56: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #58: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #59: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #60: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #61: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #62: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a038b7f68ed2>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\"\"\"or \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0maudio_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/SpeakerCounter.py\u001b[0m in \u001b[0;36mclassify_file\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mout_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/interfaces.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(self, path, savedir)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_normalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"session_2_spk_2_mixture.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7da782cced87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7da782c7f75f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7da7828ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7da7828cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7da6bdc3a58e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7da6bdc32147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5bebc993410e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5bebc9942c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5bebc993f087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5bebc992ae2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7da7bf592244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5bebc9929c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5bebc993ea64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5bebc992aa1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #24: <unknown function> + 0x16893e (0x5bebc994293e in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #26: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #27: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #28: <unknown function> + 0x13f9c6 (0x5bebc99199c6 in /usr/bin/python3)\nframe #29: PyEval_EvalCode + 0x86 (0x5bebc9a0f256 in /usr/bin/python3)\nframe #30: <unknown function> + 0x23ae2d (0x5bebc9a14e2d in /usr/bin/python3)\nframe #31: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #33: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #35: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #36: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #37: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #38: <unknown function> + 0x2557af (0x5bebc9a2f7af in /usr/bin/python3)\nframe #39: <unknown function> + 0x1662ca (0x5bebc99402ca in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #43: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #45: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #46: PyObject_Call + 0x122 (0x5bebc9943492 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #48: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #49: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #50: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #51: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #52: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #53: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #54: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #56: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #58: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #59: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #60: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #61: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #62: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "def divide_audio_into_segments(audio_path, segment_length=2):\n",
        "    # Load the audio file\n",
        "    sample_rate = 16000\n",
        "    waveform, _ = torchaudio.load(audio_path)\n",
        "\n",
        "    # Calculate the number of samples for the given segment length\n",
        "    num_samples_per_segment = sample_rate * segment_length\n",
        "\n",
        "    # Calculate the total number of segments, using standard Python operations for ceiling\n",
        "    total_segments = int(-(-waveform.size(1) // num_samples_per_segment))  # Ceiling division\n",
        "\n",
        "    # Process and save each segment\n",
        "    for i in range(total_segments):\n",
        "        # Calculate the start and end sample for the current segment\n",
        "        start_sample = i * num_samples_per_segment\n",
        "        end_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "        # If the end sample exceeds the waveform length, adjust it to the waveform length\n",
        "        end_sample = min(end_sample, waveform.size(1))\n",
        "\n",
        "        # Extract the segment\n",
        "        segment = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        # Save the segment to a file\n",
        "        segment_file_name = f'/content/samples/segment_{i + 1}.wav'\n",
        "        torchaudio.save(segment_file_name, segment, sample_rate)\n",
        "\n",
        "divide_audio_into_segments(wav_path)"
      ],
      "metadata": {
        "id": "CPMYmQ-PcCol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UePYM0IeeyIG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}