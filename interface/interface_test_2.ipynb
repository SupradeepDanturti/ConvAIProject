{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "6bCpuKZqJrDa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install speechbrain"
   ],
   "metadata": {
    "id": "fFO7-WNnbdjH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "!unzip /content/drive/MyDrive/ConvAI/interface_files.zip"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6R9k1rJ7Yp1",
    "outputId": "94f39c0e-8d33-43f0-8b85-ac1afb12cc1a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n",
      "Archive:  /content/drive/MyDrive/ConvAI/interface_files.zip\n",
      "  inflating: classifier.ckpt         \n",
      "  inflating: embedding_model.ckpt    \n",
      "  inflating: hyperparams.yaml        \n",
      "  inflating: label_encoder.txt       \n",
      "  inflating: session_2_spk_2_mixture.wav  \n",
      "  inflating: session_4_spk_1_mixture_004_segment.wav  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4KV97OetbYED",
    "ExecuteTime": {
     "end_time": "2024-04-13T16:09:49.652109Z",
     "start_time": "2024-04-13T16:09:49.627108Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from speechbrain.inference.interfaces import Pretrained\n",
    "import torchaudio\n",
    "import math\n",
    "from speechbrain.utils.data_utils import split_path\n",
    "from speechbrain.utils.fetching import fetch\n",
    "\n",
    "class SpeakerCounter(Pretrained):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sample_rate = self.hparams.sample_rate\n",
    "\n",
    "    MODULES_NEEDED = [\n",
    "        \"compute_features\",\n",
    "        \"mean_var_norm\",\n",
    "        \"embedding_model\",\n",
    "        \"classifier\",\n",
    "    ]\n",
    "\n",
    "    def resample_waveform(self, waveform, orig_sample_rate):\n",
    "        \"\"\"\n",
    "        Resample the waveform to a new sample rate.\n",
    "        \"\"\"\n",
    "        if orig_sample_rate != self.sample_rate:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resample_transform(waveform)\n",
    "        return waveform\n",
    "\n",
    "    def merge_overlapping_segments(self, segments):\n",
    "      if not segments:\n",
    "          return []\n",
    "      merged = [segments[0]]\n",
    "      for current in segments[1:]:\n",
    "          prev = merged[-1]\n",
    "          if current[0] <= prev[1]:\n",
    "              if current[2] == prev[2]:\n",
    "                  merged[-1] = (prev[0], max(prev[1], current[1]), prev[2])\n",
    "              else:\n",
    "                  merged.append(current)\n",
    "          else:\n",
    "              merged.append(current)\n",
    "      return merged\n",
    "\n",
    "    def refine_transitions(self, aggregated_predictions):\n",
    "        \"\"\"\n",
    "        Refines transition times by potentially adjusting them to be at the start\n",
    "        or end of segments, aiming to make the transitions smoother and more accurate.\n",
    "        \"\"\"\n",
    "        refined_predictions = []\n",
    "        for i in range(len(aggregated_predictions)):\n",
    "            if i == 0:\n",
    "                refined_predictions.append(aggregated_predictions[i])\n",
    "                continue\n",
    "\n",
    "            current_start, current_end, current_label = aggregated_predictions[i]\n",
    "            prev_start, prev_end, prev_label = aggregated_predictions[i-1]\n",
    "\n",
    "            if current_start - prev_end <= 1.0:\n",
    "                new_start = prev_end\n",
    "            else:\n",
    "                new_start = current_start\n",
    "\n",
    "            refined_predictions.append((new_start, current_end, current_label))\n",
    "\n",
    "        return refined_predictions\n",
    "\n",
    "    def refine_transitions_with_confidence(self, aggregated_predictions, segment_confidences):\n",
    "        refined_predictions = []\n",
    "        for i in range(len(aggregated_predictions)):\n",
    "            if i == 0:\n",
    "                refined_predictions.append(aggregated_predictions[i])\n",
    "                continue\n",
    "\n",
    "            current_start, current_end, current_label = aggregated_predictions[i]\n",
    "            prev_start, prev_end, prev_label, prev_confidence = refined_predictions[-1] + (segment_confidences[i-1],)\n",
    "\n",
    "            current_confidence = segment_confidences[i]\n",
    "\n",
    "            if current_label != prev_label:\n",
    "                if prev_confidence < current_confidence:\n",
    "                    transition_point = current_start\n",
    "                else:\n",
    "                    transition_point = prev_end\n",
    "                refined_predictions[-1] = (prev_start, transition_point, prev_label)\n",
    "                refined_predictions.append((transition_point, current_end, current_label))\n",
    "            else:\n",
    "                if prev_confidence < current_confidence:\n",
    "                    refined_predictions[-1] = (prev_start, current_end, current_label)\n",
    "                else:\n",
    "                    refined_predictions.append((current_start, current_end, current_label))\n",
    "\n",
    "        return refined_predictions\n",
    "\n",
    "\n",
    "\n",
    "    def aggregate_segments_with_overlap(self, segment_predictions):\n",
    "        aggregated_predictions = []\n",
    "        last_start, last_end, last_label = segment_predictions[0]\n",
    "\n",
    "        for start, end, label in segment_predictions[1:]:\n",
    "            if label == last_label and start <= last_end:\n",
    "                last_end = max(last_end, end)\n",
    "            else:\n",
    "                aggregated_predictions.append((last_start, last_end, last_label))\n",
    "                last_start, last_end, last_label = start, end, label\n",
    "\n",
    "        aggregated_predictions.append((last_start, last_end, last_label))\n",
    "\n",
    "        merged = self.merge_overlapping_segments(aggregated_predictions)\n",
    "        return merged\n",
    "\n",
    "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
    "        if len(wavs.shape) == 1:\n",
    "            wavs = wavs.unsqueeze(0)\n",
    "\n",
    "        if wav_lens is None:\n",
    "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
    "\n",
    "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
    "        wavs = wavs.float()\n",
    "\n",
    "        # Computing features and embeddings\n",
    "        feats = self.mods.compute_features(wavs)\n",
    "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
    "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
    "        return embeddings\n",
    "\n",
    "    def create_segments(self, waveform, segment_length, overlap):\n",
    "        num_samples = waveform.shape[1]\n",
    "        segment_samples = int(segment_length * self.sample_rate)\n",
    "        overlap_samples = int(overlap * self.sample_rate)\n",
    "        step_samples = segment_samples - overlap_samples\n",
    "        segments = []\n",
    "        segment_times = []\n",
    "\n",
    "        for start in range(0, num_samples - segment_samples + 1, step_samples):\n",
    "            end = start + segment_samples\n",
    "            segments.append(waveform[:, start:end])\n",
    "            start_time = start / self.sample_rate\n",
    "            end_time = end / self.sample_rate\n",
    "            segment_times.append((start_time, end_time))\n",
    "\n",
    "        return segments, segment_times\n",
    "\n",
    "    def classify_file(self, path, segment_length=2.0, overlap=0.47, **kwargs):\n",
    "        \"\"\"Adjusted to handle overlapped segment predictions and refining transitions\"\"\"\n",
    "        waveform, osr = torchaudio.load(path)\n",
    "        waveform = self.resample_waveform(waveform, osr)\n",
    "\n",
    "\n",
    "        \"\"\" Attempt - Overlap Segments \"\"\"\n",
    "        segments, segment_times = self.create_segments(waveform, segment_length, overlap)\n",
    "        segment_predictions = []\n",
    "\n",
    "        for segment, (start_time, end_time) in zip(segments, segment_times):\n",
    "            rel_length = torch.tensor([1.0])\n",
    "            emb = self.encode_batch(segment, rel_length)\n",
    "            out_prob = self.mods.classifier(emb).squeeze(1)\n",
    "            score, index = torch.max(out_prob, dim=-1)\n",
    "            text_lab = self.hparams.label_encoder.decode_torch(index)\n",
    "            segment_predictions.append((start_time, end_time, text_lab[0]))\n",
    "\n",
    "        aggregated_predictions = self.aggregate_segments_with_overlap(segment_predictions)\n",
    "        refined_predictions = self.refine_transitions(aggregated_predictions)\n",
    "        preds = self.refine_transitions_with_confidence(aggregated_predictions , refined_predictions)\n",
    "\n",
    "\n",
    "        with open(\"sample_segment_predictions.txt\", \"w\") as file:\n",
    "            for start_time, end_time, prediction in preds:\n",
    "                speaker_text = \"no speech\" if str(prediction) == \"0\" else (\"1 speaker\" if str(prediction) == \"1\" else f\"{prediction} speakers\")\n",
    "                print(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\")\n",
    "                file.write(f\"{start_time:.2f}-{end_time:.2f} has {speaker_text}\\n\")\n",
    "\n",
    "        \"\"\" End of Attempt - Overlap Segments \"\"\"\n",
    "\n",
    "    def forward(self, wavs, wav_lens=None):\n",
    "        \"\"\"Runs the classification\"\"\"\n",
    "        return self.classify_file(wavs, wav_lens)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from SpeakerCounter import SpeakerCounter\n",
    "wav_path = \"./session_2_spk_2_mixture.wav\"\n",
    "save_dir = \"./\"\n",
    "model_path = \"./model_dir\"\n",
    "\n",
    "# Instantiate your class using from_hparams\n",
    "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
    "\n",
    "audio_classifier.classify_file(wav_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jmb-5p4ab-Yu",
    "outputId": "7f628ba8-8c85-4055-94eb-c03f030f4798",
    "ExecuteTime": {
     "end_time": "2024-04-13T16:09:50.058439Z",
     "start_time": "2024-04-13T16:09:49.982907Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embedding_model.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./model_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Instantiate your class using from_hparams\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m audio_classifier \u001B[38;5;241m=\u001B[39m \u001B[43mSpeakerCounter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_hparams\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msavedir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m audio_classifier\u001B[38;5;241m.\u001B[39mclassify_file(wav_path)\n",
      "File \u001B[1;32mD:\\Winter 24\\COMP 691 X- Conversational AI\\Project\\ConvAIProject\\venv\\lib\\site-packages\\speechbrain\\inference\\interfaces.py:494\u001B[0m, in \u001B[0;36mPretrained.from_hparams\u001B[1;34m(cls, source, hparams_file, pymodule_file, overrides, savedir, use_auth_token, revision, download_only, huggingface_cache_dir, **kwargs)\u001B[0m\n\u001B[0;32m    491\u001B[0m \u001B[38;5;66;03m# Load on the CPU. Later the params can be moved elsewhere by specifying\u001B[39;00m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m download_only:\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;66;03m# run_opts={\"device\": ...}\u001B[39;00m\n\u001B[1;32m--> 494\u001B[0m     \u001B[43mpretrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_collected\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    496\u001B[0m     \u001B[38;5;66;03m# Now return the system\u001B[39;00m\n\u001B[0;32m    497\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(hparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodules\u001B[39m\u001B[38;5;124m\"\u001B[39m], hparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Winter 24\\COMP 691 X- Conversational AI\\Project\\ConvAIProject\\venv\\lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:307\u001B[0m, in \u001B[0;36mPretrainer.load_collected\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    303\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m    304\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRedirecting (loading from local path): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparamfiles[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m -> \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpaths[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    305\u001B[0m         )\n\u001B[0;32m    306\u001B[0m         paramfiles[name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpaths[name]\n\u001B[1;32m--> 307\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_load_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparamfiles\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Winter 24\\COMP 691 X- Conversational AI\\Project\\ConvAIProject\\venv\\lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:324\u001B[0m, in \u001B[0;36mPretrainer._call_load_hooks\u001B[1;34m(self, paramfiles)\u001B[0m\n\u001B[0;32m    322\u001B[0m default_hook \u001B[38;5;241m=\u001B[39m get_default_hook(obj, DEFAULT_TRANSFER_HOOKS)\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m default_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 324\u001B[0m     \u001B[43mdefault_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloadpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;66;03m# Otherwise find the default loader for that type:\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Winter 24\\COMP 691 X- Conversational AI\\Project\\ConvAIProject\\venv\\lib\\site-packages\\speechbrain\\utils\\checkpoints.py:152\u001B[0m, in \u001B[0;36mtorch_parameter_transfer\u001B[1;34m(obj, path)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Non-strict Torch Module state_dict load.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \n\u001B[0;32m    133\u001B[0m \u001B[38;5;124;03mLoads a set of parameters from path to obj. If obj has layers for which\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;124;03m    The object is modified in place.\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    150\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    151\u001B[0m incompatible_keys \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mload_state_dict(\n\u001B[1;32m--> 152\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, strict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    153\u001B[0m )\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m missing_key \u001B[38;5;129;01min\u001B[39;00m incompatible_keys\u001B[38;5;241m.\u001B[39mmissing_keys:\n\u001B[0;32m    155\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    156\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDuring parameter transfer to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m loading from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, the transferred parameters did not have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameters for the key: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmissing_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    159\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Winter 24\\COMP 691 X- Conversational AI\\Project\\ConvAIProject\\venv\\lib\\site-packages\\torch\\serialization.py:998\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    995\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    996\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 998\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m   1000\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mD:\\Winter 24\\COMP 691 X- Conversational AI\\Project\\ConvAIProject\\venv\\lib\\site-packages\\torch\\serialization.py:445\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 445\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mD:\\Winter 24\\COMP 691 X- Conversational AI\\Project\\ConvAIProject\\venv\\lib\\site-packages\\torch\\serialization.py:426\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 426\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'embedding_model.ckpt'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MUjFS_gQO_E7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "segments = [\n",
    "    (0.75, 1.25, 1), (1.00, 13.00, 2), (12.75, 13.25, 1),\n",
    "    (13.75, 14.25, 1), (14.00, 16.00, 2), (15.75, 18.25, 1),\n",
    "    (18.00, 59.00, 2), (58.75, 59.25, 1), (59.75, 60.25, 1),\n",
    "    (60.00, 64.00, 2), (63.75, 64.25, 1), (64.75, 65.25, 1),\n",
    "    (65.00, 96.00, 2), (95.75, 98.00, 1), (97.75, 98.25, 3),\n",
    "    (98.00, 101.00, 2), (100.75, 104.25, 1), (104.75, 107.25, 1),\n",
    "    (107.00, 119.00, 2), (118.75, 145.00, 1)\n",
    "]\n",
    "\n",
    "# Simplifying the logic: Directly reflect the provided segments in a more readable format\n",
    "# Assuming the given segments are accurate and only need to be presented continuously\n",
    "\n",
    "continuous_segments = []\n",
    "\n",
    "# Initialize with the first segment\n",
    "last_end, last_speaker = segments[0][1], segments[0][2]\n",
    "continuous_segments.append([segments[0][0], segments[0][1], segments[0][2]])\n",
    "\n",
    "for i in range(1, len(segments)):\n",
    "    start, end, speaker = segments[i]\n",
    "\n",
    "    # Check if the current segment continues directly from the last with the same speaker count\n",
    "    if last_speaker == speaker and last_end >= start:\n",
    "        # Update the end time if the current segment extends the period of the same speaker count\n",
    "        continuous_segments[-1][1] = max(continuous_segments[-1][1], end)\n",
    "    else:\n",
    "        # Otherwise, start a new segment\n",
    "        continuous_segments.append([start, end, speaker])\n",
    "\n",
    "    # Update the last end time and speaker count for the next iteration\n",
    "    last_end, last_speaker = end, speaker\n",
    "\n",
    "# Adjust the formatting for output\n",
    "formatted_segments = [f\"{seg[0]:.2f}-{seg[1]:.2f} has {seg[2]} speaker/s\" for seg in continuous_segments]\n",
    "formatted_segments"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJgLbyjBLvYM",
    "outputId": "3843d473-5e61-4197-9a3c-9416fc852276"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['0.75-1.25 has 1 speaker/s',\n",
       " '1.00-13.00 has 2 speaker/s',\n",
       " '12.75-13.25 has 1 speaker/s',\n",
       " '13.75-14.25 has 1 speaker/s',\n",
       " '14.00-16.00 has 2 speaker/s',\n",
       " '15.75-18.25 has 1 speaker/s',\n",
       " '18.00-59.00 has 2 speaker/s',\n",
       " '58.75-59.25 has 1 speaker/s',\n",
       " '59.75-60.25 has 1 speaker/s',\n",
       " '60.00-64.00 has 2 speaker/s',\n",
       " '63.75-64.25 has 1 speaker/s',\n",
       " '64.75-65.25 has 1 speaker/s',\n",
       " '65.00-96.00 has 2 speaker/s',\n",
       " '95.75-98.00 has 1 speaker/s',\n",
       " '97.75-98.25 has 3 speaker/s',\n",
       " '98.00-101.00 has 2 speaker/s',\n",
       " '100.75-104.25 has 1 speaker/s',\n",
       " '104.75-107.25 has 1 speaker/s',\n",
       " '107.00-119.00 has 2 speaker/s',\n",
       " '118.75-145.00 has 1 speaker/s']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf /content/SaveECAPAsampleinterface"
   ],
   "metadata": {
    "id": "kv1jU8pG-PDj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%file SpeakerCounter.py\n",
    "\n",
    "import torch\n",
    "from speechbrain.inference.interfaces import Pretrained\n",
    "import torchaudio\n",
    "import math\n",
    "from speechbrain.utils.data_utils import split_path\n",
    "from speechbrain.utils.fetching import fetch\n",
    "\n",
    "class SpeakerCounter(Pretrained):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sample_rate = self.hparams.sample_rate\n",
    "\n",
    "    MODULES_NEEDED = [\n",
    "        \"compute_features\",\n",
    "        \"mean_var_norm\",\n",
    "        \"embedding_model\",\n",
    "        \"classifier\",\n",
    "    ]\n",
    "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
    "\n",
    "\n",
    "        # Computing features and embeddings\n",
    "        feats = self.mods.compute_features(wavs)\n",
    "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
    "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
    "        return embeddings\n",
    "\n",
    "    def classify_batch(self, wavs, wav_lens=None):\n",
    "        emb = self.encode_batch(wavs, wav_lens)\n",
    "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
    "        score, index = torch.max(out_prob, dim=-1)\n",
    "        # text_lab = self.hparams.label_encoder.decode_torch(index)\n",
    "        return out_prob, score, index\n",
    "        # return out_prob, score, index, text_lab\n",
    "\n",
    "    def classify_file(self, path, **kwargs):\n",
    "        waveform = self.load_audio(path, **kwargs)\n",
    "        # Fake a batch:\n",
    "        batch = waveform.unsqueeze(0)\n",
    "        rel_length = torch.tensor([1.0])\n",
    "        emb = self.encode_batch(batch, rel_length)\n",
    "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
    "        score, index = torch.max(out_prob, dim=-1)\n",
    "        text_lab = self.hparams.label_encoder.decode_torch(index)\n",
    "        return out_prob, score, index, text_lab\n",
    "\n",
    "    def forward(self, wavs, wav_lens=None):\n",
    "        \"\"\"Runs the classification\"\"\"\n",
    "        return self.classify_batch(wavs, wav_lens)"
   ],
   "metadata": {
    "id": "8qwhaMK5i-DS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef12809b-7df1-4a3b-ecaa-339359593cbb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting SpeakerCounter.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from SpeakerCounter import SpeakerCounter\n",
    "wav_path = \"/content/session_2_spk_2_mixture.wav\"\n",
    "# wav_path = \"/content/session_4_spk_1_mixture_004_segment.wav\"\n",
    "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
    "model_path = \"/content\"\n",
    "\n",
    "# Instantiate your class using from_hparams\n",
    "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
    "\n",
    "# audio_classifier.hparams.label_encoder.ignore_len()\n",
    "# signal, fs = torchaudio.load(wav_path)\n",
    "# # pred = audio_classifier.classify_file(wav_path)\n",
    "# embeddings = audio_classifier.encode_batch(signal)\n",
    "# prediction = audio_classifier.classify_batch(signal)\n",
    "# print(prediction)\n",
    "\n",
    "\"\"\"or \"\"\"\n",
    "audio_classifier.classify_file(wav_path)"
   ],
   "metadata": {
    "id": "a9H0xhvyjBy7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "b14dc6a4-b180-4064-8c10-65346e8dcac9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"session_2_spk_2_mixture.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7da782cced87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7da782c7f75f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7da7828ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7da7828cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7da6bdc3a58e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7da6bdc32147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5bebc993410e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5bebc9942c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5bebc993f087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5bebc992ae2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7da7bf592244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5bebc9929c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5bebc993ea64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5bebc992aa1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #24: <unknown function> + 0x16893e (0x5bebc994293e in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #26: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #27: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #28: <unknown function> + 0x13f9c6 (0x5bebc99199c6 in /usr/bin/python3)\nframe #29: PyEval_EvalCode + 0x86 (0x5bebc9a0f256 in /usr/bin/python3)\nframe #30: <unknown function> + 0x23ae2d (0x5bebc9a14e2d in /usr/bin/python3)\nframe #31: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #33: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #35: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #36: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #37: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #38: <unknown function> + 0x2557af (0x5bebc9a2f7af in /usr/bin/python3)\nframe #39: <unknown function> + 0x1662ca (0x5bebc99402ca in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #43: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #45: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #46: PyObject_Call + 0x122 (0x5bebc9943492 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #48: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #49: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #50: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #51: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #52: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #53: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #54: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #56: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #58: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #59: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #60: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #61: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #62: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\n",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-34-a038b7f68ed2>\u001B[0m in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\"\"\"or \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m \u001B[0maudio_classifier\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclassify_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwav_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/content/SpeakerCounter.py\u001B[0m in \u001B[0;36mclassify_file\u001B[0;34m(self, path, **kwargs)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclassify_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwavs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwav_lens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m         \u001B[0memb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwavs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwav_lens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m         \u001B[0mout_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmods\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0mscore\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout_prob\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/interfaces.py\u001B[0m in \u001B[0;36mload_audio\u001B[0;34m(self, path, savedir)\u001B[0m\n\u001B[1;32m    280\u001B[0m         \u001B[0msource\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m         \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msource\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msource\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msavedir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msavedir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 282\u001B[0;31m         \u001B[0msignal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchaudio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchannels_first\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    283\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maudio_normalizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msignal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \"\"\"\n\u001B[1;32m    204\u001B[0m         \u001B[0mbackend\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdispatcher\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_frames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnormalize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchannels_first\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mload\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001B[0m\n\u001B[1;32m    295\u001B[0m         \u001B[0mbuffer_size\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m4096\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    296\u001B[0m     ) -> Tuple[torch.Tensor, int]:\n\u001B[0;32m--> 297\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mload_audio\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_frames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnormalize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchannels_first\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    298\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    299\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001B[0m in \u001B[0;36mload_audio\u001B[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001B[0m\n\u001B[1;32m     86\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"read\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mformat\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"vorbis\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m         \u001B[0mformat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"ogg\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m     \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchaudio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStreamReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m     \u001B[0msample_rate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_src_stream_info\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_audio_stream\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample_rate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[0mfilter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_load_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_frames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconvert\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, format, option, buffer_size)\u001B[0m\n\u001B[1;32m    524\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_be\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mffmpeg_ext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStreamingMediaDecoderFileObj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moption\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    525\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 526\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_be\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mffmpeg_ext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStreamingMediaDecoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormpath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moption\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    527\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_be\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_best_audio_stream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to open the input \"session_2_spk_2_mixture.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7da782cced87 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7da782c7f75f in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x7da7828ca904 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7da7828cd304 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a58e (0x7da6bdc3a58e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32147 (0x7da6bdc32147 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15a10e (0x5bebc993410e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #8: <unknown function> + 0x168c20 (0x5bebc9942c20 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165087 (0x5bebc993f087 in /usr/bin/python3)\nframe #10: <unknown function> + 0x150e2b (0x5bebc992ae2b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf244 (0x7da7bf592244 in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5bebc992aa7b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5bebc9929c14 in /usr/bin/python3)\nframe #15: <unknown function> + 0x164a64 (0x5bebc993ea64 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5bebc992aa1c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6a79 (0x5bebc9923629 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #24: <unknown function> + 0x16893e (0x5bebc994293e in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #26: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #27: _PyEval_EvalFrameDefault + 0x614a (0x5bebc9922cfa in /usr/bin/python3)\nframe #28: <unknown function> + 0x13f9c6 (0x5bebc99199c6 in /usr/bin/python3)\nframe #29: PyEval_EvalCode + 0x86 (0x5bebc9a0f256 in /usr/bin/python3)\nframe #30: <unknown function> + 0x23ae2d (0x5bebc9a14e2d in /usr/bin/python3)\nframe #31: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #33: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #35: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #36: _PyEval_EvalFrameDefault + 0x2568 (0x5bebc991f118 in /usr/bin/python3)\nframe #37: <unknown function> + 0x177ff0 (0x5bebc9951ff0 in /usr/bin/python3)\nframe #38: <unknown function> + 0x2557af (0x5bebc9a2f7af in /usr/bin/python3)\nframe #39: <unknown function> + 0x1662ca (0x5bebc99402ca in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #43: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #44: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #45: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #46: PyObject_Call + 0x122 (0x5bebc9943492 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x2a27 (0x5bebc991f5d7 in /usr/bin/python3)\nframe #48: <unknown function> + 0x1687f1 (0x5bebc99427f1 in /usr/bin/python3)\nframe #49: _PyEval_EvalFrameDefault + 0x198c (0x5bebc991e53c in /usr/bin/python3)\nframe #50: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #51: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #52: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #53: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #54: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\nframe #56: _PyFunction_Vectorcall + 0x7c (0x5bebc99349fc in /usr/bin/python3)\nframe #57: _PyEval_EvalFrameDefault + 0x8ac (0x5bebc991d45c in /usr/bin/python3)\nframe #58: <unknown function> + 0x200175 (0x5bebc99da175 in /usr/bin/python3)\nframe #59: <unknown function> + 0x15ac59 (0x5bebc9934c59 in /usr/bin/python3)\nframe #60: <unknown function> + 0x236bc5 (0x5bebc9a10bc5 in /usr/bin/python3)\nframe #61: <unknown function> + 0x2b2572 (0x5bebc9a8c572 in /usr/bin/python3)\nframe #62: <unknown function> + 0x14d99b (0x5bebc992799b in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6bd (0x5bebc991d26d in /usr/bin/python3)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "def divide_audio_into_segments(audio_path, segment_length=2):\n",
    "    # Load the audio file\n",
    "    sample_rate = 16000\n",
    "    waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "    # Calculate the number of samples for the given segment length\n",
    "    num_samples_per_segment = sample_rate * segment_length\n",
    "\n",
    "    # Calculate the total number of segments, using standard Python operations for ceiling\n",
    "    total_segments = int(-(-waveform.size(1) // num_samples_per_segment))  # Ceiling division\n",
    "\n",
    "    # Process and save each segment\n",
    "    for i in range(total_segments):\n",
    "        # Calculate the start and end sample for the current segment\n",
    "        start_sample = i * num_samples_per_segment\n",
    "        end_sample = start_sample + num_samples_per_segment\n",
    "\n",
    "        # If the end sample exceeds the waveform length, adjust it to the waveform length\n",
    "        end_sample = min(end_sample, waveform.size(1))\n",
    "\n",
    "        # Extract the segment\n",
    "        segment = waveform[:, start_sample:end_sample]\n",
    "\n",
    "        # Save the segment to a file\n",
    "        segment_file_name = f'/content/samples/segment_{i + 1}.wav'\n",
    "        torchaudio.save(segment_file_name, segment, sample_rate)\n",
    "\n",
    "divide_audio_into_segments(wav_path)"
   ],
   "metadata": {
    "id": "CPMYmQ-PcCol"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "UePYM0IeeyIG"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
