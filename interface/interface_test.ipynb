{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "L8_uFkn4omQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1afa3l2nzNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b39af9-ba75-4be0-8eb0-509e20339865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hyperparams.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hyperparams.yaml\n",
        "\n",
        "#Feature parameters\n",
        "sample_rate: 16000\n",
        "n_mels: 80\n",
        "\n",
        "#Model parameters\n",
        "n_classes: 5\n",
        "\n",
        "#model\n",
        "label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder\n",
        "\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN\n",
        "    input_size: !ref <n_mels>\n",
        "    channels: [256, 256, 256, 256, 768]\n",
        "    kernel_sizes: [5, 3, 3, 3, 1]\n",
        "    dilations: [1, 2, 3, 4, 1]\n",
        "    attention_channels: 128\n",
        "    lin_neurons: 192\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier\n",
        "    input_size: 192\n",
        "    out_neurons: !ref <n_classes>\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "\n",
        "pretrained_path: /content\n",
        "\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "    loadables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        label_encoder: !ref <label_encoder>\n",
        "    paths:\n",
        "        embedding_model: !ref <pretrained_path>/embedding_model.ckpt\n",
        "        classifier: !ref <pretrained_path>/classifier.ckpt\n",
        "        label_encoder: !ref <pretrained_path>/label_encoder.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file Counter.py\n",
        "\n",
        "import torch\n",
        "from speechbrain.inference.interfaces import Pretrained\n",
        "import torchaudio\n",
        "import math\n",
        "from speechbrain.utils.data_utils import split_path\n",
        "from speechbrain.utils.fetching import fetch\n",
        "\n",
        "class SpeakerCounter(Pretrained):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.sample_rate = self.hparams.sample_rate\n",
        "\n",
        "    MODULES_NEEDED = [\n",
        "        \"compute_features\",\n",
        "        \"mean_var_norm\",\n",
        "        \"embedding_model\",\n",
        "        \"classifier\",\n",
        "    ]\n",
        "    def encode_batch(self, wavs, wav_lens=None, normalize=False):\n",
        "        # Manage single waveforms in input\n",
        "        if len(wavs.shape) == 1:\n",
        "            wavs = wavs.unsqueeze(0)\n",
        "\n",
        "        # Assign full length if wav_lens is not assigned\n",
        "        if wav_lens is None:\n",
        "            wav_lens = torch.ones(wavs.shape[0], device=self.device)\n",
        "\n",
        "        # Storing waveform in the specified device\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "        wavs = wavs.float()\n",
        "\n",
        "        # Computing features and embeddings\n",
        "        feats = self.mods.compute_features(wavs)\n",
        "        feats = self.mods.mean_var_norm(feats, wav_lens)\n",
        "        embeddings = self.mods.embedding_model(feats, wav_lens)\n",
        "        return embeddings\n",
        "\n",
        "    def classify_batch(self, wavs, wav_lens=None):\n",
        "        emb = self.encode_batch(wavs, wav_lens)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        # text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index\n",
        "        # return out_prob, score, index, text_lab\n",
        "\n",
        "    def classify_file(self, path, **kwargs):\n",
        "        waveform = self.load_audio(path, **kwargs)\n",
        "        # Fake a batch:\n",
        "        batch = waveform.unsqueeze(0)\n",
        "        rel_length = torch.tensor([1.0])\n",
        "        emb = self.encode_batch(batch, rel_length)\n",
        "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
        "        score, index = torch.max(out_prob, dim=-1)\n",
        "        text_lab = self.hparams.label_encoder.decode_torch(index)\n",
        "        return out_prob, score, index, text_lab\n",
        "\n",
        "    def forward(self, wavs, wav_lens=None):\n",
        "        \"\"\"Runs the classification\"\"\"\n",
        "        return self.classify_batch(wavs, wav_lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnalQbNvn3JI",
        "outputId": "cb55f986-2287-4fa9-8c46-0f735fbd681f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Counter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Counter as SpeakerCounter\n",
        "wav_path = \"/content/session_4_spk_1_mixture_004_segment.wav\"\n",
        "save_dir = \"/content/SaveECAPAsampleinterface\"\n",
        "model_path = \"/content\"\n",
        "\n",
        "audio_classifier = SpeakerCounter.from_hparams(source=model_path, savedir=save_dir)\n",
        "\n",
        "res = audio_classifier.classify_file(wav_path)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cslpc5n4n41Y",
        "outputId": "b2c64d66-8e5a-4b49-acbc-5b2ea3b552da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'Counter' has no attribute 'from_hparams'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ade556c9b0e7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0maudio_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpeakerCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_hparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'Counter' has no attribute 'from_hparams'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "def divide_audio_into_segments(audio_path, segment_length=2):\n",
        "    # Load the audio file\n",
        "    sample_rate = 16000\n",
        "    waveform, _ = torchaudio.load(audio_path)\n",
        "\n",
        "    # Calculate the number of samples for the given segment length\n",
        "    num_samples_per_segment = sample_rate * segment_length\n",
        "\n",
        "    # Calculate the total number of segments, using standard Python operations for ceiling\n",
        "    total_segments = int(-(-waveform.size(1) // num_samples_per_segment))  # Ceiling division\n",
        "\n",
        "    # Process and save each segment\n",
        "    for i in range(total_segments):\n",
        "        # Calculate the start and end sample for the current segment\n",
        "        start_sample = i * num_samples_per_segment\n",
        "        end_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "        # If the end sample exceeds the waveform length, adjust it to the waveform length\n",
        "        end_sample = min(end_sample, waveform.size(1))\n",
        "\n",
        "        # Extract the segment\n",
        "        segment = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        # Save the segment to a file\n",
        "        segment_file_name = f'/content/segment_{i + 1}.wav'\n",
        "        torchaudio.save(segment_file_name, segment, sample_rate)\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = '/content/session_2_spk_2_mixture.wav'\n",
        "divide_audio_into_segments(audio_file_path)"
      ],
      "metadata": {
        "id": "YjAKjKoLVxlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b10efc-9e14-4f13-dd4a-e95dbdf71aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: segment_1.wav\n",
            "Saved: segment_2.wav\n",
            "Saved: segment_3.wav\n",
            "Saved: segment_4.wav\n",
            "Saved: segment_5.wav\n",
            "Saved: segment_6.wav\n",
            "Saved: segment_7.wav\n",
            "Saved: segment_8.wav\n",
            "Saved: segment_9.wav\n",
            "Saved: segment_10.wav\n",
            "Saved: segment_11.wav\n",
            "Saved: segment_12.wav\n",
            "Saved: segment_13.wav\n",
            "Saved: segment_14.wav\n",
            "Saved: segment_15.wav\n",
            "Saved: segment_16.wav\n",
            "Saved: segment_17.wav\n",
            "Saved: segment_18.wav\n",
            "Saved: segment_19.wav\n",
            "Saved: segment_20.wav\n",
            "Saved: segment_21.wav\n",
            "Saved: segment_22.wav\n",
            "Saved: segment_23.wav\n",
            "Saved: segment_24.wav\n",
            "Saved: segment_25.wav\n",
            "Saved: segment_26.wav\n",
            "Saved: segment_27.wav\n",
            "Saved: segment_28.wav\n",
            "Saved: segment_29.wav\n",
            "Saved: segment_30.wav\n",
            "Saved: segment_31.wav\n",
            "Saved: segment_32.wav\n",
            "Saved: segment_33.wav\n",
            "Saved: segment_34.wav\n",
            "Saved: segment_35.wav\n",
            "Saved: segment_36.wav\n",
            "Saved: segment_37.wav\n",
            "Saved: segment_38.wav\n",
            "Saved: segment_39.wav\n",
            "Saved: segment_40.wav\n",
            "Saved: segment_41.wav\n",
            "Saved: segment_42.wav\n",
            "Saved: segment_43.wav\n",
            "Saved: segment_44.wav\n",
            "Saved: segment_45.wav\n",
            "Saved: segment_46.wav\n",
            "Saved: segment_47.wav\n",
            "Saved: segment_48.wav\n",
            "Saved: segment_49.wav\n",
            "Saved: segment_50.wav\n",
            "Saved: segment_51.wav\n",
            "Saved: segment_52.wav\n",
            "Saved: segment_53.wav\n",
            "Saved: segment_54.wav\n",
            "Saved: segment_55.wav\n",
            "Saved: segment_56.wav\n",
            "Saved: segment_57.wav\n",
            "Saved: segment_58.wav\n",
            "Saved: segment_59.wav\n",
            "Saved: segment_60.wav\n",
            "Saved: segment_61.wav\n",
            "Saved: segment_62.wav\n",
            "Saved: segment_63.wav\n",
            "Saved: segment_64.wav\n",
            "Saved: segment_65.wav\n",
            "Saved: segment_66.wav\n",
            "Saved: segment_67.wav\n",
            "Saved: segment_68.wav\n",
            "Saved: segment_69.wav\n",
            "Saved: segment_70.wav\n",
            "Saved: segment_71.wav\n",
            "Saved: segment_72.wav\n",
            "Saved: segment_73.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sae-4ZX4YAKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}