{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Should be written like this end result..'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Should be written like this end result..\"\"\"\n",
    "# from speechbrain.inference.classifiers import  EncoderClassifier\n",
    "# \n",
    "# classifier = EncoderClassifier.from_hparams(source=\"./XVector_Augmented_SpeakerCounter_results/XVector/Augmented/1986/save/CKPT+2024-03-23+10-00-05+00\", \n",
    "#                                             savedir=\"./sampleinference\")\n",
    "# \n",
    "# # Load an audio file\n",
    "# signal, fs = torchaudio.load(\"./maindata/eval/session_0_spk_3/session_0_spk_3_mixture.wav\")\n",
    "# \n",
    "# # Classify the audio to get the number of speakers and overlap information\n",
    "# prediction = classifier.classify_batch(signal)\n",
    "# prediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T17:46:31.620797Z",
     "start_time": "2024-03-27T17:46:31.598794Z"
    }
   },
   "id": "cc3a65555a9bfea1",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SpeakerCounter.py\n"
     ]
    }
   ],
   "source": [
    "%%file SpeakerCounter.py\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import Pretrained\n",
    "from speechbrain.utils.data_utils import split_path\n",
    "from speechbrain.utils.fetching import fetch\n",
    "\n",
    "class SpeakerOverlapDetector(Pretrained):\n",
    "    \"\"\"\n",
    "    A class for detecting speaker count and overlaps in audio segments.\n",
    "    This custom class is inspired by SpeechBrain's VAD and classification interfaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sample_rate = self.hparams.sample_rate\n",
    "\n",
    "    def load_audio(self, path, start=0, end=None):\n",
    "        \"\"\"Load a segment of an audio file.\"\"\"\n",
    "        waveform, fs = torchaudio.load(path, frame_offset=start, num_frames=end)\n",
    "        if fs != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=fs, new_freq=self.sample_rate)(waveform)\n",
    "        return waveform\n",
    "\n",
    "    def segment_audio(self, signal, segment_length=2.0, overlap=0.5):\n",
    "        \"\"\"Segment audio into overlapping chunks.\"\"\"\n",
    "        segment_length_samples = int(segment_length * self.sample_rate)\n",
    "        overlap_samples = int(overlap * self.sample_rate)\n",
    "        segments = []\n",
    "\n",
    "        start = 0\n",
    "        while start + segment_length_samples <= signal.size(1):\n",
    "            end = start + segment_length_samples\n",
    "            segments.append(signal[:, start:end])\n",
    "            start += (segment_length_samples - overlap_samples)\n",
    "        \n",
    "        if len(segments) == 0:\n",
    "            segments.append(signal)\n",
    "        \n",
    "        return torch.stack(segments, dim=0)\n",
    "\n",
    "    def predict_speakers(self, segments):\n",
    "        \"\"\"Predict the number of speakers in each segment.\"\"\"\n",
    "        # Placeholder for actual model prediction\n",
    "        # Replace this with your model's prediction logic\n",
    "        predictions = torch.randint(0, 3, (segments.shape[0],))\n",
    "        return predictions\n",
    "\n",
    "    def detect_speaker_changes(self, audio_path):\n",
    "        \"\"\"Detect changes in speaker count in an audio file.\"\"\"\n",
    "        signal = self.load_audio(audio_path)\n",
    "        segments = self.segment_audio(signal)\n",
    "        predictions = self.predict_speakers(segments)\n",
    "\n",
    "        changes = []\n",
    "        for i in range(1, len(predictions)):\n",
    "            if predictions[i] != predictions[i-1]:\n",
    "                changes.append((i, predictions[i].item()))\n",
    "        \n",
    "        return changes\n",
    "\n",
    "    def process_audio_file(self, audio_path):\n",
    "        \"\"\"Process an audio file to detect speaker overlaps and changes.\"\"\"\n",
    "        changes = self.detect_speaker_changes(audio_path)\n",
    "        for i, num_speakers in changes:\n",
    "            print(f\"Change detected at segment {i}: {num_speakers} speakers\")\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T17:13:41.809698Z",
     "start_time": "2024-03-27T17:13:41.797168Z"
    }
   },
   "id": "initial_id",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "detector = SpeakerOverlapDetector.from_hparams(source=\"your_model_directory\", savedir=\"your_savedir\")\n",
    "detector.process_audio_file(\"path_to_your_audio_file.wav\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3efa4270be8c1d35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
