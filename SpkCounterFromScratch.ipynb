{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupradeepDanturti/ConvAIProject/blob/main/SpkCounterFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TyN0s17Bdmw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/SupradeepDanturti/ConvAIProject\n",
        "%cd ConvAIProject\n",
        "!git config core.sparseCheckout true\n",
        "!echo '/*' > .git/info/sparse-checkout\n",
        "!echo '!results/' >> .git/info/sparse-checkout\n",
        "!echo '!interface/' >> .git/info/sparse-checkout\n",
        "!git read-tree -mu HEAD"
      ],
      "metadata": {
        "id": "QBwDM9wOBoLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare_dataset/download_required_data.py --output_folder ./data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1OzIhkTBrD7",
        "outputId": "102d4b3e-32f6-4582-b840-eac7c6180b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 0: Downloading LibriSpeech\n",
            "Downloading http://www.openslr.org/resources/12/test-clean.tar.gz to ./data/test-clean.tar.gz\n",
            "test-clean.tar.gz: 347MB [00:21, 16.1MB/s]               \n",
            "Downloading http://www.openslr.org/resources/12/dev-clean.tar.gz to ./data/dev-clean.tar.gz\n",
            "dev-clean.tar.gz: 338MB [00:19, 17.1MB/s]               \n",
            "Downloading http://www.openslr.org/resources/12/train-clean-100.tar.gz to ./data/train-clean-100.tar.gz\n",
            "train-clean-100.tar.gz: 6.39GB [05:48, 18.3MB/s]                \n",
            "Stage 1: Downloading RIRs and Noises\n",
            "Downloading http://www.openslr.org/resources/28/rirs_noises.zip to ./data/rirs_noises.zip\n",
            "rirs_noises.zip: 1.31GB [01:12, 18.1MB/s]                \n",
            "Extracting ./data/rirs_noises.zip to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat prepare_dataset/dataset.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5isMPI8B7UF",
        "outputId": "5a372bd6-9a00-485f-8fdb-b9fad19a5db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# This parameter file is used to create the dataset.\n",
            "\n",
            "\n",
            "out_folder: ../data # folder where created dataset will be stored.\n",
            "metadata_folder: !ref <out_folder>/metadata\n",
            "samplerate: 16000\n",
            "\n",
            "librispeech_root: !ref <out_folder>/LibriSpeech # e.g., /data/LibriSpeech/ (typically where you've downloaded the data)\n",
            "\n",
            "librispeech_folders:\n",
            "  train:\n",
            "    - !ref <out_folder>/LibriSpeech/train-clean-100/\n",
            "  dev:\n",
            "    - !ref <out_folder>/LibriSpeech/dev-clean/\n",
            "  eval:\n",
            "    - !ref <out_folder>/LibriSpeech/test-clean/\n",
            "\n",
            "rirs_noises_root: !ref <out_folder>/RIRS_NOISES\n",
            "rirs_folders:\n",
            "  - !ref <out_folder>/RIRS_NOISES/simulated_rirs/\n",
            "  - !ref <out_folder>/RIRS_NOISES/real_rirs_isotropic_noises\n",
            "noises_folders:\n",
            "  - !ref <out_folder>/RIRS_NOISES/pointsource_noises/\n",
            "\n",
            "\n",
            "# parameters\n",
            "seed: 1234\n",
            "\n",
            "n_sessions:\n",
            "  train: 1000 # This will create 1000 sessions/utterances per class\n",
            "  dev: 200\n",
            "  eval: 200\n",
            "n_speakers: 4 # max number of speakers in each session/utterance created i.e., 0 to n_speakers.\n",
            "\n",
            "max_length: 120 # max length in seconds for each session/utterance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We'll create only 1 or 2 sessions to make the process faster. Above shown are the original set parameters."
      ],
      "metadata": {
        "id": "WCjOtz_WENu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file prepare_dataset/dataset.yaml\n",
        "# This parameter file is used to create the dataset.\n",
        "out_folder: ../data # folder where created dataset will be stored.\n",
        "metadata_folder: !ref <out_folder>/metadata\n",
        "samplerate: 16000\n",
        "\n",
        "librispeech_root: !ref <out_folder>/LibriSpeech # e.g., /data/LibriSpeech/\n",
        "#(typically where you've downloaded the data)\n",
        "\n",
        "librispeech_folders:\n",
        "  train:\n",
        "    - !ref <out_folder>/LibriSpeech/train-clean-100/\n",
        "  dev:\n",
        "    - !ref <out_folder>/LibriSpeech/dev-clean/\n",
        "  eval:\n",
        "    - !ref <out_folder>/LibriSpeech/test-clean/\n",
        "\n",
        "rirs_noises_root: !ref <out_folder>/RIRS_NOISES\n",
        "rirs_folders:\n",
        "  - !ref <out_folder>/RIRS_NOISES/simulated_rirs/\n",
        "  - !ref <out_folder>/RIRS_NOISES/real_rirs_isotropic_noises\n",
        "noises_folders:\n",
        "  - !ref <out_folder>/RIRS_NOISES/pointsource_noises/\n",
        "\n",
        "\n",
        "# parameters\n",
        "seed: 1234\n",
        "\n",
        "n_sessions:\n",
        "  train: 2 # This will create 1000 sessions/utterances per class\n",
        "  dev: 1\n",
        "  eval: 1\n",
        "n_speakers: 4 # max number of speakers in each session/utterance created i.e., 0 to n_speakers.\n",
        "\n",
        "max_length: 120 # max length in seconds for each session/utterance.\n"
      ],
      "metadata": {
        "id": "1V0Sow9iBtE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0967ead-e1f1-41b1-b712-c5783f5eebca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prepare_dataset/dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat prepare_dataset/create_custom_dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnRi-yHxBDvx",
        "outputId": "48bee7f7-919a-4d5b-ed08-e46ccb263e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\"\"\n",
            "Script which creates custom dataset based on set parameters[dataset.yaml].\n",
            "\n",
            "Usage - python create_custom_dataset.py dataset.yaml\n",
            "\"\"\"\n",
            "\n",
            "import os\n",
            "import sys\n",
            "import json\n",
            "import random\n",
            "import math\n",
            "import numpy as np\n",
            "import speechbrain as sb\n",
            "from hyperpyyaml import load_hyperpyyaml\n",
            "from speechbrain.dataio.dataio import read_audio\n",
            "from speechbrain.utils.data_utils import get_all_files\n",
            "from local.create_mixtures_metadata import create_metadata\n",
            "from local.create_mixtures_from_metadata import create_mixture\n",
            "from pathlib import Path\n",
            "import torch\n",
            "import torchaudio\n",
            "from speechbrain.augment.preparation import write_csv\n",
            "from speechbrain.augment.time_domain import AddNoise, AddReverb\n",
            "from tqdm import tqdm\n",
            "\n",
            "# Load hyperparameters file with command-line overrides\n",
            "params_file, run_opts, overrides = sb.core.parse_arguments(sys.argv[1:])\n",
            "with open(params_file) as fin:\n",
            "    params = load_hyperpyyaml(fin, overrides)\n",
            "\n",
            "# setting seeds for reproducible code.\n",
            "np.random.seed(params[\"seed\"])\n",
            "random.seed(params[\"seed\"])\n",
            "\n",
            "\n",
            "# we parse the yaml, and create mixtures for every train, dev and eval split.\n",
            "def split_list(array, split_factors):\n",
            "    assert round(sum(split_factors), 6) == 1, \"split_factors should sum to one\"\n",
            "    np.random.shuffle(array)\n",
            "    pivots = [int(len(array) * x) for x in split_factors]\n",
            "    out = []\n",
            "    indx = 0\n",
            "    for i in pivots:\n",
            "        out.append(array[indx: i + indx])\n",
            "        indx = i\n",
            "    return out\n",
            "\n",
            "\n",
            "def parse_libri_folder(libri_folders):\n",
            "    # parsing librispeech\n",
            "    utterances = []\n",
            "    txt_files = []\n",
            "    for libri_dir in libri_folders:\n",
            "        utterances.extend(get_all_files(libri_dir, match_and=[\".flac\"]))\n",
            "        txt_files.extend(get_all_files(libri_dir, match_and=[\"trans.txt\"]))\n",
            "\n",
            "    words_dict = {}\n",
            "    for trans in txt_files:\n",
            "        with open(trans, \"r\") as f:\n",
            "            for line in f:\n",
            "                splitted = line.split(\" \")\n",
            "                utt_id = splitted[0]\n",
            "                words = \" \".join(splitted[1:])\n",
            "                words_dict[utt_id] = words.strip(\"\\n\")\n",
            "\n",
            "    speakers = {}\n",
            "    for u in utterances:\n",
            "        spk_id = Path(u).parent.parent.stem\n",
            "        speakers[spk_id] = speakers.get(spk_id, []) + [u]\n",
            "\n",
            "    return speakers, words_dict\n",
            "\n",
            "\n",
            "os.makedirs(os.path.join(params[\"out_folder\"], \"metadata\"), exist_ok=True)\n",
            "\n",
            "# we generate metadata for each split\n",
            "for indx, split in enumerate([\"train\", \"dev\", \"eval\"]):\n",
            "    print(\"Generating metadata for {} set\".format(split))\n",
            "    # we parse librispeech utterances for current split\n",
            "    c_libri_folder = params[\"librispeech_folders\"][split]\n",
            "    c_utterances, c_words = parse_libri_folder(c_libri_folder)\n",
            "\n",
            "    create_metadata(\n",
            "        os.path.join(params[\"out_folder\"], \"metadata\", split),\n",
            "        params[\"n_sessions\"][split],\n",
            "        params,\n",
            "        c_utterances,\n",
            "        c_words,\n",
            "    )\n",
            "\n",
            "# from metadata we generate the actual mixtures\n",
            "for indx, split in enumerate([\"train\", \"dev\", \"eval\"]):\n",
            "    print(f\"Creating {split} set\")\n",
            "    # load metadata\n",
            "    metadata_path = os.path.join(params[\"out_folder\"], \"metadata\", f\"{split}.json\")\n",
            "    with open(metadata_path) as f:\n",
            "        c_meta = json.load(f)\n",
            "\n",
            "    c_folder = os.path.join(params[\"out_folder\"], split)\n",
            "    os.makedirs(c_folder, exist_ok=True)\n",
            "\n",
            "    # Here we loop through sessions with progress tracking\n",
            "    for sess in tqdm(list(c_meta.keys()), desc=f\"Creating {split} set\"):\n",
            "        create_mixture(sess, c_folder, params, c_meta)\n",
            "\n",
            "print(\"Creating segments....\")\n",
            "\n",
            "\n",
            "def create_segments(x=\"train\"):\n",
            "    segment_length = int(params[\"max_length\"] / 60)  # in seconds\n",
            "    sample_rate = int(params[\"samplerate\"])\n",
            "    file_list = get_all_files((os.path.join(params[\"out_folder\"], f\"{x}\")), match_and=[\".wav\"])\n",
            "\n",
            "    for file in tqdm(file_list, desc=f\"Processing {x} segments\"):\n",
            "        wav_path = file.replace(\"\\\\\", \"/\")  # Windows can be removed if you're using linux\n",
            "\n",
            "        waveform, _ = torchaudio.load(wav_path)\n",
            "        num_samples_per_segment = sample_rate * segment_length\n",
            "        total_segments = math.ceil(waveform.size(1) / num_samples_per_segment)\n",
            "\n",
            "        for segment_id in range(total_segments):\n",
            "            start_sample = segment_id * num_samples_per_segment\n",
            "            end_sample = start_sample + num_samples_per_segment\n",
            "\n",
            "            if end_sample <= waveform.size(1):\n",
            "                segment_waveform = waveform[:, start_sample:end_sample]\n",
            "\n",
            "                segment_file_name = f\"{os.path.splitext(wav_path)[0]}_{segment_id:03d}_segment.wav\"\n",
            "                torchaudio.save(segment_file_name, segment_waveform, sample_rate)\n",
            "\n",
            "\n",
            "create_segments(\"train\")\n",
            "create_segments(\"dev\")\n",
            "create_segments(\"eval\")\n",
            "print(\"Adding Noise....\")\n",
            "\n",
            "\"\"\"\n",
            "Writes all the noises into a csv and creates noisifier and reverber objects\n",
            "\"\"\"\n",
            "sample_rate = int(params[\"samplerate\"])\n",
            "\n",
            "rir_audios = get_all_files(os.path.join(params[\"rirs_noises_root\"], \"simulated_rirs\"), match_and=['.wav'])\n",
            "rir_audios.extend(\n",
            "    get_all_files(os.path.join(params[\"rirs_noises_root\"], \"real_rirs_isotropic_noises\"), match_and=['.wav']))\n",
            "\n",
            "noise_audios = get_all_files(os.path.join(params[\"rirs_noises_root\"], \"pointsource_noises\"), match_and=['.wav'])\n",
            "\n",
            "write_csv(rir_audios, os.path.join(params[\"out_folder\"], \"simulated_rirs.csv\"))\n",
            "write_csv(noise_audios, os.path.join(params[\"out_folder\"], \"noises.csv\"))\n",
            "\n",
            "noisifier = AddNoise(os.path.join(params[\"out_folder\"], \"noises.csv\"), snr_low=5, snr_high=20)\n",
            "reverber = AddReverb(os.path.join(params[\"out_folder\"], \"simulated_rirs.csv\"), reverb_sample_rate=sample_rate,\n",
            "                     clean_sample_rate=sample_rate)\n",
            "\n",
            "batch_size = 10\n",
            "probability_noise = 0.5\n",
            "probability_reverb = 0.5\n",
            "\n",
            "\n",
            "def load_and_addnoise(x=\"train\"):\n",
            "    json_data = get_all_files((os.path.join(params[\"out_folder\"], f\"{x}\")), match_and=[\".wav\"])\n",
            "    for data in tqdm(json_data, desc=f'Loading {x}'):\n",
            "        audio_path = data.replace(\"\\\\\", \"/\")  # Windows can be removed if you're using linux\n",
            "        signal = read_audio(audio_path)\n",
            "        clean = signal.unsqueeze(0)\n",
            "\n",
            "        if random.random() < probability_noise:\n",
            "            noisy = noisifier(clean, torch.ones(clean.size(0)))\n",
            "        else:\n",
            "            noisy = clean\n",
            "\n",
            "        if noisy.dim() == 2:\n",
            "            noisy = noisy.unsqueeze(-1)\n",
            "\n",
            "        if random.random() < probability_reverb:\n",
            "            reverbed = reverber(noisy)\n",
            "        else:\n",
            "            reverbed = noisy\n",
            "\n",
            "        processed_audio = reverbed.squeeze(0).transpose(0, 1)\n",
            "        output_path = audio_path\n",
            "\n",
            "        torchaudio.save(output_path, processed_audio, params[\"samplerate\"])\n",
            "\n",
            "\n",
            "load_and_addnoise(\"train\")\n",
            "load_and_addnoise(\"dev\")\n",
            "load_and_addnoise(\"eval\")\n",
            "print(\"AddedNoise\")\n",
            "\n",
            "print(\"Created Custom Dataset\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Updating the file paths to make it work for linux based systems\n",
        "Changed .replace(\"\\ \\\\\") as it is not needed in linux."
      ],
      "metadata": {
        "id": "rds058-8FgFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file prepare_dataset/create_custom_dataset.py\n",
        "\n",
        "\"\"\"\n",
        "Script which creates custom dataset based on set parameters[dataset.yaml].\n",
        "\n",
        "Usage - python create_custom_dataset.py dataset.yaml\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "from speechbrain.utils.data_utils import get_all_files\n",
        "from local.create_mixtures_metadata import create_metadata\n",
        "from local.create_mixtures_from_metadata import create_mixture\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "from speechbrain.augment.preparation import write_csv\n",
        "from speechbrain.augment.time_domain import AddNoise, AddReverb\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load hyperparameters file with command-line overrides\n",
        "params_file, run_opts, overrides = sb.core.parse_arguments(sys.argv[1:])\n",
        "with open(params_file) as fin:\n",
        "    params = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "# setting seeds for reproducible code.\n",
        "np.random.seed(params[\"seed\"])\n",
        "random.seed(params[\"seed\"])\n",
        "\n",
        "\n",
        "# we parse the yaml, and create mixtures for every train, dev and eval split.\n",
        "def split_list(array, split_factors):\n",
        "    assert round(sum(split_factors), 6) == 1, \"split_factors should sum to one\"\n",
        "    np.random.shuffle(array)\n",
        "    pivots = [int(len(array) * x) for x in split_factors]\n",
        "    out = []\n",
        "    indx = 0\n",
        "    for i in pivots:\n",
        "        out.append(array[indx: i + indx])\n",
        "        indx = i\n",
        "    return out\n",
        "\n",
        "\n",
        "def parse_libri_folder(libri_folders):\n",
        "    # parsing librispeech\n",
        "    utterances = []\n",
        "    txt_files = []\n",
        "    for libri_dir in libri_folders:\n",
        "        utterances.extend(get_all_files(libri_dir, match_and=[\".flac\"]))\n",
        "        txt_files.extend(get_all_files(libri_dir, match_and=[\"trans.txt\"]))\n",
        "\n",
        "    words_dict = {}\n",
        "    for trans in txt_files:\n",
        "        with open(trans, \"r\") as f:\n",
        "            for line in f:\n",
        "                splitted = line.split(\" \")\n",
        "                utt_id = splitted[0]\n",
        "                words = \" \".join(splitted[1:])\n",
        "                words_dict[utt_id] = words.strip(\"\\n\")\n",
        "\n",
        "    speakers = {}\n",
        "    for u in utterances:\n",
        "        spk_id = Path(u).parent.parent.stem\n",
        "        speakers[spk_id] = speakers.get(spk_id, []) + [u]\n",
        "\n",
        "    return speakers, words_dict\n",
        "\n",
        "\n",
        "os.makedirs(os.path.join(params[\"out_folder\"], \"metadata\"), exist_ok=True)\n",
        "\n",
        "# we generate metadata for each split\n",
        "for indx, split in enumerate([\"train\", \"dev\", \"eval\"]):\n",
        "    print(\"Generating metadata for {} set\".format(split))\n",
        "    # we parse librispeech utterances for current split\n",
        "    c_libri_folder = params[\"librispeech_folders\"][split]\n",
        "    c_utterances, c_words = parse_libri_folder(c_libri_folder)\n",
        "\n",
        "    create_metadata(\n",
        "        os.path.join(params[\"out_folder\"], \"metadata\", split),\n",
        "        params[\"n_sessions\"][split],\n",
        "        params,\n",
        "        c_utterances,\n",
        "        c_words,\n",
        "    )\n",
        "\n",
        "# from metadata we generate the actual mixtures\n",
        "for indx, split in enumerate([\"train\", \"dev\", \"eval\"]):\n",
        "    print(f\"Creating {split} set\")\n",
        "    # load metadata\n",
        "    metadata_path = os.path.join(params[\"out_folder\"], \"metadata\", f\"{split}.json\")\n",
        "    with open(metadata_path) as f:\n",
        "        c_meta = json.load(f)\n",
        "\n",
        "    c_folder = os.path.join(params[\"out_folder\"], split)\n",
        "    os.makedirs(c_folder, exist_ok=True)\n",
        "\n",
        "    # Here we loop through sessions with progress tracking\n",
        "    for sess in tqdm(list(c_meta.keys()), desc=f\"Creating {split} set\"):\n",
        "        create_mixture(sess, c_folder, params, c_meta)\n",
        "\n",
        "print(\"Creating segments....\")\n",
        "\n",
        "\n",
        "def create_segments(x=\"train\"):\n",
        "    segment_length = int(params[\"max_length\"] / 60)  # in seconds\n",
        "    sample_rate = int(params[\"samplerate\"])\n",
        "    file_list = get_all_files((os.path.join(params[\"out_folder\"], f\"{x}\")), match_and=[\".wav\"])\n",
        "\n",
        "    for file in tqdm(file_list, desc=f\"Processing {x} segments\"):\n",
        "        # wav_path = file.replace(\"\\\\\", \"/\")  # Windows can be removed if you're using linux\n",
        "        wav_path = file\n",
        "        waveform, _ = torchaudio.load(wav_path)\n",
        "        num_samples_per_segment = sample_rate * segment_length\n",
        "        total_segments = math.ceil(waveform.size(1) / num_samples_per_segment)\n",
        "\n",
        "        for segment_id in range(total_segments):\n",
        "            start_sample = segment_id * num_samples_per_segment\n",
        "            end_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "            if end_sample <= waveform.size(1):\n",
        "                segment_waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "                segment_file_name = f\"{os.path.splitext(wav_path)[0]}_{segment_id:03d}_segment.wav\"\n",
        "                torchaudio.save(segment_file_name, segment_waveform, sample_rate)\n",
        "\n",
        "\n",
        "create_segments(\"train\")\n",
        "create_segments(\"dev\")\n",
        "create_segments(\"eval\")\n",
        "print(\"Adding Noise....\")\n",
        "\n",
        "\"\"\"\n",
        "Writes all the noises into a csv and creates noisifier and reverber objects\n",
        "\"\"\"\n",
        "sample_rate = int(params[\"samplerate\"])\n",
        "\n",
        "rir_audios = get_all_files(os.path.join(params[\"rirs_noises_root\"], \"simulated_rirs\"), match_and=['.wav'])\n",
        "rir_audios.extend(\n",
        "    get_all_files(os.path.join(params[\"rirs_noises_root\"], \"real_rirs_isotropic_noises\"), match_and=['.wav']))\n",
        "\n",
        "noise_audios = get_all_files(os.path.join(params[\"rirs_noises_root\"], \"pointsource_noises\"), match_and=['.wav'])\n",
        "\n",
        "rir_audios = rir_audios[:5]\n",
        "noise_audios = noise_audios[:5]\n",
        "print(\"Creating simulated rirs csv\")\n",
        "write_csv(rir_audios, os.path.join(params[\"out_folder\"], \"simulated_rirs.csv\"))\n",
        "print(\"Creating noises csv\")\n",
        "write_csv(noise_audios, os.path.join(params[\"out_folder\"], \"noises.csv\"))\n",
        "\n",
        "print(\"Creating noisifier and reverber\")\n",
        "noisifier = AddNoise(os.path.join(params[\"out_folder\"], \"noises.csv\"), snr_low=5, snr_high=20)\n",
        "reverber = AddReverb(os.path.join(params[\"out_folder\"], \"simulated_rirs.csv\"), reverb_sample_rate=sample_rate,\n",
        "                     clean_sample_rate=sample_rate)\n",
        "\n",
        "batch_size = 10\n",
        "probability_noise = 0.5\n",
        "probability_reverb = 0.5\n",
        "\n",
        "\n",
        "def load_and_addnoise(x=\"train\"):\n",
        "    json_data = get_all_files((os.path.join(params[\"out_folder\"], f\"{x}\")), match_and=[\".wav\"])\n",
        "    for data in tqdm(json_data, desc=f'Loading {x}'):\n",
        "        audio_path = data #.replace(\"\\\\\", \"/\")  # Windows can be removed if you're using linux\n",
        "        signal = read_audio(audio_path)\n",
        "        clean = signal.unsqueeze(0)\n",
        "\n",
        "        if random.random() < probability_noise:\n",
        "            noisy = noisifier(clean, torch.ones(clean.size(0)))\n",
        "        else:\n",
        "            noisy = clean\n",
        "\n",
        "        if noisy.dim() == 2:\n",
        "            noisy = noisy.unsqueeze(-1)\n",
        "\n",
        "        if random.random() < probability_reverb:\n",
        "            reverbed = reverber(noisy)\n",
        "        else:\n",
        "            reverbed = noisy\n",
        "\n",
        "        processed_audio = reverbed.squeeze(0).transpose(0, 1)\n",
        "        output_path = audio_path\n",
        "\n",
        "        torchaudio.save(output_path, processed_audio, params[\"samplerate\"])\n",
        "\n",
        "\n",
        "load_and_addnoise(\"train\")\n",
        "load_and_addnoise(\"dev\")\n",
        "load_and_addnoise(\"eval\")\n",
        "print(\"AddedNoise\")\n",
        "\n",
        "print(\"Created Custom Dataset\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_mvhpSDBL8x",
        "outputId": "5f119220-7aa4-4996-aaca-20d5a8dfef2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prepare_dataset/create_custom_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd prepare_dataset\n",
        "!python create_custom_dataset.py dataset.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfWEG1PsEwKV",
        "outputId": "73720236-bd97-487d-eb73-38e4cb0c876a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ConvAIProject/prepare_dataset\n",
            "Generating metadata for train set\n",
            "Generating sessions: 100% 2/2 [00:01<00:00,  1.34it/s]\n",
            "Generating metadata for dev set\n",
            "Generating sessions: 100% 1/1 [00:00<00:00,  1.44it/s]\n",
            "Generating metadata for eval set\n",
            "Generating sessions: 100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "Creating train set\n",
            "Creating train set: 100% 10/10 [00:01<00:00,  5.53it/s]\n",
            "Creating dev set\n",
            "Creating dev set: 100% 5/5 [00:01<00:00,  3.93it/s]\n",
            "Creating eval set\n",
            "Creating eval set: 100% 5/5 [00:01<00:00,  4.51it/s]\n",
            "Creating segments....\n",
            "Processing train segments: 100% 10/10 [00:01<00:00,  8.36it/s]\n",
            "Processing dev segments: 100% 5/5 [00:00<00:00,  8.66it/s]\n",
            "Processing eval segments: 100% 5/5 [00:00<00:00,  8.20it/s]\n",
            "Adding Noise....\n",
            "Creating simulated rirs csv\n",
            "Creating noises csv\n",
            "Creating noisifier and reverber\n",
            "Loading train: 100% 638/638 [00:14<00:00, 42.99it/s]\n",
            "Loading dev: 100% 323/323 [00:10<00:00, 29.92it/s]\n",
            "Loading eval: 100% 316/316 [00:09<00:00, 33.66it/s]\n",
            "AddedNoise\n",
            "Created Custom Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train XVector"
      ],
      "metadata": {
        "id": "Mkz_0wnLDjqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ConvAIProject/xvector/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gyl-l86C5Ek",
        "outputId": "7c4001fa-6ee7-4410-f7a4-5756bdf82031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ConvAIProject/xvector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speechbrain as sb\n",
        "import os, sys\n",
        "from speechbrain.utils.data_utils import get_all_files\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "import json\n",
        "import torchaudio\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def process_file(path):\n",
        "    \"\"\"\n",
        "    Processes a single audio file to extract its identifier, path, number of speakers, and length.\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): The file path of the audio file.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing the audio file identifier and a dictionary with metadata about the audio file.\n",
        "             The metadata includes the normalized path, number of speakers, and length in seconds.\n",
        "    \"\"\"\n",
        "    parts = path.split(\"/\")[-1].split(\"_\") #.split(\"\\\\\") can be ignored when using a linux based system\n",
        "    # parts = path.split(\"/\")[-1].split(\"\\\\\")[-1].split(\"_\") #.split(\"\\\\\") can be ignored when using a linux based system\n",
        "    id = \"_\".join(parts[:-1])\n",
        "    num_speakers = parts[3]\n",
        "    info = torchaudio.info(path)\n",
        "    length = info.num_frames / 16000\n",
        "\n",
        "    return id, {\n",
        "        \"wav_path\": path, #.replace(\"\\\\\",\"/\"),  #.split(\"\\\\\") can be ignored when using a linux based system\n",
        "        \"num_speakers\": num_speakers,\n",
        "        \"length\": length\n",
        "    }\n",
        "\n",
        "def load_json(json_paths, save_file=\"train\"):\n",
        "    \"\"\"\n",
        "    Loads multiple audio files, processes each using `process_file`, and saves the metadata in a JSON file.\n",
        "\n",
        "    Parameters:\n",
        "    - json_paths (list): A list of paths to audio files to process.\n",
        "    - save_file (str): The base name for the output JSON file where the metadata will be saved.\n",
        "\n",
        "    Returns:\n",
        "    None. This function generates a JSON file in the '../data/' directory containing the metadata for each audio file.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    # Parallel processing\n",
        "    results = Parallel(n_jobs=-1, verbose=10)(\n",
        "        delayed(process_file)(path) for path in json_paths\n",
        "    )\n",
        "\n",
        "    for id, path_data in results:\n",
        "        data[id] = path_data\n",
        "\n",
        "    with open(f\"../data/{save_file}_data.json\", 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "train_files = get_all_files(\"../data/train\", match_and=['_segment.wav'])\n",
        "test_files = get_all_files(\"../data/dev\", match_and=['_segment.wav'])\n",
        "valid_files = get_all_files(\"../data/eval\", match_and=['_segment.wav'])\n",
        "\n",
        "load_json(train_files, save_file=\"train\")\n",
        "load_json(test_files, save_file=\"test\")\n",
        "load_json(valid_files, save_file=\"valid\")\n",
        "\n",
        "test_files_no_spk = [file for file in test_files if 'spk_0' in file]\n",
        "test_files_1_spk = [file for file in test_files if 'spk_1' in file]\n",
        "test_files_2_spk = [file for file in test_files if 'spk_2' in file]\n",
        "test_files_3_spk = [file for file in test_files if 'spk_3' in file]\n",
        "test_files_4_spk = [file for file in test_files if 'spk_4' in file]\n",
        "\n",
        "load_json(test_files_no_spk, save_file=\"test_files_no_spk\")\n",
        "load_json(test_files_1_spk, save_file=\"test_files_1_spk\")\n",
        "load_json(test_files_2_spk, save_file=\"test_files_2_spk\")\n",
        "load_json(test_files_3_spk, save_file=\"test_files_3_spk\")\n",
        "load_json(test_files_4_spk, save_file=\"test_files_4_spk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyiUEcQLEdXs",
        "outputId": "13b78c68-b42d-4af4-b6f1-13390cb91c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.6s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    2.7s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.7s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    2.7s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19846884186620734s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    2.8s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.024588584899902344s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05449652671813965s.) Setting batch_size=8.\n",
            "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.10674476623535156s.) Setting batch_size=16.\n",
            "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:    3.4s\n",
            "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:    4.0s\n",
            "[Parallel(n_jobs=-1)]: Done 476 tasks      | elapsed:    4.8s\n",
            "[Parallel(n_jobs=-1)]: Done 607 tasks      | elapsed:    5.8s\n",
            "[Parallel(n_jobs=-1)]: Done 628 out of 628 | elapsed:    5.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.010215520858764648s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05386543273925781s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.10198688507080078s.) Setting batch_size=8.\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.15620207786560059s.) Setting batch_size=16.\n",
            "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done 318 out of 318 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.010949850082397461s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.028878450393676758s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.048194169998168945s.) Setting batch_size=8.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.10864758491516113s.) Setting batch_size=16.\n",
            "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done 291 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done 311 out of 311 | elapsed:    1.4s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_xvector_augmentation.py hparams_xvector_augmentation.yaml"
      ],
      "metadata": {
        "id": "ekz_XMgeEhg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6609d302-2995-41ac-d819-1f97e04177de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ../results/XVector/Augmented_2/1986\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - 926.3k trainable parameters in XVectorSpkCounter\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 10/10 [00:10<00:00,  1.08s/it, train_loss=0.99]\n",
            "100% 5/5 [00:01<00:00,  2.79it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.001 to 0.001\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-03 - train loss: 9.90e-01 - valid loss: 1.56, valid error: 8.07e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in ../results/XVector/Augmented_2/1986/save/CKPT+2024-04-25+17-14-04+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from ../results/XVector/Augmented_2/1986/save/CKPT+2024-04-25+17-14-04+00\n",
            "100% 5/5 [00:02<00:00,  2.18it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 1 - test loss: 1.60, test error: 7.89e-01\n",
            "{\"loss\": 1.5597284078598022, \"error\": 0.8070739507675171, \"objective\": 0.8070739507675171}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train ECAPA-TDNN"
      ],
      "metadata": {
        "id": "kR7tZYj5Isr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ConvAIProject/ecapa_tdnn/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfEbIV9eEZAp",
        "outputId": "d75de7e1-f437-4425-e5d3-8822da796328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ConvAIProject/ecapa_tdnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_ecapa_tdnn.py hparams_ecapa_tdnn_augmentation.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8BkDia-Iy6z",
        "outputId": "4cf3b76e-58aa-46d0-8f8d-61a98a1e1637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ../results/ECAPA/Augmented/1986\n",
            "<speechbrain.dataio.encoder.CategoricalEncoder object at 0x7eb06a2d51e0>\n",
            "<speechbrain.dataio.dataset.DynamicItemDataset object at 0x7eb06a499e10>\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - 2.0M trainable parameters in ECAPABrain\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 10/10 [00:44<00:00,  4.48s/it, train_loss=9.01]\n",
            "100% 5/5 [00:01<00:00,  2.77it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 3.5e-06 to 3.7e-06\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 3.25e-06 - train loss: 9.01 - valid loss: 8.29, valid ErrorRate: 7.91e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in ../results/ECAPA/Augmented/1986/save/CKPT+2024-04-25+17-14-58+00\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "100% 5/5 [00:01<00:00,  2.64it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 1 - test loss: 8.10, test ErrorRate: 7.61e-01\n",
            "{\"loss\": 8.290588760375977, \"ErrorRate\": 0.790996789932251, \"objective\": 0.790996789932251}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Selfsupervised MLP & XVector"
      ],
      "metadata": {
        "id": "rG8LaKuyJMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ConvAIProject/selfsupervised/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfnqZ4lhJIoV",
        "outputId": "78cee8e7-1285-4c42-b2cb-858bf7f7d355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ConvAIProject/selfsupervised\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python selfsupervised_mlp.py hparams_selfsupervised_mlp.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Lt6IzgJR0i",
        "outputId": "da238d1a-318d-4ade-c938-333022fa5b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ../results/train_with_wav2vec2/1993\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - 90.2M trainable parameters in SelfSupervisedSpeakerCounter\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from ../results/train_with_wav2vec2/1993/save/CKPT+2024-04-25+17-15-29+00\n",
            "Evaluating on all classes\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from ../results/train_with_wav2vec2/1993/save/CKPT+2024-04-25+17-15-29+00\n",
            "100% 5/5 [00:05<00:00,  1.01s/it]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 1 - test loss: 9.85e-01, test error_rate: 3.49e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python selfsupervised_xvector.py hparams_selfsupervised_xvector.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krkhxCeeMVHz",
        "outputId": "b2e15896-66fe-43b0-93e8-dc3d07a90d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rconfig.json:   0% 0.00/1.84k [00:00<?, ?B/s]\rconfig.json: 100% 1.84k/1.84k [00:00<00:00, 9.27MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "pytorch_model.bin: 100% 380M/380M [00:01<00:00, 254MB/s]\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "preprocessor_config.json: 100% 159/159 [00:00<00:00, 951kB/s]\n",
            "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ../results/selfsupervised/Xvector/1986\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - 90.5M trainable parameters in SelfSupervisedSpeakerCounter\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 20/20 [00:16<00:00,  1.21it/s, train_loss=0.951]\n",
            "100% 10/10 [00:03<00:00,  2.52it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-03, ssl_lr: 1.00e-05 - train loss: 9.51e-01 - valid loss: 1.55, valid error_rate: 8.07e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in ../results/selfsupervised/Xvector/1986/save/CKPT+2024-04-25+17-16-02+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from ../results/selfsupervised/Xvector/1986/save/CKPT+2024-04-25+17-16-02+00\n",
            "100% 10/10 [00:04<00:00,  2.26it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 1 - test loss: 1.57, test error_rate: 8.11e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dv_2PccZMkcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}