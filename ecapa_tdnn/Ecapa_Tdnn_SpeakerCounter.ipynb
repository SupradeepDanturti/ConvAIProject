{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-15T15:52:39.074647Z",
     "start_time": "2024-04-15T15:51:23.795055Z"
    }
   },
   "source": [
    "%%capture\n",
    "\n",
    "import json\n",
    "import speechbrain as sb\n",
    "import os, sys\n",
    "from speechbrain.utils.data_utils import get_all_files\n",
    "import torch\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "import random\n",
    "import torchaudio\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_file(path):\n",
    "    # Optimized path operations\n",
    "    parts = path.split(\"/\")[-1].split(\"\\\\\")[-1].split(\"_\")\n",
    "    id = \"_\".join(parts[:-1])\n",
    "    num_speakers = parts[3]\n",
    "    info = torchaudio.info(path)\n",
    "    length = info.num_frames / 16000\n",
    "\n",
    "    return id, {\n",
    "        \"wav_path\": path.replace(\"\\\\\",\"/\"),\n",
    "        \"num_speakers\": num_speakers,\n",
    "        \"length\": length\n",
    "    }\n",
    "\n",
    "def load_json(json_paths, save_file=\"train\"):\n",
    "    data = {}\n",
    "\n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_file)(path) for path in json_paths\n",
    "    )\n",
    "\n",
    "    for id, path_data in results:\n",
    "        data[id] = path_data\n",
    "\n",
    "    with open(f\"../data/{save_file}_data.json\", 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_files = get_all_files(\"../data/train\", match_and=['_segment.wav'])\n",
    "test_files = get_all_files(\"../data/dev\", match_and=['_segment.wav'])\n",
    "valid_files = get_all_files(\"../data/eval\", match_and=['_segment.wav'])\n",
    "\n",
    "load_json(train_files, save_file=\"train\")\n",
    "load_json(test_files, save_file=\"test\")\n",
    "load_json(valid_files, save_file=\"valid\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "74adf2a3b963acd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:52:43.734897Z",
     "start_time": "2024-04-15T15:52:39.076642Z"
    }
   },
   "source": [
    "test_files_no_spk = [file for file in test_files if 'spk_0' in file]\n",
    "test_files_1_spk = [file for file in test_files if 'spk_1' in file]\n",
    "test_files_2_spk = [file for file in test_files if 'spk_2' in file]\n",
    "test_files_3_spk = [file for file in test_files if 'spk_3' in file]\n",
    "test_files_4_spk = [file for file in test_files if 'spk_4' in file]\n",
    "\n",
    "load_json(test_files_no_spk, save_file=\"test_files_no_spk\")\n",
    "load_json(test_files_1_spk, save_file=\"test_files_1_spk\")\n",
    "load_json(test_files_2_spk, save_file=\"test_files_2_spk\")\n",
    "load_json(test_files_3_spk, save_file=\"test_files_3_spk\")\n",
    "load_json(test_files_4_spk, save_file=\"test_files_4_spk\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.009006738662719727s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.037862539291381836s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.027352571487426758s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.02952289581298828s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03551626205444336s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04488086700439453s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.08252596855163574s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=-1)]: Done 3032 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4632 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.17066192626953125s.) Setting batch_size=256.\n",
      "[Parallel(n_jobs=-1)]: Done 7384 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9888 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10488 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10749 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11010 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11289 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11568 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 12000 out of 12000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.008041143417358398s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04323697090148926s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.030992507934570312s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.030518293380737305s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03851318359375s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04700517654418945s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.08136534690856934s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=-1)]: Done 3032 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4632 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.16750621795654297s.) Setting batch_size=256.\n",
      "[Parallel(n_jobs=-1)]: Done 7384 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 10021 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10552 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10871 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11190 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11531 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 11872 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12450 out of 12450 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.007992744445800781s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.042020559310913086s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03004169464111328s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03300666809082031s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.039000511169433594s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04751920700073242s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.08626461029052734s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=-1)]: Done 3032 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4632 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1755218505859375s.) Setting batch_size=256.\n",
      "[Parallel(n_jobs=-1)]: Done 7384 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9904 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10552 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10871 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11190 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11531 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11872 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12559 out of 12559 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.008519649505615234s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04649710655212402s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03053879737854004s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03404355049133301s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03706693649291992s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05005693435668945s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.09986567497253418s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=-1)]: Done 3032 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4632 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1820056438446045s.) Setting batch_size=256.\n",
      "[Parallel(n_jobs=-1)]: Done 7384 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9796 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 10584 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 10932 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11280 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11652 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12024 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12795 out of 12795 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.009001731872558594s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03846144676208496s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03266572952270508s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03161454200744629s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03400111198425293s.) Setting batch_size=32.\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04451942443847656s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.08118343353271484s.) Setting batch_size=128.\n",
      "[Parallel(n_jobs=-1)]: Done 3032 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4632 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.15644407272338867s.) Setting batch_size=256.\n",
      "[Parallel(n_jobs=-1)]: Done 7384 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9460 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10616 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 10993 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11370 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 11773 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12814 out of 12853 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 12853 out of 12853 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f9751bdc83a851c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-15T15:52:43.750064Z",
     "start_time": "2024-04-15T15:52:43.736046Z"
    }
   },
   "source": [
    "%%file hparams_ecapa_tdnn_augmentation.yaml\n",
    "# Basic configuration for reproducibility\n",
    "seed: 1986\n",
    "__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "# Output directories for saving the results, model checkpoints, and logs\n",
    "output_folder: !ref ../results/ECAPA/Augmented/<seed>\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "# Paths to dataset annotations and noise profiles for augmentation\n",
    "data_folder: ../data\n",
    "train_annotation: !ref <data_folder>/train_data.json\n",
    "valid_annotation: !ref <data_folder>/valid_data.json\n",
    "test_annotation: !ref <data_folder>/test_data.json\n",
    "test_0_spk_annotation: !ref <data_folder>/test_files_no_spk_data.json\n",
    "test_1_spk_annotation: !ref <data_folder>/test_files_1_spk_data.json\n",
    "test_2_spk_annotation: !ref <data_folder>/test_files_2_spk_data.json\n",
    "test_3_spk_annotation: !ref <data_folder>/test_files_3_spk_data.json\n",
    "test_4_spk_annotation: !ref <data_folder>/test_files_4_spk_data.json\n",
    "\n",
    "noise_annotation: !ref <data_folder>/noises.csv\n",
    "rir_annotation: !ref <data_folder>/simulated_rirs.csv\n",
    "\n",
    "# Training hyperparameters\n",
    "sample_rate: 16000\n",
    "number_of_epochs: 20\n",
    "batch_size: 64\n",
    "lr_start: 0.001\n",
    "lr_final: 0.0001\n",
    "weight_decay: 0.00002\n",
    "num_workers: 0 # Number of workers set to 0 for compatibility with Windows and 4 with Linux\n",
    "n_classes: 5\n",
    "dim: 192\n",
    "num_attention_channels: 128\n",
    "shuffle: True\n",
    "\n",
    "lr: 0.0001\n",
    "mode: exp_range\n",
    "gamma: 0.9998\n",
    "base_lr: 0.000001\n",
    "max_lr: !ref <lr>\n",
    "step_size: 396 # 4 times number of iterations/epoch (2 to 10 is suggested)\n",
    "\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>\n",
    "    num_workers: !ref <num_workers>\n",
    "    shuffle: !ref <shuffle>\n",
    "\n",
    "# Checkpointing configuration to enable and set intervals\n",
    "ckpt_enable: True\n",
    "ckpt_interval_minutes: 15\n",
    "\n",
    "# Feature extraction parameters\n",
    "n_mels: 40\n",
    "\n",
    "# Data Augmentation settings, including noise, speed, reverb, frequency, and temporal dropping\n",
    "skip_prep: True\n",
    "snr_low: 0  # Min SNR for noise augmentation\n",
    "snr_high: 15  # Max SNR for noise augmentation\n",
    "\n",
    "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
    "    csv_file: !ref <noise_annotation>\n",
    "    snr_low: !ref <snr_low>\n",
    "    snr_high: !ref <snr_high>\n",
    "    noise_sample_rate: !ref <sample_rate>\n",
    "    clean_sample_rate: !ref <sample_rate>\n",
    "    num_workers: !ref <num_workers>\n",
    "\n",
    "# Speed perturbation\n",
    "speed_changes: [95, 100, 105]  # List of speed changes for time-stretching\n",
    "\n",
    "speed_perturb: !new:speechbrain.augment.time_domain.SpeedPerturb\n",
    "    orig_freq: !ref <sample_rate>\n",
    "    speeds: !ref <speed_changes>\n",
    "\n",
    "# add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
    "#     csv_file: !ref <rir_annotation>\n",
    "#     reverb_sample_rate: !ref <sample_rate>\n",
    "#     clean_sample_rate: !ref <sample_rate>\n",
    "\n",
    "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
    "drop_freq_low: 0  # Min frequency band dropout probability\n",
    "drop_freq_high: 1  # Max frequency band dropout probability\n",
    "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
    "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
    "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
    "\n",
    "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
    "    drop_freq_low: !ref <drop_freq_low>\n",
    "    drop_freq_high: !ref <drop_freq_high>\n",
    "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
    "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
    "    drop_freq_width: !ref <drop_freq_width>\n",
    "\n",
    "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
    "    parallel_augment: False\n",
    "    concat_original: True\n",
    "    repeat_augment: 1\n",
    "    shuffle_augmentations: False\n",
    "    min_augmentations: 4\n",
    "    max_augmentations: 4\n",
    "    augment_prob: 1.0\n",
    "    augmentations: [\n",
    "        !ref <add_noise>,\n",
    "        !ref <speed_perturb>,\n",
    "        !ref <drop_freq>]\n",
    "\n",
    "# Feature extraction and normalization configuration\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    n_mels: !ref <n_mels>\n",
    "        \n",
    "# Model components configuration for ECAPA-TDNN\n",
    "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: sentence\n",
    "    std_norm: False\n",
    "        \n",
    "##################################\n",
    "###### Model Configuration #######\n",
    "########## ECAPA-TDNN ############\n",
    "##################################\n",
    "        \n",
    "embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN\n",
    "    input_size: !ref <n_mels>\n",
    "    channels: [256, 256, 256, 256, 768]\n",
    "    kernel_sizes: [5, 3, 3, 3, 1]\n",
    "    dilations: [1, 2, 3, 4, 1]\n",
    "    attention_channels: !ref <num_attention_channels>\n",
    "    lin_neurons: !ref <dim>\n",
    "        \n",
    "classifier: !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier\n",
    "    input_size: !ref <dim>\n",
    "    out_neurons: !ref <n_classes>\n",
    "        \n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs> \n",
    "\n",
    "modules:\n",
    "    compute_features: !ref <compute_features>\n",
    "    embedding_model: !ref <embedding_model>\n",
    "    classifier: !ref <classifier>\n",
    "    mean_var_norm: !ref <mean_var_norm>\n",
    "\n",
    "# Loss configuration with margin and scale parameters\n",
    "compute_cost: !new:speechbrain.nnet.losses.LogSoftmaxWrapper\n",
    "    loss_fn: !new:speechbrain.nnet.losses.AdditiveAngularMargin\n",
    "        margin: 0.2\n",
    "        scale: 30\n",
    "\n",
    "# Optimizer and learning rate scheduler settings\n",
    "opt_class: !name:torch.optim.Adam\n",
    "    lr: !ref <lr_start>\n",
    "    weight_decay: !ref <weight_decay>\n",
    "\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
    "    base_lr: !ref <base_lr>\n",
    "    max_lr: !ref <max_lr>\n",
    "    step_size: !ref <step_size> # 4 times number of iterations/epoch (2 to 10 is suggested)\n",
    "\n",
    "# Logging and metric evaluation settings\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
    "    metric: !name:speechbrain.nnet.losses.classification_error\n",
    "        reduction: batch\n",
    "\n",
    "\n",
    "# Checkpoint management for model saving and recovery\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        embedding_model: !ref <embedding_model>\n",
    "        classifier: !ref <classifier>\n",
    "        normalizer: !ref <mean_var_norm>\n",
    "        counter: !ref <epoch_counter>\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hparams_ecapa_tdnn_augmentation.yaml\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "197ef68fd11445be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-15T15:52:43.765431Z",
     "start_time": "2024-04-15T15:52:43.751062Z"
    }
   },
   "source": [
    "%%file train_ecapa_tdnn.py\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "class ECAPABrain(sb.Brain):\n",
    "    \"\"\"Class that manages the training loop. See speechbrain.core.Brain.\"\"\"\n",
    "    \n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Computation pipeline based on a encoder + speaker classifier.\n",
    "        Data augmentation and environmental corruption are applied to the\n",
    "        input speech.\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, lens = batch.sig\n",
    "\n",
    "        # Add waveform augmentation if specified.\n",
    "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
    "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
    "\n",
    "        # Feature extraction and normalization\n",
    "        feats = self.modules.compute_features(wavs)\n",
    "        feats = self.modules.mean_var_norm(feats, lens)\n",
    "\n",
    "        # Embeddings + classifier\n",
    "        embeddings = self.modules.embedding_model(feats)\n",
    "        outputs = self.modules.classifier(embeddings)\n",
    "\n",
    "        return outputs, lens\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss using speaker-id as label.\"\"\"\n",
    "        predictions, lens = predictions\n",
    "        spkenc, _ = batch.num_speakers_encoded\n",
    "\n",
    "        # Concatenate labels (due to data augmentation)\n",
    "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
    "            spkenc = self.hparams.wav_augment.replicate_labels(spkenc)\n",
    "\n",
    "        loss = self.hparams.compute_cost(predictions, spkenc, lens)\n",
    "\n",
    "        if stage == sb.Stage.TRAIN and hasattr(\n",
    "            self.hparams.lr_annealing, \"on_batch_end\"\n",
    "        ):\n",
    "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, predictions, spkenc, lens)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics = self.hparams.error_stats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
    "        # Compute/store important stats\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
    "\n",
    "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
    "        if stage == sb.Stage.VALID:\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
    "                train_stats=self.train_stats,\n",
    "                valid_stats=stage_stats,\n",
    "            )\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
    "                min_keys=[\"ErrorRate\"],\n",
    "            )\n",
    "\n",
    "def dataio_prep(hparams):\n",
    "    \"\"\"Prepares the data IO (loading datasets, defining processing pipelines)\"\"\"\n",
    "\n",
    "    # Initialize the label encoder\n",
    "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
    "    print(label_encoder)\n",
    "\n",
    "    # Define audio pipeline\n",
    "    @sb.utils.data_pipeline.takes(\"wav_path\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav_path):\n",
    "        sig, fs = torchaudio.load(wav_path)\n",
    "\n",
    "        sig = torchaudio.functional.resample(sig, fs, 16000).squeeze(0)\n",
    "        return sig\n",
    "\n",
    "    # Define label pipeline\n",
    "    @sb.utils.data_pipeline.takes(\"num_speakers\")\n",
    "    @sb.utils.data_pipeline.provides(\"num_speakers_encoded\")\n",
    "    def label_pipeline(num_speakers):\n",
    "        num_speakers_encoded = label_encoder.encode_label_torch(num_speakers)\n",
    "        yield num_speakers_encoded\n",
    "\n",
    "    # Create datasets\n",
    "    datasets = {}\n",
    "    for dataset_name in [\"train\", \"valid\", \"test\", \"test_0_spk\", \"test_1_spk\", \"test_2_spk\", \"test_3_spk\", \"test_4_spk\"]:\n",
    "        datasets[dataset_name] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=hparams[f\"{dataset_name}_annotation\"],\n",
    "            dynamic_items=[audio_pipeline, label_pipeline],\n",
    "            output_keys=[\"id\", \"sig\", \"num_speakers_encoded\"],\n",
    "        )\n",
    "    print(datasets[\"train\"])\n",
    "    # Load or compute label encoder\n",
    "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
    "    label_encoder.load_or_create(\n",
    "        path=lab_enc_file,\n",
    "        from_didatasets=[datasets[\"train\"]],\n",
    "        output_key=\"num_speakers\",\n",
    "    )\n",
    "\n",
    "    return datasets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Loading the hyperparameters file and command line arguments\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "\n",
    "    # Load hyperparameter configuration file\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    # Create experiment directory\n",
    "    sb.core.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "    # Prepare data IO\n",
    "    datasets = dataio_prep(hparams)\n",
    "\n",
    "    # Initialize the Brain object for training the ECAPA-TDNN model\n",
    "    ecapa_brain = ECAPABrain(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    ecapa_brain.fit(\n",
    "        epoch_counter=ecapa_brain.hparams.epoch_counter,\n",
    "        train_set=datasets[\"train\"],\n",
    "        valid_set=datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    ecapa_brain.evaluate(\n",
    "        test_set=datasets[\"test\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "    print(\"Error of No spk class\")\n",
    "    ecapa_brain.evaluate(\n",
    "        test_set=datasets[\"test_0_spk\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "    print(\"Error of No spk class\")\n",
    "    ecapa_brain.evaluate(\n",
    "        test_set=datasets[\"test_0_spk\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "    print(\"Error of 1 spk class\")\n",
    "    ecapa_brain.evaluate(\n",
    "        test_set=datasets[\"test_1_spk\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "    print(\"Error of 2 spk class\")\n",
    "    ecapa_brain.evaluate(\n",
    "        test_set=datasets[\"test_2_spk\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "    print(\"Error of 3 spk class\")\n",
    "    ecapa_brain.evaluate(\n",
    "        test_set=datasets[\"test_3_spk\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "    print(\"Error of 4 spk class\")\n",
    "    ecapa_brain.evaluate(\n",
    "        test_set=datasets[\"test_4_spk\"],\n",
    "        min_key=\"error\",\n",
    "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_ecapa_tdnn.py\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "e7a67097551cbdaf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-15T15:52:43.859472Z",
     "start_time": "2024-04-15T15:52:43.766948Z"
    }
   },
   "source": [
    "import torch\n",
    "torch.cuda.set_device(\"cuda:0\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a952f03eae1436e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:39:11.220165Z",
     "start_time": "2024-04-15T15:37:23.869283Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train_ecapa_tdnn.py hparams_ecapa_tdnn_augmentation.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c1a35963801b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
